{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib; matplotlib.use('Agg')\n",
    "from matplotlib import transforms\n",
    "from matplotlib import rc\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "from astropy.io import fits as pyfits\n",
    "from astropy.io import fits\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import scipy\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "from scipy.special import factorial\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import quad\n",
    "from itertools import starmap\n",
    "from scipy import optimize\n",
    "import corner\n",
    "import time\n",
    "from mpl_toolkits import mplot3d\n",
    "import healpy as hp\n",
    "from scipy import nan\n",
    "import dark_matter_jfactors_test as dmj\n",
    "import math\n",
    "import random\n",
    "import importlib\n",
    "from pymultinest.solve import solve\n",
    "import pymultinest\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy as sp\n",
    "import scipy.interpolate\n",
    "import json\n",
    "from os import walk\n",
    "import re\n",
    "import acceptance_psf_eastrogam as aaa\n",
    "import photon_spectrum\n",
    "import evaporating_black_hole_template as dm_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist1 = ['Bremss_00320087_E_50-814008_MeV_healpix_128.fits', 'Bremss_SL_Z6_R20_T100000_C5_E_50-814008_MeV_healpix_128.fits', 'pi0_Model_A_E_50-814008_MeV_healpix_128.fits', 'pi0_Model_F_E_50-814008_MeV_healpix_128.fits', 'ICS_Model_A_E_50-814008_MeV_healpix_128.fits', 'ICS_Model_F_E_50-814008_MeV_healpix_128.fits']\n",
    "filelist = ['bremss_healpix_reshuffled_61templates.fits', 'bremss_healpix_reshuffled_61templates.fits', 'pi0_decay_healpix_reshuffled_61templates.fits', 'pi0_decay_healpix_reshuffled_61templates.fits', 'ics_isotropic_healpix_reshuffled_61templates.fits', 'ics_isotropic_healpix_reshuffled_61templates.fits']\n",
    "evermore_shifted = np.asarray([\n",
    "    [33, 21, 49],\n",
    "    [154, 112, 82],\n",
    "    [241, 149, 91],\n",
    "    [142, 52, 38],\n",
    "    [33, 21, 49],\n",
    "])/256\n",
    "\n",
    "point_sources = ['Pointsources_central_23deg_61energybins.fits']\n",
    "\n",
    "#for astrogam, use 5 years\n",
    "exposure_time = 1.578e8\n",
    "#exposure_time = 4.1e6 #.13 years 47 days\n",
    "#exposure_time = 4.1e8 #13 years\n",
    "#exposure_time = 4.1e9 #130 years\n",
    "#exposure_time = 4.1e11 #13000 years\n",
    "#exposure_time = 4.1e12 #13000 years\n",
    "#exposure_time = 3.154e10\n",
    "\n",
    "acceptance_interp = aaa.get_acceptance_interp() #put in the energy in MeV!\n",
    "angle_interp = aaa.get_angle_interp()\n",
    "\n",
    "central_energies = np.array([1.22474487e+00, 1.54186244e+00, 1.94108981e+00, 2.44368729e+00,\n",
    "       3.07642002e+00, 3.87298335e+00, 4.87579715e+00, 6.13826494e+00,\n",
    "       7.72761772e+00, 9.72849432e+00, 1.22474487e+01, 1.54186244e+01,\n",
    "       1.94108981e+01, 2.44368729e+01, 3.07642002e+01, 3.87298335e+01,\n",
    "       4.87579715e+01, 6.13826494e+01, 7.72761772e+01, 9.72849432e+01,\n",
    "       1.22474487e+02, 1.54186244e+02, 1.94108981e+02, 2.44368729e+02,\n",
    "       3.07642002e+02, 3.87298335e+02, 4.87579715e+02, 6.13826494e+02,\n",
    "       7.72761772e+02, 9.72849432e+02, 1.22474487e+03, 1.54186244e+03,\n",
    "       1.94108981e+03, 2.44368729e+03, 3.07642002e+03, 3.87298335e+03,\n",
    "       4.87579715e+03, 6.13826494e+03, 7.72761772e+03, 9.72849432e+03,\n",
    "       1.22474487e+04, 1.54186244e+04, 1.94108981e+04, 2.44368729e+04,\n",
    "       3.07642002e+04, 3.87298335e+04, 4.87579715e+04, 6.13826494e+04,\n",
    "       7.72761772e+04, 9.72849432e+04, 1.22474487e+05, 1.54186244e+05,\n",
    "       1.94108981e+05, 2.44368729e+05, 3.07642002e+05, 3.87298335e+05,\n",
    "       4.87579715e+05, 6.13826494e+05, 7.72761772e+05, 9.72849431e+05,\n",
    "       1.22474487e+06])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Egammas for evaporating black holes\n",
    "energiesforBH = np.logspace(np.log10(.05), np.log10(1e7), num = 1000)\n",
    "#egamma_values = photon_spectrum.get_egammas(energiesforBH)\n",
    "egamma_values = fits.open('egammavals.fits')[0].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    \"\"\"\n",
    "    A simple function to read the maps of a given number n and given filename.\n",
    "    \"\"\"\n",
    "    file_to_read = fits.open(filename)\n",
    "    return file_to_read\n",
    "\n",
    "def reshape_file(hdu, n, inner20 = True):\n",
    "    \"\"\"\n",
    "    Reshapes the data to be in the size we want\n",
    "    \"\"\"\n",
    "    hdu1 = readfile(filelist1[0])\n",
    "    \n",
    "    if inner20:\n",
    "        numpix = np.linspace(0, hdu1[0].header['NPIX']-1, num = hdu1[0].header['NPIX'])\n",
    "        NSIDE = int(hdu1[0].header['NSIDE'])\n",
    "        degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "        vec = hp.ang2vec(np.pi/2, 0)\n",
    "        ipix_disc20 = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "        \n",
    "        data4 = hdu[n].data\n",
    "        test20 = np.copy(data4)[ipix_disc20]\n",
    "        \n",
    "        #might need this for plotting, not sure\n",
    "        #data4 = hdu[n].data\n",
    "        #test20 = np.copy(data4)\n",
    "        #test20[inner_20] = np.nan\n",
    "        #testbin = np.reshape(test20, (128*3, 1536//3))\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        testbin = np.reshape(hdu[n].data, (128*3, 1536//3))\n",
    "        \n",
    "        \"\"\"\n",
    "        numpix = np.linspace(0, hdu1[0].header['NPIX']-1, num = hdu1[0].header['NPIX'])\n",
    "        NSIDE = int(hdu1[0].header['NSIDE'])\n",
    "        degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "        \n",
    "        inner_20_pos = (np.sqrt((degrees[0])**2+degrees[1]**2)> 20)\n",
    "        inner_20_neg = (np.sqrt((degrees[0]-360)**2+degrees[1]**2)> 20)\n",
    "        inner_20 = np.logical_and(inner_20_pos, inner_20_neg)\n",
    "        \n",
    "        data4 = hdu[n].data\n",
    "        test20 = np.copy(data4)\n",
    "        #test20[inner_20] = np.nan\n",
    "        #print(sum(~np.isnan(test20)))\n",
    "        #testbin = np.reshape(test20, (128*3, 1536//3))\n",
    "        \n",
    "    return test20\n",
    "\n",
    "def get_energy_index(E_desired):\n",
    "    idx = find_nearest(central_energies, E_desired)\n",
    "    return idx\n",
    "    \n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def log_interp1d(xx, yy, kind='linear'):\n",
    "    logx = np.log10(xx)\n",
    "    logy = np.log10(yy)\n",
    "    lin_interp = sp.interpolate.interp1d(logx, logy, kind=kind, fill_value=\"extrapolate\")\n",
    "    log_interp = lambda zz: np.power(10.0, lin_interp(np.log10(zz)))\n",
    "    return log_interp\n",
    "\n",
    "def get_all_egb_data(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "    return middle_bin, bin_width, egb_intensity\n",
    "\n",
    "def get_all_egb(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "        \n",
    "\n",
    "    log_interp = log_interp1d(middle_bin, egb_intensity/bin_width, kind='linear')\n",
    "    \n",
    "    '''\n",
    "    print(egb_intensity[2]/bin_width[2]*deltae[6])\n",
    "    print(energies[6])\n",
    "    x_trapz = np.logspace(np.log10(np.nanmin(energies)), np.log10(np.nanmax(energies)), num = 100)\n",
    "    plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "    plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "    plt.scatter(energies[6], log_interp(energies[6]), color = 'green')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    '''\n",
    "    \n",
    "    counts = []\n",
    "    #only want energies from 3 onward (about lowest at 80 MeV)\n",
    "    for x in range(3, len(energies)):\n",
    "        highest_val = energies[x]+deltae[x]\n",
    "        lowest_val = energies[x]-deltae[x]\n",
    "        x_trapz = np.logspace(np.log10(lowest_val), np.log10(highest_val), num = 400)\n",
    "        #counts.append(log_interp(energies[x])*deltae[x])\n",
    "        total_counts = np.trapz(log_interp(x_trapz), x = x_trapz)\n",
    "        \n",
    "        #print('total counts: {}'.format(total_counts))\n",
    "        '''\n",
    "        plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "        plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "        plt.scatter(energies[x], total_counts)\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        asdfads\n",
    "        '''\n",
    "        counts.append(total_counts)  \n",
    "    return counts #returns counts per cm^2 per sec per str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract(n):\n",
    "    ##Template for 1 GeV and 10 GeV\n",
    "    icsa = readfile(filelist[n])\n",
    "    \n",
    "    idx1 = get_energy_index(1*1e3, icsa)\n",
    "    idx10 = get_energy_index(10*1e3, icsa)\n",
    "    \n",
    "    array1 = reshape_file(icsa, idx1, inner20 = False)\n",
    "    array10 = reshape_file(icsa, idx10, inner20 = False)\n",
    "    \n",
    "    ##Sum up idx1 and idx 10, make them equal in sum\n",
    "    sum1 = np.nansum(array1)\n",
    "    sum10 = np.nansum(array10)\n",
    "    array10_adjusted = (array10*sum1/sum10)\n",
    "    subtract110 = np.abs(array1-array10_adjusted)/array1\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    image = ax.imshow(subtract110)\n",
    "    fig.colorbar(image, ax=ax, anchor=(0, 0.3), shrink=0.7)\n",
    "    plt.title(str(filelist[n]))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#n = 4 for ICSA, n = 2 for pi0\n",
    "def psf_smoothing(n, energyidx, inner20psf = False, pointsource = False, use_og = False):\n",
    "    inner20psf = False\n",
    "    if pointsource:\n",
    "        icsa = readfile(point_sources[0])\n",
    "        data50 = icsa[energyidx].data\n",
    "    else:\n",
    "        icsa = readfile(filelist[n])\n",
    "        data50 = reshape_file(icsa, energyidx, inner20 = inner20psf)\n",
    "    hdu = readfile(filelist1[0])\n",
    "\n",
    "    #data50 = reshape_file(icsa, energyidx, inner20 = inner20psf) #get the data at 50 MeV\n",
    "    \n",
    "    #get_where_within_20deg\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    ipix_disc = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "    \n",
    "    init_sum = np.sum(data50[ipix_disc])\n",
    "    #print(init_sum)\n",
    "    energy_here = float(central_energies[energyidx])\n",
    "    sig = np.pi/180*angle_interp(energy_here)\n",
    "    data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=sig)\n",
    "\n",
    "    fin_sum = np.sum(data50_convolved[ipix_disc])\n",
    "    #print(fin_sum)\n",
    "    \n",
    "\n",
    "    #data50_convolved = gaussian_filter(data50, sigma=0)\n",
    "    \n",
    "    #only get data within 20 degrees\n",
    "    #hp.mollview(np.log10(data50_convolved), coord = 'G')\n",
    "    \n",
    "    \n",
    "    testbin = data50_convolved[ipix_disc]\n",
    "    #print(len(testbin))\n",
    "    \n",
    "    #print(init_sum)\n",
    "    #print(fin_sum)\n",
    "    #print('---------------')\n",
    "\n",
    "\n",
    "    return np.array(testbin)*init_sum/fin_sum\n",
    "\n",
    "def psf_smoothing_DM(energyidx, crosssec, anal_data = False, mass_dm = 40, use_og = False, evapbh = False, gam = 1, massbh = 2e16, lum_interp = 1):\n",
    "    energybin = central_energies[energyidx]\n",
    "    bins_in_lin = np.log10(energybin)\n",
    "    deltae = get_deltaE(energyidx)\n",
    "    \n",
    "    highe = (energybin+deltae)/1e3\n",
    "    lowe = (energybin-deltae)/1e3\n",
    "    \n",
    "    hdu = readfile(filelist1[0])\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "    degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "\n",
    "    \n",
    "\n",
    "    #need to make sure the initsum is *only within the inner 20 degrees, same for finsum\n",
    "    if evapbh == True:\n",
    "        data50 = dm_template.get_dNdE(egamma_values, energyidx, lum_interp, gamma = gam, mass = massbh, for_normals = False)\n",
    "        #photons per cm^2 per sec per str\n",
    "    else:\n",
    "        data50 = dmj.get_dNdE(highe, lowe, sigmav = crosssec, analyze_data = anal_data, massx = mass_dm)[1] #photons per cm^2 per sec per str\n",
    "\n",
    "    \n",
    "    #get_where_within_20deg\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    ipix_disc = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "    \n",
    "    init_sum = np.sum(data50[ipix_disc])\n",
    "    #print('init sum: {}'.format(init_sum))\n",
    "    \n",
    "    energy_here = central_energies[energyidx]\n",
    "    sig = np.pi/180*angle_interp(energy_here)\n",
    "    if use_og:\n",
    "        data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=np.pi/180/1.508)\n",
    "    else:\n",
    "        data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=sig)\n",
    "    fin_sum = np.sum(data50_convolved[ipix_disc])\n",
    "    \n",
    "    #hp.mollview((data50_convolved), coord = 'G')\n",
    "    \n",
    "    #photons per cm^2 per sec per str\n",
    "\n",
    "    \n",
    "    return np.array(data50_convolved[ipix_disc])*init_sum/fin_sum\n",
    "\n",
    "def get_deltaE(n):\n",
    "    energybins = central_energies\n",
    "    bins_in_lin = np.log10(energybins)[n]\n",
    "    #spacing = 0.05691431 #old spacing\n",
    "    spacing = 0.17609125905568124 #new spacing\n",
    "    \n",
    "    high_bin = 10**(bins_in_lin + spacing)\n",
    "    low_bin = 10**(bins_in_lin - spacing)\n",
    "    \n",
    "    deltaE = np.abs(high_bin - low_bin)\n",
    "    #print('delta E: {}'.format(deltaE))\n",
    "    \n",
    "    return deltaE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def poisson_dist_general(n, energyidx, cross_section =1.4e-26, dm = False, dm_bh = False, analyze_data = False, dm_mass = 40, egb = False, points = False, counts = 0, evapbh = True, blackholem = 2e16,  luminterpolated = 1, gamm = 1.6):   \n",
    "    '''\n",
    "    Performs a PSF smoothing of the array, before converting it into photons per pixel\n",
    "    \n",
    "    '''\n",
    "    deltaE = get_deltaE(energyidx)\n",
    "    #print(deltaE)\n",
    "    energy_here = central_energies[energyidx]\n",
    "    #print(energy_here)\n",
    "    acceptance_for_poisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    if dm:\n",
    "        convolved_data = psf_smoothing_DM(energyidx, cross_section, anal_data = analyze_data, mass_dm = dm_mass)/deltaE\n",
    "    elif egb:\n",
    "        convolved_data_init = np.empty(5938) #needs to be the length of the good vals\n",
    "        convolved_data_init.fill(1) #counts per cm^2 per sec per str\n",
    "        convolved_data = convolved_data_init*counts #in units of photons per cm^2 per mev per str per sec\n",
    "    elif points:\n",
    "        convolved_data = psf_smoothing(n, energyidx, pointsource = True)\n",
    "    elif dm_bh:\n",
    "        convolved_data = psf_smoothing_DM(energyidx, cross_section, lum_interp = luminterpolated, anal_data = analyze_data, evapbh = True, massbh = blackholem, gam = gamm)/deltaE\n",
    "    else:\n",
    "        convolved_data = psf_smoothing(n, energyidx) #data in units of photons cm^-2 MeV^-1 str^-1\n",
    "    #n_gamma = np.array(convolved_data)*deltaE*exposure_time*8500*4*np.pi/196608*.2 #13 years*.85meters^2, units of photons per pixel\n",
    "    n_gamma = np.array(convolved_data)*deltaE #photons per cm^2 per sec per sr\n",
    "\n",
    "    \n",
    "    return n_gamma\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "#get all ktests\n",
    "\n",
    "def get_ktests(fbh = 0, save= False, ks = True):\n",
    "\n",
    "    ktest_array = []\n",
    "    importlib.reload(aaa)\n",
    "\n",
    "    energies = np.copy(central_energies)\n",
    "    #cross_sec normalized to 1.4e-26\n",
    "    deltae_cut = np.copy(deltae)\n",
    "\n",
    "    egb_counts = get_all_egb(energies, deltae)/deltae_cut\n",
    "    counting = 0\n",
    "    test_cross = 1.4e-26\n",
    "    blackholem = 2e16\n",
    "    gam = 1.6\n",
    "\n",
    "    energiesforBH = np.logspace(np.log10(.05), np.log10(1e7), num = 1000) #in MeV\n",
    "    #eventually make it so this is only calculated once per mass_bh\n",
    "    lum = (photon_spectrum.get_integral(egamma_values, mass_bh = blackholem)[1]) #units of photons per MeV per sec per BH\n",
    "    #interpolate the luminosity\n",
    "    lum_interp = log_interp1d(energiesforBH, lum, kind='linear') #integrate dn/dE in units of MeV\n",
    "    print('frac bh: {}'.format(fbh))\n",
    "\n",
    "    for energyidx in range(0, 10):\n",
    "        print('energy here: {}'.format(energies[energyidx]))\n",
    "        en = energies[energyidx]\n",
    "        print(energyidx)\n",
    "        pitest = poisson_dist_general(2, int(energyidx))\n",
    "        icstest = poisson_dist_general(4, int(energyidx))\n",
    "        bremtest = poisson_dist_general(0, int(energyidx))\n",
    "        #EGB counts, we have at each energy bin in units of per cm^2 per s per str\n",
    "        egbtest = poisson_dist_general(np.nan, int(energyidx), egb = True, counts = egb_counts[energyidx])\n",
    "        energy_here = central_energies[energyidx]\n",
    "        acceptance_for_poisson = acceptance_interp(energy_here)\n",
    "\n",
    "\n",
    "        #Point Sources\n",
    "        pointstest = poisson_dist_general(np.nan, energyidx, points = True) \n",
    "\n",
    "\n",
    "        #Dark matter\n",
    "        importlib.reload(dmj) \n",
    "        darkmtest = poisson_dist_general(np.nan, int(energyidx), dm = True, analyze_data = False) #units of photons per cm^2 per s per str\n",
    "        darkmtest[np.isnan(darkmtest)] = 0\n",
    "        \n",
    "        #Black Holes\n",
    "        blackholetest = poisson_dist_general(np.nan, int(energyidx), cross_section = test_cross, dm_bh = True, evapbh = True, blackholem = 2e16, luminterpolated = lum_interp, gamm = gam)*fbh\n",
    "        blackholetest[np.isnan(blackholetest)] = 0\n",
    "        \n",
    "        #print(np.max((pitest+icstest+bremtest+egbtest+pointstest+darkmtest+blackholetest)))\n",
    "        #print(np.min((pitest+icstest+bremtest+egbtest+pointstest+darkmtest+blackholetest)))\n",
    "\n",
    "        if ks == True:\n",
    "            #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "            #print((pitest+icstest+bremtest+egbtest+pointstest+darkmtest+blackholetest)*exposure_time*acceptance_for_poisson/196608)\n",
    "            ktest1 = simulated_data(int(energyidx), list(np.array([pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest])*exposure_time*acceptance_for_poisson/196608))\n",
    "            ktest_array.append(ktest1*en/(exposure_time*acceptance_for_poisson/196608))\n",
    "            \n",
    "        else:\n",
    "            ktest_array.append((pitest+icstest+bremtest+egbtest+pointstest+darkmtest+blackholetest)*en) #units of MeV/s/str/cm^2\n",
    "\n",
    "        counting += 1\n",
    "        \n",
    "    if save:\n",
    "        hdu = fits.PrimaryHDU(ktest_array)\n",
    "        hdulist = fits.HDUList([hdu])\n",
    "        hdulist.writeto('ktestsfile1MeVfin5yrs.fits')\n",
    "    return np.array(sum(list(ktest_array)))\n",
    "\n",
    "def simulated_data(energyidx, templates, fac = 1):\n",
    "    \n",
    "    '''\n",
    "    Use PSF smoothed data to create a random poisson draw to obtain simulated data\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    n_gammatot = 0\n",
    "    for i in range(0, len(templates)):\n",
    "        n_gammatot += templates[i]\n",
    "    len_of_rand = len(n_gammatot)\n",
    "    simdata = np.zeros(len_of_rand)\n",
    "    randdata = np.random.rand(len_of_rand)\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(n_gammatot)\n",
    "    n_gammatot = n_gammatot\n",
    "    \n",
    "    for i in range(0, len_of_rand):\n",
    "        if n_gammatot[i]<.1:\n",
    "            if n_gammatot[i] < randdata[i]:\n",
    "                simdata[i] = 0\n",
    "            else:\n",
    "                simdata[i] = 1\n",
    "        else:\n",
    "            tempdata = np.random.poisson(lam = n_gammatot[i])\n",
    "            simdata[i] = tempdata\n",
    "            #print(simdata[i])\n",
    "    #print('simdata just 20: ', np.nansum(simdata))\n",
    "    \n",
    "    return simdata\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n",
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  29.,  101.,  144.,  160.,  190.,  180.,  904., 1783., 2116.,\n",
       "         331.]),\n",
       " array([-1.75431933, -1.68054013, -1.60676092, -1.53298172, -1.45920252,\n",
       "        -1.38542332, -1.31164411, -1.23786491, -1.16408571, -1.09030651,\n",
       "        -1.0165273 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test1data0 = get_ktests(fbh = 0, ks = True)\n",
    "test2data0 = get_ktests(fbh = 0, ks = False)\n",
    "plt.hist(test1data0-test2data0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n",
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test1data0 = get_ktests(fbh = 0, ks = True)\n",
    "test2data0 = get_ktests(fbh = 0, ks = False)\n",
    "plt.hist(np.abs(test1data0-test2data0))\n",
    "make_maps(data0, log = False, titlehere = 'data FBH = 0 test')\n",
    "make_maps(model0, log = False, titlehere = 'model FBH = 0 test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n",
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n",
      "frac bh: 0.0001\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n",
      "frac bh: 0.0001\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "data0 = get_ktests(fbh = 0, ks = True)\n",
    "model0 = get_ktests(fbh = 0, ks = False)\n",
    "data4 = get_ktests(fbh = 1e-4, ks = True)\n",
    "model4 = get_ktests(fbh = 1e-4, ks = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(data0-model0, log = False, titlehere = '2data FBH = 0 - model FBH = 0')\n",
    "make_maps(data4-model0, log = False, titlehere = '2data FBH = 1e-4 - model FBH = 0')\n",
    "make_maps(data4-model4, log = False, titlehere = '2data FBH = 1e-4 - model FBH = 1e-4')\n",
    "make_maps(data0-model0, log = False, titlehere = '2data FBH = 0 - model FBH = 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(data0, log = False, titlehere = 'data FBH = 0')\n",
    "make_maps(model0, log = False, titlehere = 'model FBH = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "0.4445298964333439\n",
      "0.25755258258051866\n",
      "energy here: 1.54186244\n",
      "1\n",
      "0.32729726872095116\n",
      "0.1808646311115651\n",
      "energy here: 1.94108981\n",
      "2\n",
      "0.24296693577926054\n",
      "0.12703899361432458\n",
      "energy here: 2.44368729\n",
      "3\n",
      "0.18054867842601185\n",
      "0.08931572232416099\n",
      "energy here: 3.07642002\n",
      "4\n",
      "0.1357040480166317\n",
      "0.06285538461114085\n",
      "energy here: 3.87298335\n",
      "5\n",
      "0.10294178135910716\n",
      "0.04427485637088296\n",
      "energy here: 4.87579715\n",
      "6\n",
      "0.07886270170740288\n",
      "0.03122058174156893\n",
      "energy here: 6.13826494\n",
      "7\n",
      "0.05828683602649239\n",
      "0.02204627222559587\n",
      "frac bh: 0.0001\n",
      "energy here: 1.22474487\n",
      "0\n",
      "0.44458322666813427\n",
      "0.2576005015614946\n",
      "energy here: 1.54186244\n",
      "1\n",
      "0.3273767332674015\n",
      "0.1809360980652555\n",
      "energy here: 1.94108981\n",
      "2\n",
      "0.2430788601754015\n",
      "0.12713965963160326\n",
      "energy here: 2.44368729\n",
      "3\n",
      "0.18068094668830342\n",
      "0.0894370548684452\n",
      "energy here: 3.07642002\n",
      "4\n",
      "0.13583265896020538\n",
      "0.0629733643002884\n",
      "energy here: 3.87298335\n",
      "5\n",
      "0.10303999762015324\n",
      "0.044364955189876126\n",
      "energy here: 4.87579715\n",
      "6\n",
      "0.07891914840796212\n",
      "0.03127236384198159\n",
      "energy here: 6.13826494\n",
      "7\n",
      "0.058314371580562525\n",
      "0.022067305327722768\n"
     ]
    }
   ],
   "source": [
    "model0 = get_ktests(fbh = 0, ks = False) #units of MeV/s/str/cm^2\n",
    "model4 = get_ktests(fbh = 1e-4, ks = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 1\n",
      "energy here: 1.22474487\n",
      "0\n",
      "4.507959896083286\n",
      "0.7357057926816576\n",
      "energy here: 1.54186244\n",
      "1\n",
      "6.879082578328222\n",
      "0.8933404832815339\n",
      "energy here: 1.94108981\n",
      "2\n",
      "10.137551994937787\n",
      "1.1292202935162243\n",
      "energy here: 2.44368729\n",
      "3\n",
      "12.75331642049929\n",
      "1.2964249611158254\n",
      "energy here: 3.07642002\n",
      "4\n",
      "12.969303822191618\n",
      "1.2363645082498818\n",
      "energy here: 3.87298335\n",
      "5\n",
      "10.37464466964543\n",
      "0.9406000371222392\n",
      "energy here: 4.87579715\n",
      "6\n",
      "6.2565826147337145\n",
      "0.5467303349408172\n",
      "energy here: 6.13826494\n",
      "7\n",
      "2.5793539401347267\n",
      "0.23183845770212996\n"
     ]
    }
   ],
   "source": [
    "model1 = get_ktests(fbh = 1, ks = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0.01\n",
      "energy here: 1.22474487\n",
      "0\n",
      "0.4498629199123814\n",
      "0.2623444806781155\n",
      "energy here: 1.54186244\n",
      "1\n",
      "0.3352437233659873\n",
      "0.18801132648060778\n",
      "energy here: 1.94108981\n",
      "2\n",
      "0.2870101589304596\n",
      "0.1371055953421934\n",
      "energy here: 2.44368729\n",
      "3\n",
      "0.2673089693221178\n",
      "0.10144897675258226\n",
      "energy here: 3.07642002\n",
      "4\n",
      "0.23639416017665332\n",
      "0.07465335352589564\n",
      "energy here: 3.87298335\n",
      "5\n",
      "0.1865241292868004\n",
      "0.05328473827019954\n",
      "energy here: 4.87579715\n",
      "6\n",
      "0.12803520180689992\n",
      "0.0363987917828353\n",
      "energy here: 6.13826494\n",
      "7\n",
      "0.07791778325927415\n",
      "0.02414958243828557\n",
      "frac bh: 100.0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "417.3514270208695\n",
      "47.89530657965927\n",
      "energy here: 1.54186244\n",
      "1\n",
      "663.2711955039558\n",
      "71.1693976633989\n",
      "energy here: 1.94108981\n",
      "2\n",
      "995.3834735464345\n",
      "100.1157783602521\n",
      "energy here: 2.44368729\n",
      "3\n",
      "1261.4617005356968\n",
      "120.60320070743978\n",
      "energy here: 3.07642002\n",
      "4\n",
      "1286.3058949784584\n",
      "117.24504691084341\n",
      "energy here: 3.87298335\n",
      "5\n",
      "1029.200633906241\n",
      "89.53340334585315\n",
      "energy here: 4.87579715\n",
      "6\n",
      "619.1215681954914\n",
      "51.46289409825492\n",
      "energy here: 6.13826494\n",
      "7\n",
      "252.74271045014729\n",
      "20.913268962577746\n"
     ]
    }
   ],
   "source": [
    "model2 = get_ktests(fbh = 1e-2, ks = False)\n",
    "\n",
    "model22 = get_ktests(fbh = 1e2, ks = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 7.72761772\n",
      "8\n",
      "0.043550603145987486\n",
      "0.015582946370761862\n",
      "energy here: 9.72849432\n",
      "9\n",
      "0.03486002818806063\n",
      "0.011023838728347769\n",
      "energy here: 12.2474487\n",
      "10\n",
      "0.012450422779057876\n",
      "0.008392142379833063\n",
      "energy here: 15.4186244\n",
      "11\n",
      "0.009647209776831117\n",
      "0.005878397355121188\n",
      "energy here: 19.4108981\n",
      "12\n",
      "0.007611429580672952\n",
      "0.004126543590827904\n",
      "frac bh: 0.0001\n",
      "energy here: 7.72761772\n",
      "8\n",
      "0.04355836269770681\n",
      "0.015588873523829344\n",
      "energy here: 9.72849432\n",
      "9\n",
      "0.034871351247345556\n",
      "0.011024998124826522\n",
      "energy here: 12.2474487\n",
      "10\n",
      "0.012450841983498159\n",
      "0.008392296057773047\n",
      "energy here: 15.4186244\n",
      "11\n",
      "0.009647243426229916\n",
      "0.0058784086120348905\n",
      "energy here: 19.4108981\n",
      "12\n",
      "0.00761143101735521\n",
      "0.004126544033623072\n"
     ]
    }
   ],
   "source": [
    "data0 = get_ktests(fbh = 0, ks = True)\n",
    "data4 = get_ktests(fbh = 1e-4, ks = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(data0, log = False, titlehere = 'data3 FBH = 0')\n",
    "make_maps(data4, log = False, titlehere = 'data3 FBH = 1e-4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "0.4445298964333439\n",
      "0.25755258258051866\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asdfasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e78f47a41507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ktests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#units of MeV/s/str/cm^2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ktests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ktests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ktests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata4_model0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata4\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-24fd8d89b8ce>\u001b[0m in \u001b[0;36mget_ktests\u001b[0;34m(fbh, save, ks)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpitest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0micstest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbremtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0megbtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpointstest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdarkmtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblackholetest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpitest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0micstest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbremtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0megbtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpointstest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdarkmtest\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblackholetest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0masdfasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mks\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfasd' is not defined"
     ]
    }
   ],
   "source": [
    "model0 = get_ktests(fbh = 0, ks = False)\n",
    "data0 = get_ktests(fbh = 0, ks = True)\n",
    "model4 = get_ktests(fbh = 1e-4, ks = False)\n",
    "data4 = get_ktests(fbh = 1e-4, ks = True)\n",
    "data4_model0 = data4-model0\n",
    "data4_model4 = data4-model4\n",
    "data0_model0 = data0-model0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15595.828599679804\n",
      "15728.02420458\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(model0))\n",
    "print(np.sum(data0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_maps(datahere, log = False, titlehere = ''):\n",
    "    hdu = readfile(filelist1[0])\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "    degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    removepix = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(200), inclusive = False)\n",
    "    goodpix = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "\n",
    "    dat = hdu[0].data\n",
    "\n",
    "    dat[removepix] = np.nan\n",
    "    dat[goodpix] = datahere\n",
    "\n",
    "    if log == True:\n",
    "        hp.mollview(np.log10(dat), unit = 'MeV/cm^2/s/str', xsize = 1200, title = titlehere)\n",
    "    else:\n",
    "        hp.mollview(dat, unit = 'MeV/cm^2/s/str', xsize = 1200, title = titlehere)\n",
    "        \n",
    "    plt.savefig(titlehere + '.pdf')\n",
    "    plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac bh: 0\n",
      "energy here: 1.22474487\n",
      "0\n",
      "energy here: 1.54186244\n",
      "1\n",
      "energy here: 1.94108981\n",
      "2\n",
      "energy here: 2.44368729\n",
      "3\n",
      "energy here: 3.07642002\n",
      "4\n",
      "energy here: 3.87298335\n",
      "5\n",
      "energy here: 4.87579715\n",
      "6\n",
      "energy here: 6.13826494\n",
      "7\n",
      "energy here: 7.72761772\n",
      "8\n",
      "energy here: 9.72849432\n",
      "9\n",
      "energy here: 12.2474487\n",
      "10\n",
      "energy here: 15.4186244\n",
      "11\n",
      "energy here: 19.4108981\n",
      "12\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(data0/196608, log = False, titlehere = 'data FBH =0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_maps(model1, log = False, titlehere = 'Model FBH = 1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_maps(model0, log = False, titlehere = 'Model FBH = 0')\n",
    "make_maps(model4, log = False, titlehere = 'Model FBH = 1e-4')\n",
    "make_maps(data0, log = False, titlehere = 'Data FBH = 0')\n",
    "make_maps(data4, log = False, titlehere = 'Data FBH = 1e-4')\n",
    "make_maps(data4_model0, log = False, titlehere = 'data 1e-4 - model 0')\n",
    "make_maps(data4_model4, log = False, titlehere = 'data 1e-4 - model 1e-4')\n",
    "make_maps(data0_model0, log = False, titlehere = 'data 0 - model 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def poisson_dist(n, energyidx, cross_section =1.4e-26, dm = False, dm_bh = False, analyze_data = False, dm_mass = 40, egb = False, points = False, counts = 0, evapbh = True, blackholem = 2e16,  luminterpolated = 1, gamm = 1.6):   \n",
    "    '''\n",
    "    Performs a PSF smoothing of the array, before converting it into photons per pixel\n",
    "    \n",
    "    '''\n",
    "    deltaE = get_deltaE(energyidx)\n",
    "    #print(deltaE)\n",
    "    energy_here = central_energies[energyidx]\n",
    "    #print(energy_here)\n",
    "    acceptance_for_poisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    if dm:\n",
    "        convolved_data = psf_smoothing_DM(energyidx, cross_section, anal_data = analyze_data, mass_dm = dm_mass)/deltaE\n",
    "    elif egb:\n",
    "        convolved_data_init = np.empty(5938) #needs to be the length of the good vals\n",
    "        convolved_data_init.fill(1) #counts per cm^2 per sec per str\n",
    "        convolved_data = convolved_data_init*counts #in units of photons per cm^2 per mev per str per sec\n",
    "    elif points:\n",
    "        convolved_data = psf_smoothing(n, energyidx, pointsource = True)\n",
    "    elif dm_bh:\n",
    "        convolved_data = psf_smoothing_DM(energyidx, cross_section, lum_interp = luminterpolated, anal_data = analyze_data, evapbh = True, massbh = blackholem, gam = gamm)/deltaE\n",
    "    else:\n",
    "        convolved_data = psf_smoothing(n, energyidx) #data in units of photons cm^-2 MeV^-1 str^-1\n",
    "    #n_gamma = np.array(convolved_data)*deltaE*exposure_time*8500*4*np.pi/196608*.2 #13 years*.85meters^2, units of photons per pixel\n",
    "    n_gamma = np.array(convolved_data)*deltaE*exposure_time*acceptance_for_poisson/196608 #13 years*.85meters^2, units of photons per pixel\n",
    "\n",
    "    \n",
    "    return n_gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_image(data):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    image = ax.imshow(data)\n",
    "    fig.colorbar(image, ax=ax, anchor=(0, 0.3), shrink=0.7)\n",
    "    return\n",
    "\n",
    "def simulated_data(energyidx, templates):\n",
    "    \n",
    "    '''\n",
    "    Use PSF smoothed data to create a random poisson draw to obtain simulated data\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    n_gammatot = 0\n",
    "    for i in range(0, len(templates)):\n",
    "        n_gammatot += templates[i]\n",
    "    len_of_rand = len(n_gammatot)\n",
    "    simdata = np.zeros(len_of_rand)\n",
    "    randdata = np.random.rand(len_of_rand)\n",
    "    \n",
    "    #print(n_gammatot)\n",
    "    \n",
    "    for i in range(0, len_of_rand):\n",
    "        #print(n_gammatot[i])\n",
    "        if n_gammatot[i]<.01:\n",
    "            if n_gammatot[i] < randdata[i]:\n",
    "                simdata[i] = 0\n",
    "            else:\n",
    "                simdata[i] = 1\n",
    "        else:\n",
    "            simdata[i] = np.random.poisson(lam = n_gammatot[i])\n",
    "    #print('simdata just 20: ', np.nansum(simdata))\n",
    "    \n",
    "    return simdata\n",
    "\n",
    "def minimize_likelihood(energyidx, cross_sec = 1.4e-26, massdm = 40):\n",
    "    pi = poisson_dist(2, energyidx)\n",
    "    ics = poisson_dist(4, energyidx)   \n",
    "    brem = poisson_dist(0, energyidx)\n",
    "\n",
    "    darkm = poisson_dist(np.nan, energyidx, cross_section = cross_sec, dm = True, dm_mass = massdm)\n",
    "\n",
    "    k = simulated_data(energyidx, [pi, ics, brem])#remove DM for accurate\n",
    "    #print(np.nanmean(lamb))\n",
    "    #asdfasd\n",
    "    \n",
    "    #Need to minimize for lamb < 50 and lamb > 50\n",
    "    \n",
    "    bnds = ((0, np.inf), (0, np.inf), (0, np.inf), (1e-30, np.inf))\n",
    "    result = minimize(likelihood, (1, 1, 1, 1), args = (k, pi, ics, brem, darkm), bounds = bnds)\n",
    "    min_likelihood = result.x\n",
    "    chi2 = result.fun\n",
    "    #print(result)\n",
    "\n",
    "    return min_likelihood\n",
    "\n",
    "\n",
    "def likelihood(constants, k, pi, ics, brem, dm):\n",
    "    \"\"\"\n",
    "    Gets the Total Likelihoods from the Gaussian Regime and the Poisson Regime\n",
    "    Once each has been calculated, multiplies values together for final likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    likelihood_poiss = likelihood_poisson(constants, k, pi, ics, brem, dm)\n",
    "    #print('likelihood: {}'.format(likelihood_poiss)) \n",
    "    return likelihood_poiss\n",
    "\n",
    "def merge(list1, list2):\n",
    "      \n",
    "    merged_list = tuple(zip(list1, list2)) \n",
    "    return merged_list\n",
    "\n",
    "def likelihood_gaussian(constants, lamb, pi, ics):\n",
    "    sigma = np.sqrt(constants[0]*pi+constants[1]*ics)\n",
    "    mu = lamb\n",
    "    rng = 0.5\n",
    "    \n",
    "    #flatten arrays\n",
    "    sigma_flat = sigma.flatten()\n",
    "    mu_flat = mu.flatten()\n",
    "    length_flattened = len(mu_flat)\n",
    "    #get arrays in sigma, mu tuple format\n",
    "    ms_tuples = list(merge(mu_flat, sigma_flat))\n",
    "    ms = np.array(ms_tuples, dtype = 'f,f')\n",
    "    lower_bound_arr = mu_flat - rng\n",
    "    upper_bound_arr = mu_flat + rng\n",
    "    \n",
    "    args = np.concatenate((np.full((length_flattened, 1), prob_func), lower_bound_arr.reshape((length_flattened, 1)), upper_bound_arr.reshape((length_flattened, 1)), ms.reshape((length_flattened, 1))), axis = 1)\n",
    "    \n",
    "    #log likelihood\n",
    "    prob = list(starmap(lambda a, b, c, d: quad(a, b, c, d)[0], args))\n",
    "\n",
    "    #reshape for testing\n",
    "    l = np.sum(np.log(prob))\n",
    "    likely = -2*l\n",
    "    \n",
    "    return likely\n",
    "    \n",
    "def prob_func(x, mu, sigma):\n",
    "    probdens = 1/(sigma*np.sqrt(2*np.pi))*np.exp(-1/2*((x-sigma**2)/sigma)**2)\n",
    "    return probdens\n",
    "\n",
    "#def likelihood_poisson(a0):\n",
    "def likelihood_poisson(a0, a1, a2, a3, a4):\n",
    "#def likelihood_poisson(constants, ktest, pitest, icstest, bremtest, dmtest):\n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+a3*darkmtest+a4*egbtest\n",
    "    #lamb = a0*egbtest\n",
    "    \n",
    "    #print(a0, a1, a2, a3)\n",
    "    #print(lamb)\n",
    "\n",
    "    #lamb = constants[0]*pitest+constants[1]*icstest+constants[2]*bremtest+constants[3]*dmtest\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    #print(fprob)\n",
    "    return -2*np.nansum(fprob)\n",
    "\n",
    "def get_curves_norm(n, energyidx, inner20psf = True):\n",
    "    icsa = readfile(filelist[n])\n",
    "    data50 = reshape_file(icsa, energyidx, inner20 = inner20psf) #get the data at energyidx MeV\n",
    "    delt = get_deltaE(energyidx)\n",
    "    \n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = central_energies[energyidx]\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    \n",
    "    return np.asarray(data50)*delt\n",
    "    #return np.asarray(data50)*exposure_time*acceptance_forpoisson*delt #try to get in terms of counts per bin\n",
    "\n",
    "def get_curves(n, energyidx, inner20psf = True):\n",
    "    icsa = readfile(filelist[n])\n",
    "    data50 = reshape_file(icsa, energyidx, inner20 = inner20psf) #get the data at energyidx MeV\n",
    "    delt = get_deltaE(energyidx)\n",
    "    \n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = central_energies[energyidx]\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    \n",
    "    return np.asarray(data50)*exposure_time*acceptance_forpoisson*delt #try to get in terms of counts per bin\n",
    "    \n",
    "    \n",
    "def get_normalizations_spectrum(deltaE, cross_sec = 1.4e-26, dm_mass = 40, f_bh = 1, gam = 1, massbh = 2e26):\n",
    "    \n",
    "    range_templates = [2, 4, 0] #pi, ics, brem\n",
    "    energies = np.copy(central_energies)\n",
    "    templates = []\n",
    "    temp = []\n",
    "    #deltaomega = 1/196608 #i think this should maybe still be total bins in whole image pre-20?\n",
    "    deltaomega=1\n",
    "    acceptances = []\n",
    "    for energy_here in range(0, len(energies)):\n",
    "        acceptance_forpoisson = acceptance_interp(energy_here) #in cm^2*str\n",
    "        acceptances.append(acceptance_forpoisson)\n",
    "    \n",
    "\n",
    "    for n in range_templates:\n",
    "        temp = []\n",
    "        for index in range(0, len(energies)):\n",
    "            temp.append(np.nansum(get_curves_norm(n, index))) #units of counts per cm^2 per sec per sr\n",
    "        templates.append(np.asarray(temp)*deltaomega) #gets counts per bin\n",
    "    print('finished new templates!!')\n",
    "    \n",
    "    #eavporating black holes\n",
    "    dmevap_temp = []\n",
    "    dmevap_tot = []\n",
    "    energiesforBH = np.logspace(np.log10(.05), np.log10(1e7), num = 1000) #in MeV\n",
    "    #eventually make it so this is only calculated once per mass_bh\n",
    "    lum = (photon_spectrum.get_integral(egamma_values, mass_bh = massbh)[1]) #units of photons per MeV per sec per BH\n",
    "    #interpolate the luminosity\n",
    "    lum_interp = log_interp1d(energiesforBH, lum, kind='linear') #integrate dn/dE in units of MeV\n",
    "    \n",
    "    for index in range(0, 13):\n",
    "        #print(index)\n",
    "        energybin = energies[index]\n",
    "        bins_in_lin = np.log10(energybin)\n",
    "    \n",
    "    \n",
    "        data50 = dm_template.get_dNdE(egamma_values, index, lum_interp, gamma = gam, mass = massbh, for_normals = True) #units of photons cm^-2 str^-1 per sec\n",
    "        #need to cut this to be inner 20 degrees\n",
    "        \n",
    "        \n",
    "        #dm_temp.append(np.nansum(data50))\n",
    "        dmevap_temp.append(np.nansum(data50)*4*np.pi)#units of photons cm^-2 per sec\n",
    "        #print(np.asarray(dmevap_temp)/.38*f_bh/196608)\n",
    "        #print('--------------------------------------')\n",
    "    dmevap_tot.append(np.asarray(dmevap_temp)/.38*f_bh/196608) #photons per cm^2 per sec per sr\n",
    "\n",
    "        \n",
    "    #dark matter template\n",
    "\n",
    "    dm_temp = []\n",
    "    dm_templates_tot = []\n",
    "    for index in range(0, len(energies)):\n",
    "        #print(index)\n",
    "        energybin = energies[index]\n",
    "        bins_in_lin = np.log10(energybin)\n",
    "        deltae = get_deltaE(index)\n",
    "    \n",
    "        highe = (energybin+deltae)/1e3\n",
    "        lowe = (energybin-deltae)/1e3\n",
    "\n",
    "        data50 = dmj.get_dNdE(highe, lowe, sigmav = cross_sec, massx = dm_mass, for_normals = True, energyhere = energybin/1e3)[1]*deltae #units of photons cm^-2 str^-1 per sec\n",
    "        #need to cut this to be inner 20 degrees\n",
    "        \n",
    "        #dm_temp.append(np.nansum(data50))\n",
    "        dm_temp.append(np.nansum(data50)*4*np.pi) #units of photons cm^-2 per sec\n",
    "        #print(np.asarray(dm_temp)/.38/196608)\n",
    "        #print('--------------------------------------')\n",
    "    dm_templates_tot.append(np.asarray(dm_temp)/.38/196608) #photons per cm^2 per sec per sr, .38 is delta omega\n",
    "    \n",
    "    print('DONE WITH  THE DARK MATTER TEMPLATE')\n",
    "    \n",
    "\n",
    "    \n",
    "    #return range_templates, energies, [np.array(templates[0]), np.array(templates[1]), np.array(templates[2]), np.array(dmevap_tot)]\n",
    "\n",
    "    \n",
    "    \n",
    "    #EGB template\n",
    "    egb_templates = np.array(get_all_egb(energies, deltaE)) #units of counts per cm^2 per sec per str\n",
    "    \n",
    "    #Point Source Template\n",
    "    point_source_arr = []\n",
    "    for index in range(0, len(energies)):\n",
    "        smaller_index = index\n",
    "        point_source_arr.append(np.nansum(get_curves_pointsource(index, smaller_index, inner20psf = True))) ##units of photons per cm^2 per sec per sr\n",
    "    \n",
    "    \n",
    "    print('yay!')\n",
    "        \n",
    "    #return range_templates, energies, [np.array(templates[0]), np.array(templates[1]), np.array(templates[2]), np.array(egb_temp_fin), np.array(dm_templates_tot), np.array(dmevap_tot)] #counts per pixel\n",
    "    #return range_templates, energies, [np.array(templates[0]), np.array(templates[1]), np.array(templates[2]), np.array(egb_templates), np.array(point_source_arr), np.array(dm_templates_tot)]\n",
    "\n",
    "    return range_templates, energies, [np.array(templates[0]), np.array(templates[1]), np.array(templates[2]), np.array(egb_templates), np.array(point_source_arr), np.array(dm_templates_tot), np.array(dmevap_tot)] #counts per pixel\n",
    "    \n",
    "def get_curves_pointsource(energyidx, smallindex, inner20psf = True):\n",
    "    pointsourcedata = readfile(point_sources[0])[smallindex].data\n",
    "    \n",
    "    hdu = readfile(filelist1[0])\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "    degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    ipix_disc20 = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "        \n",
    "    test20 = np.copy(pointsourcedata)[ipix_disc20]\n",
    "    \n",
    "    delt = get_deltaE(energyidx)\n",
    "    \n",
    "    deltaE = get_deltaE(energyidx)\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = central_energies[energyidx]\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    \n",
    "    return np.asarray(test20)*delt #units of photons per cm^2 per sec per sr\n",
    "    \n",
    "    #return np.asarray(test20)*exposure_time*acceptance_forpoisson*delt #13 years * .85 m^2 * .2, return in photons /bin\n",
    "    \n",
    "    \n",
    "def get_normalized(energyidx, normals, template_val, energies):\n",
    "    '''\n",
    "    Normalizes the ROI based on the shape the spectrums should have\n",
    "    \n",
    "    \n",
    "    Do not need to use this, as long as you stay consistent across all Fermi data\n",
    "    for the exposure time and collecting area\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    poisson_pi = poisson_dist(template_val, int(energyidx)) #units of photons per pixel\n",
    "    init_sum_pi = np.nansum(poisson_pi)\n",
    "    #print(np.nansum(init_sum_pi))\n",
    "    if template_val == 2:\n",
    "        normval = 0\n",
    "    if template_val == 4:\n",
    "        normval = 1\n",
    "    if template_val == 0:\n",
    "        normval = 2\n",
    "    print('normval for pi at 0: {}'.format(normals[normval][energyidx])) \n",
    "    print('delta E at 0: {}'.format(get_deltaE(energyidx)))\n",
    "    normal_pi = normals[normval][energyidx]*get_deltaE(energyidx)\n",
    "    pitest = poisson_pi*normal_pi/init_sum_pi\n",
    "    print('normalization: {}'.format(np.nansum(pitest)))\n",
    "    #print(np.nansum(pitest))\n",
    "    \n",
    "    #print('----------------------')\n",
    "    \n",
    "    return pitest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_darksusy_counts():\n",
    "    x = np.loadtxt('yield_DS_keith40.dat', dtype=str).T\n",
    "    energies = x[1].astype(np.float)*1e3\n",
    "    yieldperann = x[2].astype(np.float)/1e3 #convert from per GeV to per MeV\n",
    "    energybins = np.copy(central_energies)\n",
    "    \n",
    "    counts = []\n",
    "    delta = []\n",
    "    for n in range(0, len(energybins)):\n",
    "        \n",
    "        bins_in_lin = energybins[n]\n",
    "        deltae = get_deltaE(n)\n",
    "    \n",
    "        highe = (bins_in_lin+deltae)\n",
    "        lowe = (bins_in_lin-deltae)\n",
    "        \n",
    "        good_energies = np.where((energies <=highe) & (energies >= lowe))\n",
    "        \n",
    "        final_integral = np.trapz(yieldperann[good_energies], x = energies[good_energies])\n",
    "\n",
    "        counts.append(final_integral)\n",
    "        delta.append(deltae)\n",
    "    return np.array(counts), np.array(delta)\n",
    "\n",
    "\n",
    "\n",
    "def log_interp1d(xx, yy, kind='linear'):\n",
    "    logx = np.log10(xx)\n",
    "    logy = np.log10(yy)\n",
    "    lin_interp = sp.interpolate.interp1d(logx, logy, kind=kind, fill_value=\"extrapolate\")\n",
    "    log_interp = lambda zz: np.power(10.0, lin_interp(np.log10(zz)))\n",
    "    return log_interp\n",
    "\n",
    "def get_all_egb_data(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "    return middle_bin, bin_width, egb_intensity\n",
    "\n",
    "def get_all_egb(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "        \n",
    "\n",
    "    log_interp = log_interp1d(middle_bin, egb_intensity/bin_width, kind='linear')\n",
    "    \n",
    "    '''\n",
    "    print(egb_intensity[2]/bin_width[2]*deltae[6])\n",
    "    print(energies[6])\n",
    "    x_trapz = np.logspace(np.log10(np.nanmin(energies)), np.log10(np.nanmax(energies)), num = 100)\n",
    "    plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "    plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "    plt.scatter(energies[6], log_interp(energies[6]), color = 'green')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    '''\n",
    "    \n",
    "    counts = []\n",
    "    #only want energies from 3 onward (about lowest at 80 MeV)\n",
    "    for x in range(0, len(energies)):\n",
    "        highest_val = energies[x]+deltae[x]\n",
    "        lowest_val = energies[x]-deltae[x]\n",
    "        x_trapz = np.logspace(np.log10(lowest_val), np.log10(highest_val), num = 600)\n",
    "        #counts.append(log_interp(energies[x])*deltae[x])\n",
    "        total_counts = np.trapz(log_interp(x_trapz), x = x_trapz)\n",
    "        \n",
    "        #print('total counts: {}'.format(total_counts))\n",
    "        '''\n",
    "        plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "        plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "        plt.scatter(energies[x], total_counts)\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        asdfads\n",
    "        '''\n",
    "        counts.append(total_counts)  \n",
    "    return counts #returns counts per cm^2 per sec per str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, deltae = get_darksusy_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished new templates!!\n",
      "DONE WITH  THE DARK MATTER TEMPLATE\n",
      "yay!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(dmj)\n",
    "importlib.reload(dm_template)\n",
    "\n",
    "#gets darksusy counts for a specific dark matter mass\n",
    "counts, deltae = get_darksusy_counts()\n",
    "#be sure to change the dm mass\n",
    "temps, energies, normals = get_normalizations_spectrum(deltae, cross_sec = 1.4e-26, f_bh = 4e-4, gam = 1.6, massbh = 2e16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acceptances = []\n",
    "cut_energy = np.copy(central_energies)\n",
    "for energyidx in range(0, len(cut_energy)):\n",
    "\n",
    "    energy_here =central_energies[energyidx]\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "labels = ['pi', 'ics', 'brem', 'egb', 'point source', 'dm', 'BH']\n",
    "stile = [\"v\", \"^\", \"s\", \"P\", \"H\", \"d\", \"o\"]\n",
    "\n",
    "fntsz=20\n",
    "plt.close()\n",
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "\n",
    "for i in range(0, len(normals)):\n",
    "    #if ((i == 6) | (i==5)):\n",
    "        #plt.scatter(energies, normals[i]*.38/0.0957/deltae*energies**2, label = labels[i], marker = stile[i], s = 40)\n",
    "    #else:\n",
    "    plt.scatter(energies, normals[i]/deltae*energies**2, label = labels[i], marker = stile[i], s = 40)\n",
    "    \n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 1e3)\n",
    "plt.xlim(1e0, 1e6)\n",
    "plt.axhline(y = 1e3*3.6e-6)\n",
    "#plt.axhline(y = 4.6e-4, color = 'pink')\n",
    "#plt.axhline(y = 1e-3, color = 'brown')\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('test_evap_norms3.pdf')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#code that returns the log-likelihood at a specific point. cube is first parameter, everything else are random parameters\n",
    "#that are not part of the fitting cube\n",
    "\n",
    "#cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest\n",
    "def likelihood_poisson_multinest(cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest):\n",
    "    \n",
    "    a0 = 10**cube[0]\n",
    "    a1 = 10**cube[1]\n",
    "    a2 = 10**cube[2]\n",
    "    a3 = 10**cube[3]\n",
    "    a4 = 10**cube[4]\n",
    "    a5 = 10**cube[5]\n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+egbtest+a3*pointstest+a4*darkmtest + a5*blackholetest\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest\n",
    "def likelihood_poisson_multinest2(cube, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest):\n",
    "    a0 = cube[0]\n",
    "    a1 = cube[1]\n",
    "    a2 = cube[2]\n",
    "    a3 = cube[3]\n",
    "    a4 = cube[4]\n",
    "    a5 = cube[5]\n",
    "    \n",
    "    lamb = a0 * pitest + a1 * icstest + a2 * bremtest + egbtest + a3 * pointstest + a4*darkmtest + blackholetest\n",
    "\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return -2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#finds the log likelihood based on the constants that pymultinest have found. The constants have already\n",
    "#been moved from logspace to linearspace\n",
    "#I do this for every energy bin, and come out with one big log likelihood value for some value of sigmav.\n",
    "#constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest\n",
    "def likelihood_poisson_forsigmav(constants, piflux, icsflux, bremflux, egbflux, pointsflux, ktesthere,\n",
    "                                 darkmatter, blackholetest):\n",
    "    #Constants are my consants for pi, ics, brem, and point sources. the constant in front of egb is 1.\n",
    "    #The 'constant' in front of the dark matter is already incorporated by changing sigmav\n",
    "    #This is my value of lambda\n",
    "    lambd = constants[0]*piflux+constants[1]*icsflux+constants[2]*bremflux+egbflux+constants[3]*pointsflux+constants[4]*darkmatter+constants[5]*blackholetest\n",
    "\n",
    "    \n",
    "    #my value of k (ktest) is the poisson drawn values from sum of all the templates except dm\n",
    "    fprob = -scipy.special.gammaln(ktesthere+1)+ktesthere*np.log(lambd)-lambd #log likelihood of poisson\n",
    "    #scipy.special.gammaln is for the log of a factorial\n",
    "    return -2*np.nansum(fprob) #the sum of all the log likelihoods for each spatial point. \n",
    "\n",
    "##This is the prior function, it is flat in linear space, from a minimum value specified by the first number, \n",
    "#to a maximum number specified by the first number + the second number. \n",
    "#The variables were called a/b/c/phi0/phi1/norm in this code.\n",
    "#You need to define a prior for every parameter you send\n",
    "\n",
    "cube_limits = [(-2, 4), (-2, 4), (-2, 4), (-2, 4), (-2, 4), (-2, 4)]\n",
    "\n",
    "def prior(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*cube_limits[0][1] + cube_limits[0][0])\n",
    "    cube[1] = (cube[1]*cube_limits[1][1] + cube_limits[1][0])\n",
    "    cube[2] = (cube[2]*cube_limits[2][1] + cube_limits[2][0])\n",
    "    cube[3] = (cube[3]*cube_limits[3][1] + cube_limits[3][0])\n",
    "    cube[4] = (cube[4]*cube_limits[4][1] + cube_limits[4][0])\n",
    "    cube[5] = (cube[5]*cube_limits[5][1] + cube_limits[5][0])\n",
    "    return cube\n",
    "\n",
    "##This is the loglikelihood function for multinest – it sends the cube,\n",
    "#and then a bunch of different arrays that were used in fitting, but were constant, to the pymultinest code\n",
    "def loglikelihood_formulti(cube, ndim, nparms):\n",
    "    return likelihood_poisson_multinest(cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#code that returns the log-likelihood at a specific point. cube is first parameter, everything else are random parameters\n",
    "#that are not part of the fitting cube\n",
    "\n",
    "'''\n",
    "BLACK HOLES!!\n",
    "'''\n",
    "\n",
    "#cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest\n",
    "def likelihood_poisson_multinest(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest):\n",
    "    \n",
    "    a0 = 10**cube[0]\n",
    "    a1 = 10**cube[1]\n",
    "    a2 = 10**cube[2]\n",
    "    a3 = 10**cube[3]\n",
    "    \n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+a3*pointstest+egbtest+darkmtest+blackholetest #egb is constant\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisso\n",
    "    #print('---------------------------------')\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest\n",
    "def likelihood_poisson_multinest2(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest):\n",
    "    a0 = cube[0]\n",
    "    a1 = cube[1]\n",
    "    a2 = cube[2]\n",
    "    a3 = cube[3]\n",
    "    \n",
    "    lamb = a0 * pitest + a1 * icstest + a2 * bremtest + a3 * pointstest + egbtest + darkmtest + blackholetest\n",
    "\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return -2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#finds the log likelihood based on the constants that pymultinest have found. The constants have already\n",
    "#been moved from logspace to linearspace\n",
    "#I do this for every energy bin, and come out with one big log likelihood value for some value of sigmav.\n",
    "#likelihood_poisson_forsigmav(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest)\n",
    "def likelihood_poisson_forsigmav(constants, piflux, icsflux, bremflux, egbtest, pointstest, ktesthere, darkmtest, blackholetest):\n",
    "    #Constants are my consants for pi, ics, brem, and point sources. the constant in front of egb is 1.\n",
    "    #The 'constant' in front of the dark matter is already incorporated by changing sigmav\n",
    "    #This is my value of lambda\n",
    "    lambd = constants[0]*piflux+constants[1]*icsflux+constants[2]*bremflux+constants[3]*pointstest+egbtest+darkmtest+blackholetest\n",
    "\n",
    "    \n",
    "    #my value of k (ktest) is the poisson drawn values from sum of all the templates except dm\n",
    "    fprob = -scipy.special.gammaln(ktesthere+1)+ktesthere*np.log(lambd)-lambd #log likelihood of poisson\n",
    "    #scipy.special.gammaln is for the log of a factorial\n",
    "    return -2*np.nansum(fprob) #the sum of all the log likelihoods for each spatial point. \n",
    "\n",
    "##This is the prior function, it is flat in linear space, from a minimum value specified by the first number, \n",
    "#to a maximum number specified by the first number + the second number. \n",
    "#The variables were called a/b/c/phi0/phi1/norm in this code.\n",
    "#You need to define a prior for every parameter you send\n",
    "\n",
    "cube_limits = [(-4, 8), (-4, 8), (-4, 8), (-4, 8)]\n",
    "\n",
    "def prior(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*cube_limits[0][1] + cube_limits[0][0])\n",
    "    cube[1] = (cube[1]*cube_limits[1][1] + cube_limits[1][0])\n",
    "    cube[2] = (cube[2]*cube_limits[2][1] + cube_limits[2][0])\n",
    "    cube[3] = (cube[3]*cube_limits[3][1] + cube_limits[3][0])\n",
    "    \n",
    "##This is the loglikelihood function for multinest – it sends the cube,\n",
    "#and then a bunch of different arrays that were used in fitting, but were constant, to the pymultinest code\n",
    "def loglikelihood_formulti(cube, ndim, nparms):\n",
    "    return likelihood_poisson_multinest(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that returns the log-likelihood at a specific point. cube is first parameter, everything else are random parameters\n",
    "#that are not part of the fitting cube\n",
    "'''\n",
    "\n",
    "#cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest\n",
    "def likelihood_poisson_multinest(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest):\n",
    "    \n",
    "    a0 = 10**cube[0]\n",
    "    a1 = 10**cube[1]\n",
    "    a2 = 10**cube[2]\n",
    "    a3 = 10**cube[3]\n",
    "    a4 = 10**cube[4]\n",
    "    a5 = 10**cube[5]\n",
    "    \n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+a3*pointstest+egbtest+a4*darkmtest+a5*blackholetest #egb is constant\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest\n",
    "def likelihood_poisson_multinest2(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest):\n",
    "    a0 = cube[0]\n",
    "    a1 = cube[1]\n",
    "    a2 = cube[2]\n",
    "    a3 = cube[3]\n",
    "    a4 = cube[4]\n",
    "    a5 = cube[5]\n",
    "    \n",
    "    lamb = a0 * pitest + a1 * icstest + a2 * bremtest + a3 * pointstest + egbtest + a4 * darkmtest + a5 * blackholetest\n",
    "\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return -2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#finds the log likelihood based on the constants that pymultinest have found. The constants have already\n",
    "#been moved from logspace to linearspace\n",
    "#I do this for every energy bin, and come out with one big log likelihood value for some value of sigmav.\n",
    "#likelihood_poisson_forsigmav(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest)\n",
    "def likelihood_poisson_forsigmav(constants, piflux, icsflux, bremflux, egbtest, pointstest, ktesthere, darkmtest, blackholetest):\n",
    "    #Constants are my consants for pi, ics, brem, and point sources. the constant in front of egb is 1.\n",
    "    #The 'constant' in front of the dark matter is already incorporated by changing sigmav\n",
    "    #This is my value of lambda\n",
    "    lambd = constants[0]*piflux+constants[1]*icsflux+constants[2]*bremflux+constants[3]*pointstest+egbtest+constants[4]*darkmtest+ constants[5]*blackholetest\n",
    "\n",
    "    \n",
    "    #my value of k (ktest) is the poisson drawn values from sum of all the templates except dm\n",
    "    fprob = -scipy.special.gammaln(ktesthere+1)+ktesthere*np.log(lambd)-lambd #log likelihood of poisson\n",
    "    #scipy.special.gammaln is for the log of a factorial\n",
    "    return -2*np.nansum(fprob) #the sum of all the log likelihoods for each spatial point. \n",
    "\n",
    "##This is the prior function, it is flat in linear space, from a minimum value specified by the first number, \n",
    "#to a maximum number specified by the first number + the second number. \n",
    "#The variables were called a/b/c/phi0/phi1/norm in this code.\n",
    "#You need to define a prior for every parameter you send\n",
    "\n",
    "cube_limits = [(-8, 10), (-2, 4), (-2, 4), (-2, 4), (-2, 4), (-8, 9)]\n",
    "\n",
    "def prior(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*cube_limits[0][1] + cube_limits[0][0])\n",
    "    cube[1] = (cube[1]*cube_limits[1][1] + cube_limits[1][0])\n",
    "    cube[2] = (cube[2]*cube_limits[2][1] + cube_limits[2][0])\n",
    "    cube[3] = (cube[3]*cube_limits[3][1] + cube_limits[3][0])\n",
    "    cube[4] = (cube[4]*cube_limits[4][1] + cube_limits[4][0])\n",
    "    cube[5] = (cube[5]*cube_limits[5][1] + cube_limits[5][0])\n",
    "    \n",
    "##This is the loglikelihood function for multinest – it sends the cube,\n",
    "#and then a bunch of different arrays that were used in fitting, but were constant, to the pymultinest code\n",
    "def loglikelihood_formulti(cube, ndim, nparms):\n",
    "    return likelihood_poisson_multinest(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "#get all ktests\n",
    "\n",
    "def get_ktests(save= False):\n",
    "\n",
    "    ktest_array = []\n",
    "    importlib.reload(aaa)\n",
    "\n",
    "    energies = np.copy(central_energies)\n",
    "    #cross_sec normalized to 1.4e-26\n",
    "    deltae_cut = np.copy(deltae)\n",
    "\n",
    "    egb_counts = get_all_egb(energies, deltae)/deltae_cut\n",
    "    counting = 0\n",
    "    test_cross = 1.4e-26\n",
    "    blackholem = 2e16\n",
    "    gam = 1.6\n",
    "    fbh = 4e-4\n",
    "\n",
    "    energiesforBH = np.logspace(np.log10(.05), np.log10(1e7), num = 1000) #in MeV\n",
    "    #eventually make it so this is only calculated once per mass_bh\n",
    "    lum = (photon_spectrum.get_integral(egamma_values, mass_bh = blackholem)[1]) #units of photons per MeV per sec per BH\n",
    "    #interpolate the luminosity\n",
    "    lum_interp = log_interp1d(energiesforBH, lum, kind='linear') #integrate dn/dE in units of MeV\n",
    "\n",
    "\n",
    "    for energyidx in range(0, len(energies)):\n",
    "        print(energyidx)\n",
    "        pitest = poisson_dist(2, int(energyidx))\n",
    "        icstest = poisson_dist(4, int(energyidx))\n",
    "        bremtest = poisson_dist(0, int(energyidx))\n",
    "        #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "        egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[energyidx])\n",
    "\n",
    "\n",
    "        #Point Sources\n",
    "        pointstest = poisson_dist(np.nan, energyidx, points = True) \n",
    "\n",
    "\n",
    "        #Dark matter\n",
    "        importlib.reload(dmj) \n",
    "        darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, analyze_data = False)\n",
    "        darkmtest[np.isnan(darkmtest)] = 0\n",
    "\n",
    "        #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "        ktest1 = simulated_data(int(energyidx), [pitest, icstest, bremtest, egbtest, pointstest, darkmtest])\n",
    "        ktest_array.append(ktest1)\n",
    "\n",
    "        counting += 1\n",
    "        \n",
    "    if save:\n",
    "        hdu = fits.PrimaryHDU(ktest_array)\n",
    "        hdulist = fits.HDUList([hdu])\n",
    "        hdulist.writeto('ktestsfile1MeVfin5yrs.fits')\n",
    "    return ktest_array\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ktest_array = get_ktests(save= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktest_array = readfile('ktestsfile1MeVfin5yrs.fits')[0].data\n",
    "print(ktest_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#code that returns the log-likelihood at a specific point. cube is first parameter, everything else are random parameters\n",
    "#that are not part of the fitting cube\n",
    "\n",
    "\n",
    "BLACK HOLES!!\n",
    "\n",
    "\n",
    "#cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest\n",
    "def likelihood_poisson_multinest(cube, *args):\n",
    "    pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest = args\n",
    "    \n",
    "    a0 = 10**cube[0]\n",
    "    a1 = 10**cube[1]\n",
    "    a2 = 10**cube[2]\n",
    "    a3 = 10**cube[3]\n",
    "    \n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+a3*pointstest+egbtest+darkmtest+blackholetest #egb is constant\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisso\n",
    "    #print('---------------------------------')\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest\n",
    "def likelihood_poisson_multinest2(cube, pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest):\n",
    "    a0 = cube[0]\n",
    "    a1 = cube[1]\n",
    "    a2 = cube[2]\n",
    "    a3 = cube[3]\n",
    "    \n",
    "    lamb = a0 * pitest + a1 * icstest + a2 * bremtest + a3 * pointstest + egbtest + darkmtest + blackholetest\n",
    "\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return -2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#finds the log likelihood based on the constants that pymultinest have found. The constants have already\n",
    "#been moved from logspace to linearspace\n",
    "#I do this for every energy bin, and come out with one big log likelihood value for some value of sigmav.\n",
    "#likelihood_poisson_forsigmav(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest)\n",
    "def likelihood_poisson_forsigmav(constants, piflux, icsflux, bremflux, egbtest, pointstest, ktesthere, darkmtest, blackholetest):\n",
    "    #Constants are my consants for pi, ics, brem, and point sources. the constant in front of egb is 1.\n",
    "    #The 'constant' in front of the dark matter is already incorporated by changing sigmav\n",
    "    #This is my value of lambda\n",
    "    lambd = constants[0]*piflux+constants[1]*icsflux+constants[2]*bremflux+constants[3]*pointstest+egbtest+darkmtest+blackholetest\n",
    "\n",
    "    \n",
    "    #my value of k (ktest) is the poisson drawn values from sum of all the templates except dm\n",
    "    fprob = -scipy.special.gammaln(ktesthere+1)+ktesthere*np.log(lambd)-lambd #log likelihood of poisson\n",
    "    #scipy.special.gammaln is for the log of a factorial\n",
    "    return -2*np.nansum(fprob) #the sum of all the log likelihoods for each spatial point. \n",
    "\n",
    "##This is the prior function, it is flat in linear space, from a minimum value specified by the first number, \n",
    "#to a maximum number specified by the first number + the second number. \n",
    "#The variables were called a/b/c/phi0/phi1/norm in this code.\n",
    "#You need to define a prior for every parameter you send\n",
    "\n",
    "cube_limits = [(-4, 8), (-4, 8), (-4, 8), (-4, 8)]\n",
    "\n",
    "def prior(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*cube_limits[0][1] + cube_limits[0][0])\n",
    "    cube[1] = (cube[1]*cube_limits[1][1] + cube_limits[1][0])\n",
    "    cube[2] = (cube[2]*cube_limits[2][1] + cube_limits[2][0])\n",
    "    cube[3] = (cube[3]*cube_limits[3][1] + cube_limits[3][0])\n",
    "    \n",
    "##This is the loglikelihood function for multinest – it sends the cube,\n",
    "#and then a bunch of different arrays that were used in fitting, but were constant, to the pymultinest code\n",
    "def loglikelihood_formulti(cube, ndim, nparms):\n",
    "    return likelihood_poisson_multinest(cube)\n",
    "'''    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loglikeli(MYDIR, energyidx, MYDIR1, templates):\n",
    "    print(MYDIR)\n",
    "    print(energyidx)\n",
    "    print(MYDIR1)\n",
    "\n",
    "    !python3 ./pymultinest_chains/multinest_marginals.py ./{MYDIR1}/chain{energyidx}/{energyidx}\n",
    "    #Open the file\n",
    "    path_to_this_file = MYDIR + '/' + str(energyidx) + 'stats.json'\n",
    "    f = open(path_to_this_file)\n",
    "    filehere = json.load(f)\n",
    "    testpoints0 = (10**(filehere['marginals'][0]['median'])) #pitest\n",
    "    testpoints1 = (10**(filehere['marginals'][1]['median'])) #icstest\n",
    "    testpoints2 = (10**(filehere['marginals'][2]['median'])) #bremtest\n",
    "    testpoints3 = (10**(filehere['marginals'][3]['median'])) #points\n",
    "    \n",
    "    print((filehere['marginals'][0]['median']))\n",
    "    print((filehere['marginals'][1]['median']))\n",
    "    print((filehere['marginals'][2]['median']))\n",
    "    print((filehere['marginals'][3]['median']))\n",
    "          \n",
    "    constants=[testpoints0, testpoints1, testpoints2, testpoints3]\n",
    "    print('constants: ')\n",
    "    print(constants)\n",
    "    #constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest\n",
    "    likehere = likelihood_poisson_forsigmav(constants, templates[0], templates[1], templates[2], templates[3], templates[4], templates[5], templates[6], templates[7])\n",
    "    return likehere\n",
    "\n",
    "def likelihood_poisson_forsigmav(constants, piflux, icsflux, bremflux, egbtest, pointstest, ktesthere, darkmtest, blackholetest):\n",
    "    #Constants are my consants for pi, ics, brem, and point sources. the constant in front of egb is 1.\n",
    "    #The 'constant' in front of the dark matter is already incorporated by changing sigmav\n",
    "    #This is my value of lambda\n",
    "    lambd = constants[0]*piflux+constants[1]*icsflux+constants[2]*bremflux+constants[3]*pointstest+egbtest+darkmtest+blackholetest\n",
    "\n",
    "    \n",
    "    #my value of k (ktest) is the poisson drawn values from sum of all the templates except dm\n",
    "    fprob = -scipy.special.gammaln(ktesthere+1)+ktesthere*np.log(lambd)-lambd #log likelihood of poisson\n",
    "    #scipy.special.gammaln is for the log of a factorial\n",
    "    return -2*np.nansum(fprob) #the sum of all the log likelihoods for each spatial point. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dmj)\n",
    "importlib.reload(dm_template)\n",
    "\n",
    "def get_likelihoodat(fbh, name_of_file = 'testingnew/'):\n",
    "    energies = np.copy(central_energies)\n",
    "    deltae_cut = np.copy(deltae)\n",
    "\n",
    "    test_cross = 1.4e-26\n",
    "    gam = 1.6\n",
    "    blackholem=2e16\n",
    "    likefbh = 0\n",
    "\n",
    "    lum = (photon_spectrum.get_integral(egamma_values, mass_bh = blackholem)[1]) #units of photons per MeV per sec per BH\n",
    "    lum_interp = log_interp1d(energiesforBH, lum, kind='linear') #integrate dn/dE in units of MeV\n",
    "\n",
    "    #first find likelihood where fbh = 0\n",
    "\n",
    "    egb_counts = get_all_egb(energies, deltae)/deltae_cut #units of counts per cm^2 per sec per str per MeV\n",
    "    ##These are the names of the parameters we are fitting.\n",
    "    parameters = ['a0', 'a1', 'a2', 'a3']\n",
    "    folder = './pymultinest_chains/' + str(name_of_file)\n",
    "    livepoints = 200\n",
    "\n",
    "    '''Check if directory exists, if not, create it'''\n",
    "    import os\n",
    "\n",
    "    # You should change 'test' to your preferred folder.\n",
    "    MYDIR1 = (folder + \"fbh\" + str(fbh))\n",
    "    folder_name = \"fbh\" + str(fbh)\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR1)\n",
    "        print(\"created folder : \", MYDIR1)\n",
    "\n",
    "    else:\n",
    "        print(MYDIR1, \"folder already exists.\")\n",
    "\n",
    "\n",
    "\n",
    "    for energyidx in range(0, 13):\n",
    "        print(energyidx)\n",
    "        print('energy here: {}'.format(energies[energyidx]))\n",
    "        # You should change 'test' to your preferred folder.\n",
    "        MYDIR = (MYDIR1 + \"/chain\" + str(energyidx))\n",
    "        CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "        # If folder doesn't exist, then create it.\n",
    "        if not CHECK_FOLDER:\n",
    "            os.makedirs(MYDIR)\n",
    "            print(\"created folder : \", MYDIR)\n",
    "        else:\n",
    "            print(MYDIR, \"folder already exists.\")\n",
    "        print(energyidx, energies[energyidx])\n",
    "\n",
    "        #photons per pixel\n",
    "        pitest = poisson_dist(2, int(energyidx))\n",
    "        icstest = poisson_dist(4, int(energyidx))\n",
    "        bremtest = poisson_dist(0, int(energyidx))\n",
    "\n",
    "        #Point Sources\n",
    "        pointstest = poisson_dist(np.nan, energyidx, points = True)\n",
    "\n",
    "        #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "        egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[energyidx], cross_section = test_cross)\n",
    "\n",
    "        #Dark matter\n",
    "        darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "        darkmtest[np.isnan(darkmtest)] = 0\n",
    "\n",
    "        #Black Holes\n",
    "        blackholetest = poisson_dist(np.nan, int(energyidx), cross_section = test_cross, dm_bh = True, evapbh = True, blackholem = 2e16, luminterpolated = lum_interp, gamm = gam)*fbh\n",
    "        blackholetest[np.isnan(blackholetest)] = 0\n",
    "\n",
    "        #original data\n",
    "        ktest = ktest_array[energyidx] #ktest does not include black holes\n",
    "        \n",
    "        args = (pitest, icstest, bremtest, pointstest, egbtest, darkmtest, blackholetest, ktest)\n",
    "        print('-----------------------')\n",
    "        print(np.nansum(ktest))\n",
    "        print(np.nansum(pitest+ icstest+ bremtest+ pointstest+ egbtest+ darkmtest))\n",
    "        print(np.nansum(pitest+ icstest+ bremtest+ pointstest+ egbtest+ darkmtest+blackholetest))\n",
    "        print('-----------------------')\n",
    "        \n",
    "        cube_limits = [(-4, 8), (-4, 8), (-4, 8), (-4, 8)]\n",
    "        \n",
    "        def prior(cube, ndim, nparams):\n",
    "            #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "            cube[0] = (cube[0]*cube_limits[0][1] + cube_limits[0][0])\n",
    "            cube[1] = (cube[1]*cube_limits[1][1] + cube_limits[1][0])\n",
    "            cube[2] = (cube[2]*cube_limits[2][1] + cube_limits[2][0])\n",
    "            cube[3] = (cube[3]*cube_limits[3][1] + cube_limits[3][0])\n",
    "            \n",
    "    \n",
    "        ##This is the loglikelihood function for multinest – it sends the cube,\n",
    "        #and then a bunch of different arrays that were used in fitting, but were constant, to the pymultinest code\n",
    "        def loglikelihood_formulti(cube, ndim, nparms):\n",
    "            return likelihood_poisson_multinest(cube, ndim, nparms)\n",
    "        \n",
    "        #cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest\n",
    "        def likelihood_poisson_multinest(cube, ndim, nparams):\n",
    "            pi, ics, brem, points, egb, darkm, blackhole, k = args\n",
    "            #print(np.sum(pi+ics+brem+points+egb+darkm+blackhole))\n",
    "            \n",
    "            a0 = 10**cube[0]\n",
    "            a1 = 10**cube[1]\n",
    "            a2 = 10**cube[2]\n",
    "            a3 = 10**cube[3]\n",
    "            \n",
    "            #print(a0, a1, a2, a3)\n",
    "    \n",
    "            lamb = a0*pi+a1*ics+a2*brem+a3*points+egb+darkm+blackhole #egb is constant\n",
    "            fprob = -scipy.special.gammaln(k+1)+k*np.log(lamb)-lamb #log likelihood of poisso\n",
    "            #print('---------------------------------')\n",
    "            return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "        finals = pymultinest.run(loglikelihood_formulti, prior, int(len(parameters)), outputfiles_basename=MYDIR+\"/\"+ str(energyidx), n_live_points=livepoints, resume=True, verbose=True)\n",
    "        json.dump(parameters, open(MYDIR+'/' + str(energyidx) + 'params' +'.json', 'w'))\n",
    "        likelihood = get_loglikeli(MYDIR, energyidx, MYDIR1, [pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest])\n",
    "        print('current likelihood: {}'.format(likelihood))\n",
    "        likefbh += likelihood\n",
    "    print('final likelihood: {}'.format(likefbh))\n",
    "    return likefbh\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like14 = get_likelihoodat(1e-14)\n",
    "print(like14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like0 = get_likelihoodat(0)\n",
    "print(like0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like3 = get_likelihoodat(1e-3)\n",
    "print(like3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def twosig_fbh(testfbh, fbh0):\n",
    "    print(10**testfbh)\n",
    "    return root_find(float(10**testfbh))-4-fbh0\n",
    "\n",
    "def root_find(fbhtesting):\n",
    "    \n",
    "    likefbh = get_likelihoodat(fbhtesting)\n",
    "    print('likelihood here: {}'.format(likefbh))\n",
    "    return np.abs(likefbh)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(like0)\n",
    "\n",
    "import scipy.optimize as opt\n",
    "start = time.time()\n",
    "smalllog = opt.root_scalar(twosig_fbh, args = (like0), x0=float(-9), x1 = float(-5))\n",
    "end = time.time()\n",
    "print('total time: ')\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfbh1 = np.logspace(-10, -2, num = 9)\n",
    "testfbh = np.append(testfbh1, 0)\n",
    "print(testfbh)\n",
    "likelihood_plot1 = []\n",
    "likelihood_plot2 = []\n",
    "likelihood_plot3 = []\n",
    "likelihood_plot4 = []\n",
    "\n",
    "#likelihood_plot1 = readfile('likelihood_plot1.fits')[0].data\n",
    "#likelihood_plot2 = readfile('likelihood_plot2.fits')[0].data\n",
    "#likelihood_plot3 = readfile('likelihood_plot3.fits')[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for frac in testfbh:\n",
    "    likelihood_plot1.append(get_likelihoodat(frac, name_of_file = 'testingnew1/'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihood_plot1)\n",
    "print(likelihood_plot2)\n",
    "print(likelihood_plot3)\n",
    "print(likelihood_plot4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "size = 40\n",
    "plt.hlines(4, 0, 1e3, color = 'blueviolet')\n",
    "plt.scatter(testfbh, np.abs(likelihood_plot1)-likelihood_plot1[-1], color = 'plum', marker = 'v', s = size)\n",
    "plt.scatter(testfbh, np.abs(likelihood_plot2)-likelihood_plot2[-1], color = 'mediumvioletred', marker = '*', s = size)\n",
    "plt.scatter(testfbh, np.abs(likelihood_plot3)-likelihood_plot3[-1], color = 'hotpink', marker = '<', s = size)\n",
    "plt.scatter(testfbh, np.abs(likelihood_plot4)-likelihood_plot4[-1], color = 'lightpink', marker = '8', s = size)\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(8e-11, 3e-2)\n",
    "plt.ylim(-3, 200)\n",
    "plt.xlabel('fbh')\n",
    "plt.ylabel('|2lnL_0-2lnL|')\n",
    "#plt.ylim(np.nanmin(likelihood_plot)*.98, np.nanmax(likelihood_plot)*1.02)\n",
    "plt.savefig('multipletestfbh4.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu = fits.PrimaryHDU(likelihood_plot1)\n",
    "hdulist = fits.HDUList([hdu])\n",
    "hdulist.writeto('likelihood_plot1.fits')\n",
    "hdu = fits.PrimaryHDU(likelihood_plot2)\n",
    "hdulist = fits.HDUList([hdu])\n",
    "hdulist.writeto('likelihood_plot2.fits')\n",
    "hdu = fits.PrimaryHDU(likelihood_plot3)\n",
    "hdulist = fits.HDUList([hdu])\n",
    "hdulist.writeto('likelihood_plot3.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time() \n",
    "for frac in testfbh:\n",
    "    likelihood_plot2.append(get_likelihoodat(frac, name_of_file = 'testingnew2/'))\n",
    "end = time.time()\n",
    "print('total time:')\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frac in testfbh:\n",
    "    likelihood_plot3.append(get_likelihoodat(frac, name_of_file = 'testingnew3/'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frac in testfbh:\n",
    "    likelihood_plot4.append(get_likelihoodat(frac, name_of_file = 'testingnew4/'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihood_plot-like0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.scatter(testfbh, likelihood_plot-like0)\n",
    "plt.hlines(4, 0, 1e3, color = 'green')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(9e-11, 2e-6)\n",
    "plt.ylim(1e-1, 1e3)\n",
    "#plt.ylim(np.nanmin(likelihood_plot)*.98, np.nanmax(likelihood_plot)*1.02)\n",
    "plt.savefig('testfbh.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### import time\n",
    "\n",
    "importlib.reload(dmj)\n",
    "importlib.reload(dm_template)\n",
    "\n",
    "#now need to get the values from the folder, and then calculate the loglikelihood with the dark matter\n",
    "test_crosses_init = np.logspace(np.log10(1e-25), np.log10(1e-29), num = 6)\n",
    "\n",
    "test_crosses = np.concatenate((test_crosses_init, [0]))\n",
    "print(test_crosses)\n",
    "total_likelihood = 0\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "#energies = np.array(list(e)).T[0]\n",
    "energy = np.copy(central_energies)\n",
    "energies = np.copy(central_energies)\n",
    "#cross_sec normalized to 1.4e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae)\n",
    "    \n",
    "#probably do for every value of sigmav:\n",
    "counting_crossec = 0\n",
    "importlib.reload(dmj)\n",
    "\n",
    "likelihood_collection = []\n",
    "test_cross_here = 1.4e-26\n",
    "\n",
    "frac_bh = [1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "frac_bh = [0, 4e-4, 1e-4, 1e-3, 5e-3]\n",
    "\n",
    "#timing\n",
    "start = time.time()\n",
    "\n",
    "test_cross_here = 1.4e-26\n",
    "gam = 1.6\n",
    "blackholem=2e16\n",
    "energiesforBH = np.logspace(np.log10(.05), np.log10(1e7), num = 1000) #in MeV\n",
    "lum = (photon_spectrum.get_integral(egamma_values, mass_bh = blackholem)[1]) #units of photons per MeV per sec per BH\n",
    "#interpolate the luminosity\n",
    "lum_interp = log_interp1d(energiesforBH, lum, kind='linear') #integrate dn/dE in units of MeV\n",
    "\n",
    "#for dark matter, going to choose a cross section, maybe 2.2e-26?\n",
    "\n",
    "for fbh in frac_bh:\n",
    "    #test_cross_here = 1e-26\n",
    "    print('--------------------------')\n",
    "    print('Frac of DM In Black Holes:')\n",
    "    test_cross = float(\"%.5g\" % test_cross_here)\n",
    "    print(fbh)\n",
    "    print('--------------------------')\n",
    "    points = []\n",
    "    errors = []\n",
    "    values = []\n",
    "    fluxes = []\n",
    "    deltae_cut = np.copy(deltae)\n",
    "\n",
    "\n",
    "    #EGB getting now, first get all the counts at each energy bin\n",
    "    egb_counts = get_all_egb(energies, deltae)/deltae_cut #units of counts per cm^2 per sec per str per MeV\n",
    "\n",
    "    ##These are the names of the parameters we are fitting.\n",
    "    parameters = ['a0', 'a1', 'a2', 'a3']#, 'a4']#, 'a5']\n",
    "    #parameters = ['a0', 'a1', 'a2']\n",
    "    folder = './pymultinest_chains/testingnew/'\n",
    "    livepoints = 400\n",
    "\n",
    "    '''Check if directory exists, if not, create it'''\n",
    "    import os\n",
    "\n",
    "    # You should change 'test' to your preferred folder.\n",
    "    MYDIR1 = (folder + \"fbh\" + str(fbh))\n",
    "    folder_name = \"fbh\" + str(fbh)\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR1)\n",
    "        print(\"created folder : \", MYDIR1)\n",
    "\n",
    "    else:\n",
    "        print(MYDIR1, \"folder already exists.\")\n",
    "    \n",
    "    #getting the constants in front of the poission dist. arrays\n",
    "    temp_likelihood = 0\n",
    "    counting = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    #only need to go up to the mass of the BH in MeV\n",
    "    massbh_index = find_nearest(energies, blackholem*5.61e26)\n",
    "    print('highest energy: {}'.format(energies[massbh_index]))\n",
    "    massbh_index = 13\n",
    "    for energyidx in range(0, massbh_index):\n",
    "        print('energy here: {}'.format(energies[energyidx]))\n",
    "        counting = energyidx\n",
    "        #counting = 18-3\n",
    "        # You should change 'test' to your preferred folder.\n",
    "        MYDIR = (MYDIR1 + \"/chain\" + str(energyidx))\n",
    "        CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "        # If folder doesn't exist, then create it.\n",
    "        if not CHECK_FOLDER:\n",
    "            os.makedirs(MYDIR)\n",
    "            print(\"created folder : \", MYDIR)\n",
    "\n",
    "        else:\n",
    "            print(MYDIR, \"folder already exists.\")\n",
    "        print(energyidx, energies[energyidx])\n",
    "        \n",
    "        #photons per pixel\n",
    "        pitest = poisson_dist(2, int(energyidx))\n",
    "        icstest = poisson_dist(4, int(energyidx))\n",
    "        bremtest = poisson_dist(0, int(energyidx))\n",
    "        \n",
    "        #Point Sources\n",
    "        pointstest = poisson_dist(np.nan, energyidx, points = True) \n",
    "        print('points done')\n",
    "        \n",
    "\n",
    "        \n",
    "        #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "        egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[energyidx], cross_section = test_cross)\n",
    "        print('egb done')\n",
    "        #Dark matter\n",
    "        darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "\n",
    "        darkmtest[np.isnan(darkmtest)] = 0\n",
    "        print('darkm done')\n",
    "        \n",
    "        #Black Holes\n",
    "\n",
    "        blackholetest = poisson_dist(np.nan, int(energyidx), cross_section = test_cross, dm_bh = True, evapbh = True, blackholem = 2e16, luminterpolated = lum_interp, gamm = gam)*fbh\n",
    "        blackholetest[np.isnan(blackholetest)] = 0\n",
    "        print('black hole done')\n",
    "\n",
    "        #gotta add egbtest to the ktest\n",
    "        \n",
    "\n",
    "        #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "        ktest = ktest_array[counting] #ktest does not include black holes\n",
    "        print(np.sum(ktest))\n",
    "        print(np.sum(pitest+icstest+bremtest+pointstest+egbtest+darkmtest+blackholetest))\n",
    "        #print(np.sum(pitest+icstest+bremtest+pointstest+egbtest+darkmtest+blackholetest))\n",
    "        \n",
    "        constants = [1, 1, 1, 1]\n",
    "    \n",
    "        lamb = constants[0]*pitest+constants[1]*icstest+constants[2]*bremtest+egbtest+constants[3]*pointstest+darkmtest+blackholetest\n",
    "        #constants, piflux, icsflux, bremflux, egbtest, pointstest, ktesthere, darkmtest\n",
    "        likehere = likelihood_poisson_forsigmav(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest)\n",
    "        print('true likelihood: {}'.format(likehere))\n",
    "        \n",
    "        \n",
    "        finals = pymultinest.run(loglikelihood_formulti, prior, int(len(parameters)), outputfiles_basename=MYDIR+\"/\"+ str(energyidx), n_live_points=livepoints, resume=True, verbose=True)\n",
    "        json.dump(parameters, open(MYDIR+'/' + str(energyidx) + 'params' +'.json', 'w'))\n",
    "        counting += 1\n",
    "        \n",
    "        #now need to find the likelihood for this best fit value\n",
    "        \n",
    "        #create the .data file\n",
    "        !python3 ./pymultinest_chains/multinest_marginals.py ./{MYDIR1}/chain{energyidx}/{energyidx}\n",
    "        #Open the file\n",
    "        path_to_this_file = MYDIR + '/' + str(energyidx) + 'stats.json'\n",
    "        f = open(path_to_this_file)\n",
    "        filehere = json.load(f)\n",
    "        print(path_to_this_file)\n",
    "        constants = []\n",
    "        testpoints0 = (10**(filehere['marginals'][0]['median'])) #pitest\n",
    "        testpoints1 = (10**(filehere['marginals'][1]['median'])) #icstest\n",
    "        testpoints2 = (10**(filehere['marginals'][2]['median'])) #bremtest\n",
    "        testpoints3 = (10**(filehere['marginals'][3]['median'])) #points\n",
    "        #testpoints4 = (10**(filehere['marginals'][4]['median'])) #darkm\n",
    "        #testpoints5 = (10**(filehere['marginals'][5]['median'])) #bhs\n",
    "        print('---------------')\n",
    "        constants=[testpoints0, testpoints1, testpoints2, testpoints3]#, testpoints4]#, testpoints3]#, testpoints4, testpoints5, testpoints6]\n",
    "        #print(constants)\n",
    "        #print(likelihood_poisson_multinest2(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest))\n",
    "        print('---------------')\n",
    "        lamb = constants[0]*pitest+constants[1]*icstest+constants[2]*bremtest+egbtest+constants[3]*pointstest+darkmtest+blackholetest\n",
    "        \n",
    "        likehere = likelihood_poisson_forsigmav(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest)\n",
    "        temp_likelihood += likehere\n",
    "        #temp_likelihood += -2*filehere[\"modes\"][0][\"strictly local log-evidence\"]\n",
    "        print('likelihood final: {}'.format(likehere))\n",
    "        print(likehere)\n",
    "    print('------------')\n",
    "    end = time.time()\n",
    "    print('total time in here:')\n",
    "    print(end - start)\n",
    "    likelihood_collection.append(temp_likelihood)\n",
    "    counting_crossec +=1\n",
    "\n",
    "print('----------')\n",
    "\n",
    "\n",
    "#break\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "end = time.time()\n",
    "print('total time:')\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(frac_bh, likelihood_collection)\n",
    "plt.savefig('likelihoods1.pdf')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 6e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### collect the data from the saved files\n",
    "\n",
    "'''\n",
    "Collect the data from the files created\n",
    "'''\n",
    "def reverse_norm(arrhere, norm, deltae, acceptance_here, error = False):\n",
    "    if error:\n",
    "        normhere = np.copy(norm)\n",
    "        deltaehere = np.copy(deltae)\n",
    "        arr = np.array(arrhere).T\n",
    "        adjusted_arr0 = np.array(arr[0])*np.array(normhere)\n",
    "        adjusted_arr1 = np.array(arr[1])*np.array(normhere)\n",
    "        counts0 = np.array((adjusted_arr0*196608)).T #counts\n",
    "        counts1 = np.array((adjusted_arr1*196608)).T #counts\n",
    "        per0 = counts0/(np.array(exposure_time*acceptance_here)) #counts divided by exposure time divided by acceptance rate\n",
    "        per1 = counts1/(np.array(exposure_time*acceptance_here)) #counts divided by exposure time divided by acceptance rate\n",
    "        fin = np.array([np.array(per0/deltaehere), np.array(per1/deltaehere)])\n",
    "    else:\n",
    "        adjusted_arr = np.array(arrhere)*np.array(norm)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*acceptance_here) #counts divided by exposure time divided by acceptance rate\n",
    "\n",
    "        fin = per/deltae\n",
    "    return np.array(fin) #counts per MeV per sec per str per cm^2\n",
    "\n",
    "def get_errors(middle, index, filehere, val):\n",
    "    sigma1_down = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][0]))\n",
    "    sigma1_up = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][1]))\n",
    "    sigma_gap = 2*np.abs(10**(filehere['marginals'][val]['1sigma'][0])-middle)\n",
    "    \n",
    "    \n",
    "    dist = np.abs(10**(cube_limits[val][0])-middle)\n",
    "    \n",
    "    if sigma_gap > dist:\n",
    "        return [sigma1_down, sigma1_up], True\n",
    "    else:\n",
    "        return [sigma1_down, sigma1_up], False\n",
    "\n",
    "deltae_cut = np.copy(deltae)\n",
    "points0 = []\n",
    "points1 = []\n",
    "points2 = []\n",
    "points3 = []\n",
    "\n",
    "errors0 = []\n",
    "errors1 = []\n",
    "errors2 = []\n",
    "errors3 = []\n",
    "\n",
    "errors0_twosig = []\n",
    "errors1_twosig = []\n",
    "errors2_twosig = []\n",
    "errors3_twosig = []\n",
    "\n",
    "testerr3 = []\n",
    "\n",
    "arrow = []\n",
    "\n",
    "hdu = readfile(filelist[0])\n",
    "energy_here = np.concatenate(hdu[38].data, axis = 0)\n",
    "acceptance_here = acceptance_interp(energy_here)\n",
    "\n",
    "f = []\n",
    "folder = './pymultinest_chains/f_BH_chains_13yrs/'\n",
    "test_crosses = [1.4e-26]\n",
    "mypath = folder\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "print(chains_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think i need to go in order of the energies, not just the list of filenames\n",
    "print(mypath)\n",
    "for fbh in frac_bh:\n",
    "    for number in range(0, len(energies)):\n",
    "        file = 'chain' + str(number)\n",
    "        number = re.sub('\\D', '', str(file))\n",
    "        path_to_this_file = mypath + \"fbh\" + str(fbh) + \"/\" + file + '/' + str(number) + 'stats.json'\n",
    "        #print(path_to_this_file)\n",
    "        f = open(path_to_this_file)\n",
    "        filehere = json.load(f)\n",
    "\n",
    "        testpoints0 = (10**(filehere['marginals'][0]['median']))\n",
    "        testpoints1 = (10**(filehere['marginals'][1]['median']))\n",
    "        testpoints2 = (10**(filehere['marginals'][2]['median']))\n",
    "        testpoints3 = (10**(filehere['marginals'][3]['median']))\n",
    "        #testpoints4 = (10**(filehere['marginals'][4]['median']))\n",
    "        #testpoints5 = (10**(filehere['marginals'][5]['median']))\n",
    "\n",
    "        errors_pi, errors0_twosig = get_errors(testpoints0, 0, filehere, 0)\n",
    "        errors_ics, errors1_twosig = get_errors(testpoints1, 0, filehere, 1)\n",
    "        errors_brem, errors2_twosig = get_errors(testpoints2, 0, filehere, 2)\n",
    "        #errors_dm, errors3_twosig = get_errors(testpoints3, 0, filehere, 3)\n",
    "        #errors_egb, errors4_twosig = get_errors(testpoints4, 0, filehere, 4)\n",
    "        errors_sources, errors3_twosig = get_errors(testpoints3, 0, filehere, 3)\n",
    "\n",
    "        arrow.append([errors0_twosig, errors1_twosig, errors2_twosig, errors3_twosig])\n",
    "\n",
    "        order = 100\n",
    "    \n",
    "        if errors0_twosig == True:\n",
    "            points0.append(10**(filehere['marginals'][0]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][0]['2sigma'][1])\n",
    "            errors0.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points0.append(10**(filehere['marginals'][0]['median']))\n",
    "            errors0.append(errors_pi)\n",
    "        if errors1_twosig == True:\n",
    "            points1.append(10**(filehere['marginals'][1]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][1]['2sigma'][1])\n",
    "            errors1.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points1.append(10**(filehere['marginals'][1]['median']))\n",
    "            errors1.append(errors_ics)\n",
    "        if errors2_twosig == True:\n",
    "            points2.append(10**(filehere['marginals'][2]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][2]['2sigma'][1])\n",
    "            errors2.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points2.append(10**(filehere['marginals'][2]['median']))\n",
    "            errors2.append(errors_brem)\n",
    "        if errors3_twosig == True:\n",
    "            points3.append(10**(filehere['marginals'][3]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][3]['2sigma'][1])\n",
    "            errors3.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points3.append(10**(filehere['marginals'][3]['median']))\n",
    "            errors3.append(errors_sources)\n",
    "            testerr3.append(10**(filehere['marginals'][3]['median']))\n",
    "        '''\n",
    "        if errors4_twosig == True:\n",
    "            points4.append(10**(filehere['marginals'][4]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][4]['2sigma'][1])\n",
    "            errors4.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points4.append(10**(filehere['marginals'][4]['median']))\n",
    "            errors4.append(errors_egb)\n",
    "        if errors5_twosig == True:\n",
    "            points5.append(10**(filehere['marginals'][5]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][5]['2sigma'][1])\n",
    "            errors5.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points5.append(10**(filehere['marginals'][5]['median']))\n",
    "            errors5.append(errors_egb)\n",
    "        '''\n",
    "\n",
    "\n",
    "piflux = reverse_norm(points0, normals[0], deltae_cut, acceptance_here)\n",
    "piflux_err = reverse_norm(errors0, normals[0], deltae_cut, acceptance_here, error = True).T\n",
    "icsflux = reverse_norm(points1, normals[1], deltae_cut, acceptance_here)\n",
    "icsflux_err = reverse_norm(errors1, normals[1], deltae_cut, acceptance_here, error = True).T\n",
    "bremflux = reverse_norm(points2, normals[2], deltae_cut, acceptance_here)\n",
    "bremflux_err = reverse_norm(errors2, normals[2], deltae_cut, acceptance_here, error = True).T\n",
    "#dmflux = reverse_norm(points3, normals[3][0], deltae_cut)\n",
    "#dmfluxtest = reverse_norm(testerr3, normals[3][0], deltae_cut)\n",
    "#dmflux_err = reverse_norm(errors3, normals[3][0], deltae_cut, error = True).T\n",
    "#egbflux = reverse_norm(points3, normals[3], deltae_cut, acceptance_here)\n",
    "#egbflux_err = reverse_norm(errors3, normals[3], deltae_cut, acceptance_here, error = True).T\n",
    "pointsflux = reverse_norm(points3, normals[4], deltae_cut, acceptance_here)\n",
    "pointsflux_err = reverse_norm(errors3, normals[4], deltae_cut, acceptance_here, error = True).T\n",
    "arrow = np.array(arrow).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cut_energy[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[0])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time*acceptance_here/196608\n",
    "cut_energy = np.copy(energies)\n",
    "\n",
    "#Pi0\n",
    "for i in range(0, len(arrow[0])):\n",
    "    if arrow[0][i] == True:\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[i][0]*cut_energy[i]**2, piflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[0][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], piflux[i]*cut_energy[i]**2, color = 'greenyellow', marker = 'X', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[i][0]*cut_energy[i]**2, piflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[0]/units*cut_energy**2, color = 'green')\n",
    "plt.scatter(0, 0, label = r'$\\pi^0$', color = 'greenyellow', marker = 'x', s = sizeofdot)\n",
    "\n",
    "#ICS\n",
    "for i in range(0, len(arrow[1])):\n",
    "    if arrow[1][i] == True:\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[i][0]*cut_energy[i]**2, icsflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[1][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], icsflux[i]*cut_energy[i]**2, color = 'skyblue', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[i][0]*cut_energy[i]**2, icsflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[1]/units*cut_energy**2, color = 'blue')\n",
    "plt.scatter(0, 0, label = 'ICS', color = 'skyblue', marker = 'x', s = sizeofdot)\n",
    "\n",
    "#Bremm\n",
    "for i in range(0, len(arrow[2])):\n",
    "    if arrow[2][i] == True:\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[i][0]*cut_energy[i]**2, bremflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[2][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], bremflux[i]*cut_energy[i]**2, color = 'peachpuff', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[i][0]*cut_energy[i]**2, bremflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[2]/units*cut_energy**2, color = 'darkorange')\n",
    "plt.scatter(0, 0, label = 'Bremm', color = 'peachpuff', marker = '*', s = sizeofdot)\n",
    "\n",
    "#Point sources\n",
    "for i in range(0, len(arrow[3])):\n",
    "    if arrow[3][i] == True:\n",
    "        plt.errorbar(cut_energy[i], pointsflux[i]*cut_energy[i]**2, yerr = np.array([(pointsflux_err[i][0]*cut_energy[i]**2, pointsflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[3][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], pointsflux[i]*cut_energy[i]**2, color = 'orchid', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], pointsflux[i]*cut_energy[i]**2, yerr = np.array([(pointsflux_err[i][0]*cut_energy[i]**2, pointsflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[4]/units*cut_energy**2, color = 'thistle', label = 'Fermi')\n",
    "plt.scatter(0, 0, label = 'Point Sources', color = 'orchid', marker = '>', s = sizeofdot)\n",
    "\n",
    "\n",
    "#EGB\n",
    "plt.scatter(cut_energy, normals[3]/units*cut_energy**2, color = 'darkgoldenrod')\n",
    "plt.scatter(0, 0, label = 'EGB', color = 'gold', marker = '*', s = sizeofdot)\n",
    "\n",
    "\n",
    "#Dark Matter\n",
    "plt.scatter(cut_energy, normals[5][0]/units*cut_energy**2, color = 'red', label = 'DarkSUSY')\n",
    "plt.scatter(0, 0, label = 'DM', color = 'maroon', marker = '>', s = sizeofdot)\n",
    "\n",
    "\n",
    "#Black Holes\n",
    "plt.scatter(cut_energy, normals[6][0]/units*cut_energy**2, color = 'lightblue', label = 'Black Holes')\n",
    "plt.scatter(0, 0, label = 'BHs', color = 'aqua', marker = '*', s = sizeofdot)\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 3e2)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('testing1.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "\n",
    "plt.scatter(test_crosses[0:10], np.abs(np.array(likelihood_collection[0:10])-likelihood_collection[-1]), s = 50)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(np.nanmin(test_crosses)*.95, np.nanmax(test_crosses)*1.5)\n",
    "plt.xlabel(r'$\\sigma$v', fontsize=fntsz)\n",
    "plt.ylabel('2lnL', fontsize=fntsz)\n",
    "#plt.ylim(0, 10)\n",
    "plt.xlim(1e-30, 1e-25)\n",
    "plt.ylim(1e-1, 1e6)\n",
    "plt.hlines(4, 1e-30, 1e-23)\n",
    "plt.show()\n",
    "#plt.savefig('loglikelihood_zoomtest1.pdf')\n",
    "\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = ktest_array[12]\n",
    "\n",
    "\n",
    "\n",
    "energyidx = 12+3\n",
    "counting = 12\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "\n",
    "egb_counts = get_all_egb(energies, deltae)/deltae_cut\n",
    "\n",
    "#photons per pixel\n",
    "pitest = poisson_dist(2, int(energyidx), cross_section = test_cross)\n",
    "icstest = poisson_dist(4, int(energyidx), cross_section = test_cross)\n",
    "bremtest = poisson_dist(0, int(energyidx), cross_section = test_cross)\n",
    "\n",
    "\n",
    "#EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[counting], cross_section = test_cross)\n",
    "\n",
    "importlib.reload(dmj) \n",
    "darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "\n",
    "darkmtest[np.isnan(darkmtest)] = 0\n",
    "\n",
    "#Point Sources\n",
    "pointstest = poisson_dist(np.nan, energyidx, points = True, cross_section = test_cross) \n",
    "\n",
    "vals_w_dm = np.copy(lamb)\n",
    "\n",
    "print(test_cross)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energybin = np.concatenate(readfile(filelist[0])[38].data, axis = 0)[energyidx]\n",
    "bins_in_lin = np.log10(energybin)\n",
    "deltae_here = get_deltaE(energyidx)\n",
    "    \n",
    "highe = (energybin+deltae_here)/1e3\n",
    "lowe = (energybin-deltae_here)/1e3\n",
    "    \n",
    "hdu = readfile(filelist[0])\n",
    "numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "    \n",
    "    \n",
    "#need to make sure the initsum is *only within the inner 20 degrees, same for finsum\n",
    "data50 = dmj.get_dNdE(highe, lowe, sigmav = test_cross, analyze_data = False, massx = 100)[1] #photons per cm^2 per sec per str per MeV\n",
    "    \n",
    "#get_where_within_20deg\n",
    "numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "NSIDE = int(hdu[0].header['NSIDE'])\n",
    "vec = hp.ang2vec(np.pi/2, 0)\n",
    "ipix_disc = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "\n",
    "ipix_disc_big = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(360), inclusive = False)\n",
    "\n",
    "data50[ipix_disc_big] = np.nan\n",
    "\n",
    "data501 = np.copy(data50)\n",
    "data502 = np.copy(data50)\n",
    "\n",
    "data501[ipix_disc] = true_values\n",
    "data502[ipix_disc] = vals_w_dm\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview((data501), coord = 'G')\n",
    "\n",
    "hp.mollview((data502), coord = 'G')\n",
    "\n",
    "hp.mollview((np.abs(data501-data502)), coord = 'G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipix_disc1 = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(1), inclusive = False)\n",
    "print(ipix_disc1)\n",
    "print(data50[99071])\n",
    "print(np.where(ipix_disc == 99071))\n",
    "\n",
    "print(pitest[3053])\n",
    "print(bremtest[3053])\n",
    "print(egbtest[3053])\n",
    "print(pointstest[3053])\n",
    "print(icstest[3053])\n",
    "print(darkmtest[3053])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data501)\n",
    "print(data502)\n",
    "\n",
    "plt.hist(np.abs(data501-data502), bins = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crosses_init = np.logspace(np.log10(1e-22), np.log10(1e-27), num = 20)\n",
    "test_crosses = np.concatenate((test_crosses_init, [0]))\n",
    "test_cross = test_crosses_init[0]\n",
    "counting = 0\n",
    "for energyidx in range(3, len(energies)):        \n",
    "    #photons per pixel\n",
    "    pitest = poisson_dist(2, int(energyidx))\n",
    "    icstest = poisson_dist(4, int(energyidx))\n",
    "    bremtest = poisson_dist(0, int(energyidx))\n",
    "\n",
    "\n",
    "    #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "    egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[counting])\n",
    "\n",
    "    importlib.reload(dmj) \n",
    "    darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "\n",
    "    darkmtest[np.isnan(darkmtest)] = 0\n",
    "\n",
    "    #Point Sources\n",
    "    pointstest = poisson_dist(np.nan, energyidx, points = True) \n",
    "\n",
    "\n",
    "    #gotta add egbtest to the ktest\n",
    "\n",
    "    #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "    ktest = simulated_data(int(energyidx), [pitest, icstest, bremtest, egbtest, pointstest, darkmtest])\n",
    "    tot = pitest+icstest+bremtest+egbtest+pointstest+darkmtest\n",
    "    \n",
    "    plt.hist(ktest, color = 'red')\n",
    "    plt.hist(tot, color = 'blue')\n",
    "    counting += 1\n",
    "    sdfasd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(likelihood_collection[0:10]-likelihood_collection[-1]))\n",
    "print(likelihood_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "\n",
    "plt.scatter(test_crosses[0:10], np.abs(np.array(likelihood_collection[0:10]-likelihood_collection[-1])), s = 50)\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(np.nanmin(test_crosses)*.95, np.nanmax(test_crosses)*1.5)\n",
    "plt.xlabel(r'$\\sigma$v', fontsize=fntsz)\n",
    "plt.ylabel('2lnL', fontsize=fntsz)\n",
    "#plt.ylim(0, 10)\n",
    "plt.xlim(1e-27, 1e-20)\n",
    "plt.ylim(0, 1e2)\n",
    "#plt.savefig('loglikelihood_zoom.pdf')\n",
    "\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to get the log likelihoods another way\n",
    "import os\n",
    "\n",
    "test_crosses_init = np.logspace(np.log10(1e-22), np.log10(1e-27), num = 10)\n",
    "test_crosses = np.concatenate((test_crosses_init, [0]))\n",
    "\n",
    "likelihoods_test2 = []\n",
    "\n",
    "for test_cross in test_crosses:\n",
    "    # You should change 'test' to your preferred folder.\n",
    "    MYDIR1 = (folder + \"cross_sec\" + str(test_cross))\n",
    "    folder_name = \"cross_sec\" + str(test_cross)\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "    templike = 0\n",
    "    for energyidx in range(3, len(energies)):\n",
    "        MYDIR = (MYDIR1 + \"/chain\" + str(energyidx))\n",
    "        path_to_this_file = MYDIR + '/' + str(energyidx) + 'stats.json'\n",
    "        f = open(path_to_this_file)\n",
    "        filehere = json.load(f)\n",
    "        likeli = -filehere['modes'][0]['strictly local log-evidence']\n",
    "        templike += likeli\n",
    "    likelihoods_test2.append(templike)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihoods_test2[0:10])\n",
    "print(likelihoods_test2[-1])\n",
    "print(np.array(likelihoods_test2[0:10])-likelihoods_test2[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "\n",
    "plt.scatter(test_crosses[0:10], np.array(np.array(likelihoods_test2[0:10])-float(likelihoods_test2[-1])), s = 50)\n",
    "plt.scatter(test_crosses[0:10], np.array(np.array(likelihood_collection[0:10])-float(likelihood_collection[-1])), s = 50, color = 'red')\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(np.nanmin(test_crosses)*.95, np.nanmax(test_crosses)*1.5)\n",
    "plt.xlabel(r'$\\sigma$v', fontsize=fntsz)\n",
    "plt.ylabel('2lnL', fontsize=fntsz)\n",
    "#plt.ylim(0, 10)\n",
    "\n",
    "plt.savefig('loglikelihood1.pdf')\n",
    "plt.xlim(1e-27, 1e-22)\n",
    "plt.ylim(0, 10)\n",
    "#plt.ylim(1e-1, 1e3)\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))\n",
    "plt.savefig('weird_loglikelihood.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best fit\n",
    "\n",
    "#now find best fit for DM\n",
    "\n",
    "def likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux):\n",
    "    a0 = 10**cube[0]\n",
    "    lamb = piflux+icsflux+bremflux+egbflux+sourcesflux+a0*dmflux\n",
    "    fprob = -scipy.special.gammaln(kflux+1)+kflux*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "def prior_bestfit(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*2 -1)\n",
    "    return cube\n",
    "\n",
    "def loglikelihood_fordmbestfit(cube, ndim, nparms):\n",
    "    return likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux)\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae[3:])\n",
    "\n",
    "\n",
    "##These are the names of the parameters we are fitting.\n",
    "parameters = ['a0']\n",
    "folder = './pymultinest_chains/'\n",
    "livepoints = 200\n",
    "\n",
    "'''Check if directory exists, if not, create it'''\n",
    "import os\n",
    "\n",
    "# You should change 'test' to your preferred folder.\n",
    "MYDIR1 = (folder + \"bestfit_dm\")\n",
    "CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(MYDIR1)\n",
    "    print(\"created folder : \", MYDIR1)\n",
    "\n",
    "else:\n",
    "    print(MYDIR1, \"folder already exists.\")\n",
    "dmflux = (get_dm_array(cross_section = 2.2e-26, dm_mass = 100)*cut_energy**2)[0]\n",
    "print('got DM flux.')\n",
    "kflux = piflux+icsflux+bremflux+egbflux+sourcesflux\n",
    "\n",
    "finals = pymultinest.run(loglikelihood_fordmbestfit, prior_bestfit, int(len(parameters)), outputfiles_basename=MYDIR1+\"/bestfitDM\", n_live_points=livepoints, resume=True, verbose=True)\n",
    "\n",
    "json.dump(parameters, open(MYDIR1+'/' + 'params' +'.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now need to get the minimal sigmav\n",
    "mypath = './pymultinest_chains/bestfit_dm/'\n",
    "\n",
    "!python3 ./pymultinest_chains/multinest_marginals.py ./pymultinest_chains/bestfit_dm/bestfitDM\n",
    "number = re.sub('\\D', '', str(file))\n",
    "path_to_this_file = mypath + '/' + 'bestfitDM' + 'stats.json'\n",
    "f = open(path_to_this_file)\n",
    "filehere = json.load(f)\n",
    "print(path_to_this_file)\n",
    "bestfit_sigmav = (10**(filehere['marginals'][0]['median']))\n",
    "\n",
    "print(bestfit_sigmav*2.2e-26)\n",
    "\n",
    "ktesthere = piflux+icsflux+bremflux+egbflux+sourcesflux\n",
    "darkmatter_min = (get_dm_array(cross_section = bestfit_sigmav*2.2e-26, dm_mass = 100)*cut_energy**2)[0]\n",
    "min_loglikelihood = likelihood_poisson_forsigmav(piflux, icsflux, bremflux, egbflux, sourcesflux, ktesthere, darkmatter_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_energy = energies[3:]\n",
    "darkmatter = get_dm_array(cross_section = 1e-23, dm_mass = 100)*cut_energy**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmax(darkmatter[0]))\n",
    "print(darkmatter[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ktest_array))\n",
    "print(len(cut_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(cut_energy, egbflux, color = 'red')\n",
    "plt.scatter(cut_energy, darkmatter, color = 'orange')\n",
    "plt.scatter(cut_energy[14], darkmatter[0][14], color = 'black', s = 50)\n",
    "plt.scatter(cut_energy, piflux, color = 'yellow')\n",
    "plt.scatter(cut_energy, icsflux, color = 'green')\n",
    "plt.scatter(cut_energy, bremflux, color = 'blue')\n",
    "plt.scatter(cut_energy, sourcesflux, color = 'purple')\n",
    "plt.scatter(cut_energy[14], darkmatter[0][14], color = 'black', s = 100)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylim(1e-5, 1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to find best fit to dark matter\n",
    "#basically just take our best fit values from the 5 templates, and fit them with the dm template\n",
    "'''\n",
    "Collect the data from the files created\n",
    "'''\n",
    "\n",
    "def reverse_norm(arr, norm, deltae, accept, error = False):\n",
    "    \n",
    "    if error:\n",
    "        normhere = norm.reshape(35, 1)\n",
    "        deltaehere = deltae.reshape(35, 1)\n",
    "        adjusted_arr = np.array(arr)*np.array(normhere)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltaehere\n",
    "    else:\n",
    "        adjusted_arr = np.array(arr)*np.array(norm)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time**accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltae\n",
    "    return fin #counts per MeV per sec per str per cm^2\n",
    "\n",
    "        \n",
    "points0 = []\n",
    "points1 = []\n",
    "points2 = []\n",
    "points3 = []\n",
    "points4 = []\n",
    "points5 = []\n",
    "\n",
    "arrow = []\n",
    "\n",
    "f = []\n",
    "mypath = './pymultinest_chains/6templates_13yrs/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "\n",
    "for number in range(3, len(energies)):\n",
    "    file = 'chain' + str(number)\n",
    "    number = re.sub('\\D', '', str(file))\n",
    "    path_to_this_file = mypath + file + '/' + str(number) + 'stats.json'\n",
    "    f = open(path_to_this_file)\n",
    "    filehere = json.load(f)\n",
    "    print(path_to_this_file)\n",
    "\n",
    "    \n",
    "    testpoints0 = (10**(filehere['marginals'][0]['median']))\n",
    "    testpoints1 = (10**(filehere['marginals'][1]['median']))\n",
    "    testpoints2 = (10**(filehere['marginals'][2]['median']))\n",
    "    testpoints3 = (10**(filehere['marginals'][3]['median']))\n",
    "    testpoints4 = (10**(filehere['marginals'][4]['median']))\n",
    "    \n",
    "    points0.append(testpoints0)\n",
    "    points1.append(testpoints1)\n",
    "    points2.append(testpoints2)\n",
    "    points3.append(testpoints3)\n",
    "    points4.append(testpoints4)\n",
    "\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "\n",
    "acceptances = []\n",
    "for energyidxhere in range(0, len(cut_energy))\n",
    "    energyidx = energyidxhere+3\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson)\n",
    "\n",
    "piflux = reverse_norm(points0, normals[0], deltae_cut, acceptances)*cut_energy**2\n",
    "icsflux = reverse_norm(points1, normals[1], deltae_cut, acceptances)*cut_energy**2\n",
    "bremflux = reverse_norm(points2, normals[2], deltae_cut, acceptances)*cut_energy**2\n",
    "egbflux = reverse_norm(points3, normals[3], deltae_cut, acceptances)*cut_energy**2\n",
    "sourcesflux = reverse_norm(points4, normals[4], deltae_cut, acceptances)*cut_energy**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dmflux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now find best fit for DM\n",
    "\n",
    "def likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux):\n",
    "    a0 = 10**cube[0]\n",
    "    lamb = piflux+icsflux+bremflux+egbflux+sourcesflux+a0*dmflux\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "def prior_bestfit(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*2 -1)\n",
    "    return cube\n",
    "\n",
    "def loglikelihood_fordmbestfit(cube, ndim, nparms):\n",
    "    return likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux)\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae[3:])\n",
    "\n",
    "\n",
    "##These are the names of the parameters we are fitting.\n",
    "parameters = ['a0']\n",
    "folder = './pymultinest_chains/'\n",
    "livepoints = 200\n",
    "\n",
    "'''Check if directory exists, if not, create it'''\n",
    "import os\n",
    "\n",
    "# You should change 'test' to your preferred folder.\n",
    "MYDIR1 = (folder + \"bestfit_dm\")\n",
    "CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(MYDIR1)\n",
    "    print(\"created folder : \", MYDIR1)\n",
    "\n",
    "else:\n",
    "    print(MYDIR1, \"folder already exists.\")\n",
    "    \n",
    "kflux = piflux+icsflux+bremflux+egbflux+sourcesflux\n",
    "finals = pymultinest.run(loglikelihood_fordmbestfit, prior, int(len(parameters)), outputfiles_basename=MYDIR+\"/\"+ str(energyidx), n_live_points=livepoints, resume=True, verbose=True)\n",
    "\n",
    "#now need to get the minimal sigmav\n",
    "mypath = './pymultinest_chains/bestfit_dm/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "\n",
    "file = 'chain' + str(number)\n",
    "number = re.sub('\\D', '', str(file))\n",
    "path_to_this_file = mypath + file + '/' + str(number) + 'stats.json'\n",
    "f = open(path_to_this_file)\n",
    "filehere = json.load(f)\n",
    "print(path_to_this_file)\n",
    "bestfit_sigmav = (10**(filehere['marginals'][0]['median']))\n",
    "\n",
    "print(bestfit_sigmav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(test_crosses, likelihood_collection)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1e-24, 1e-28)\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3, len(energies)):\n",
    "    !python3 ./pymultinest_chains/multinest_marginals.py ./pymultinest_chains/6templates_13yrs/chain{idx}/{idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collect the data from the files created\n",
    "'''\n",
    "\n",
    "def reverse_norm(arr, norm, deltae, accept, error = False):\n",
    "    if error:\n",
    "        normhere = norm.reshape(35, 1)\n",
    "        deltaehere = deltae.reshape(35, 1)\n",
    "        adjusted_arr = np.array(arr)*np.array(normhere)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltaehere\n",
    "    else:\n",
    "        adjusted_arr = np.array(arr)*np.array(norm)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltae\n",
    "    return fin #counts per MeV per sec per str per cm^2\n",
    "\n",
    "def get_errors(middle, index, filehere, val):\n",
    "    sigma1_down = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][0]))\n",
    "    sigma1_up = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][1]))\n",
    "    sigma_gap = 2*np.abs(10**(filehere['marginals'][val]['1sigma'][0])-middle)\n",
    "    \n",
    "    \n",
    "    dist = np.abs(10**(cube_limits[val][0])-middle)\n",
    "    \n",
    "    if sigma_gap > dist:\n",
    "        return [sigma1_down, sigma1_up], True\n",
    "    else:\n",
    "        return [sigma1_down, sigma1_up], False\n",
    "        \n",
    "points0 = []\n",
    "points1 = []\n",
    "points2 = []\n",
    "points3 = []\n",
    "points4 = []\n",
    "points5 = []\n",
    "\n",
    "errors0 = []\n",
    "errors1 = []\n",
    "errors2 = []\n",
    "errors3 = []\n",
    "errors4 = []\n",
    "errors5 = []\n",
    "\n",
    "errors0_twosig = []\n",
    "errors1_twosig = []\n",
    "errors2_twosig = []\n",
    "errors3_twosig = []\n",
    "errors4_twosig = []\n",
    "errors5_twosig = []\n",
    "\n",
    "testerr3 = []\n",
    "\n",
    "arrow = []\n",
    "\n",
    "f = []\n",
    "mypath = './pymultinest_chains/6templates_13yrs/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "\n",
    "#for i in range(3, len(energies)):\n",
    "    #x = (np.loadtxt(MYDIR + '/params' + str(i) + 'stats.dat', skiprows = 4, dtype = 'float', max_rows = 5).T)\n",
    "\n",
    "# i think i need to go in order of the energies, not just the list of filenames\n",
    "for number in range(3, len(energies)):\n",
    "    file = 'chain' + str(number)\n",
    "    number = re.sub('\\D', '', str(file))\n",
    "    path_to_this_file = mypath + file + '/' + str(number) + 'stats.json'\n",
    "    f = open(path_to_this_file)\n",
    "    filehere = json.load(f)\n",
    "    print(path_to_this_file)\n",
    "    '''\n",
    "    points0.append((filehere['marginals'][0]['median']))\n",
    "    points1.append((filehere['marginals'][1]['median']))\n",
    "    points2.append((filehere['marginals'][2]['median']))\n",
    "    points3.append((filehere['marginals'][3]['median']))\n",
    "    points4.append((filehere['marginals'][4]['median']))\n",
    "    errors0.append((filehere['marginals'][0]['sigma']))\n",
    "    errors1.append((filehere['marginals'][1]['sigma']))\n",
    "    errors2.append((filehere['marginals'][2]['sigma']))\n",
    "    errors3.append((filehere['marginals'][3]['sigma']))\n",
    "    errors4.append((filehere['marginals'][4]['sigma']))\n",
    "    '''\n",
    "    \n",
    "    testpoints0 = (10**(filehere['marginals'][0]['median']))\n",
    "    testpoints1 = (10**(filehere['marginals'][1]['median']))\n",
    "    testpoints2 = (10**(filehere['marginals'][2]['median']))\n",
    "    testpoints3 = (10**(filehere['marginals'][3]['median']))\n",
    "    testpoints4 = (10**(filehere['marginals'][4]['median']))\n",
    "    testpoints5 = (10**(filehere['marginals'][5]['median']))\n",
    "\n",
    "    errors_pi, errors0_twosig = get_errors(testpoints0, 0, filehere, 0)\n",
    "    errors_ics, errors1_twosig = get_errors(testpoints1, 0, filehere, 1)\n",
    "    errors_brem, errors2_twosig = get_errors(testpoints2, 0, filehere, 2)\n",
    "    errors_dm, errors3_twosig = get_errors(testpoints3, 0, filehere, 3)\n",
    "    errors_egb, errors4_twosig = get_errors(testpoints4, 0, filehere, 4)\n",
    "    errors_sources, errors5_twosig = get_errors(testpoints5, 0, filehere, 4)\n",
    "    \n",
    "\n",
    "    \n",
    "    arrow.append([errors0_twosig, errors1_twosig, errors2_twosig, errors3_twosig, errors4_twosig, errors5_twosig])\n",
    "    \n",
    "    #want line to be one order of magnitude below the top part of the line\n",
    "    order = 100\n",
    "    \n",
    "    if errors0_twosig == True:\n",
    "        points0.append(10**(filehere['marginals'][0]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][0]['2sigma'][1])\n",
    "        errors0.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points0.append(10**(filehere['marginals'][0]['median']))\n",
    "        errors0.append(errors_pi)\n",
    "    if errors1_twosig == True:\n",
    "        points1.append(10**(filehere['marginals'][1]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][1]['2sigma'][1])\n",
    "        errors1.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points1.append(10**(filehere['marginals'][1]['median']))\n",
    "        errors1.append(errors_ics)\n",
    "    if errors2_twosig == True:\n",
    "        points2.append(10**(filehere['marginals'][2]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][2]['2sigma'][1])\n",
    "        errors2.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points2.append(10**(filehere['marginals'][2]['median']))\n",
    "        errors2.append(errors_brem)\n",
    "    if errors3_twosig == True:\n",
    "        points3.append(10**(filehere['marginals'][3]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][3]['2sigma'][1])\n",
    "        errors3.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points3.append(10**(filehere['marginals'][3]['median']))\n",
    "        errors3.append(errors_dm)\n",
    "    testerr3.append(10**(filehere['marginals'][3]['median']))\n",
    "    if errors4_twosig == True:\n",
    "        points4.append(10**(filehere['marginals'][4]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][4]['2sigma'][1])\n",
    "        errors4.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points4.append(10**(filehere['marginals'][4]['median']))\n",
    "        errors4.append(errors_egb)\n",
    "    if errors5_twosig == True:\n",
    "        points5.append(10**(filehere['marginals'][5]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][5]['2sigma'][1])\n",
    "        errors5.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points5.append(10**(filehere['marginals'][5]['median']))\n",
    "        errors5.append(errors_egb)\n",
    "    \n",
    "    #cube_limits = [(-2, 4), (-2, 4), (-3, 6), (-6, 12), (-3, 6)]\n",
    "    #print(int(number))\n",
    "    \n",
    "acceptances = []\n",
    "for energyidxhere in range(0, len(cut_energy))\n",
    "    energyidx = energyidxhere+3\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson) \n",
    "    \n",
    "piflux = reverse_norm(points0, normals[0], deltae_cut, acceptances)\n",
    "piflux_err = reverse_norm(errors0, normals[0], deltae_cut, acceptances, error = True).T\n",
    "icsflux = reverse_norm(points1, normals[1], deltae_cut, acceptances, )\n",
    "icsflux_err = reverse_norm(errors1, normals[1], deltae_cut, acceptances,  error = True).T\n",
    "bremflux = reverse_norm(points2, normals[2], deltae_cut, acceptances)\n",
    "bremflux_err = reverse_norm(errors2, normals[2], deltae_cut, acceptances, error = True).T\n",
    "dmflux = reverse_norm(points3, normals[3][0], deltae_cut, acceptances)\n",
    "dmfluxtest = reverse_norm(testerr3, normals[3][0], deltae_cut, acceptances)\n",
    "dmflux_err = reverse_norm(errors3, normals[3][0], deltae_cut, acceptances, error = True).T\n",
    "egbflux = reverse_norm(points4, normals[4], deltae_cut, acceptances)\n",
    "egbflux_err = reverse_norm(errors4, normals[4], deltae_cut, acceptances, error = True).T\n",
    "sourcesflux = reverse_norm(points5, normals[5], deltae_cut, acceptances)\n",
    "sourcesflux_err = reverse_norm(errors5, normals[5], deltae_cut, acceptances, error = True).T\n",
    "arrow = np.array(arrow).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptances = []\n",
    "for energyidxhere in range(0, len(cut_energy))\n",
    "    energyidx = energyidxhere+3\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = np.copy(energies)\n",
    "\n",
    "#Pi0\n",
    "for i in range(0, len(arrow[0])):\n",
    "    if arrow[0][i] == True:\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[0][i]*cut_energy[i]**2, piflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[0][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], piflux[i]*cut_energy[i]**2, color = 'greenyellow', marker = 'X', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[0][i]*cut_energy[i]**2, piflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[0]/units*cut_energy**2, color = 'green')\n",
    "plt.scatter(0, 0, label = r'$\\pi^0$', color = 'greenyellow', marker = 'x', s = sizeofdot)\n",
    "\n",
    "\n",
    "\n",
    "#ICS\n",
    "for i in range(0, len(arrow[1])):\n",
    "    if arrow[1][i] == True:\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[0][i]*cut_energy[i]**2, icsflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[1][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], icsflux[i]*cut_energy[i]**2, color = 'skyblue', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[0][i]*cut_energy[i]**2, icsflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[1]/units*cut_energy**2, color = 'blue')\n",
    "plt.scatter(0, 0, label = 'ICS', color = 'skyblue', marker = 'x', s = sizeofdot)\n",
    "\n",
    "\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.049139889547365305), normals[1][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(-0.19407879963313424), normals[1][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(-0.054794496088809604), normals[1][27], deltae_cut[27])*cut_energy[27]**2, color = 'purple')\n",
    "\n",
    "\n",
    "#Bremm\n",
    "for i in range(0, len(arrow[2])):\n",
    "    if arrow[2][i] == True:\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[0][i]*cut_energy[i]**2, bremflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[2][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], bremflux[i]*cut_energy[i]**2, color = 'peachpuff', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[0][i]*cut_energy[i]**2, bremflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[2]/units*cut_energy**2, color = 'darkorange')\n",
    "plt.scatter(0, 0, label = 'Bremm', color = 'peachpuff', marker = '*', s = sizeofdot)\n",
    "\n",
    "\n",
    "#Dark matter\n",
    "for i in range(0, len(arrow[3])):\n",
    "    if arrow[3][i] == True:\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[3][i], capsize = 4, color = 'maroon', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], dmflux[i]*cut_energy[i]**2, color = 'maroon', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[3]/units*cut_energy**2, color = 'red', label = 'DarkSUSY')\n",
    "plt.scatter(0, 0, label = 'DM', color = 'maroon', marker = '>', s = sizeofdot)\n",
    "\n",
    "\n",
    "#EGB\n",
    "for i in range(0, len(arrow[4])):\n",
    "    if arrow[4][i] == True:\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[4][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], egbflux[i]*cut_energy[i]**2, color = 'orchid', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[4]/units*cut_energy**2, color = 'thistle', label = 'Fermi')\n",
    "plt.scatter(0, 0, label = 'EGB', color = 'orchid', marker = '>', s = sizeofdot)\n",
    "\n",
    "#Point Sources\n",
    "for i in range(0, len(arrow[5])):\n",
    "    if arrow[5][i] == True:\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[5][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, color = 'gold', marker = '*', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[5]/units*cut_energy**2, color = 'darkgoldenrod')\n",
    "plt.scatter(0, 0, label = 'Point Sources', color = 'gold', marker = '*', s = sizeofdot)\n",
    "\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.1060309379367271), normals[4][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.013060946061642112), normals[4][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.058146620211512644), normals[4][27], deltae_cut[27])*cut_energy[27]**2, color = 'purple')\n",
    "\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 3e1)\n",
    "plt.legend()\n",
    "plt.savefig('images/6templates1_13yrs.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "#Point Sources\n",
    "for i in range(0, len(arrow[5])):\n",
    "    if arrow[5][i] == True:\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[5][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, color = 'gold', marker = '*', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[5]/units*cut_energy**2, color = 'darkgoldenrod')\n",
    "plt.scatter(0, 0, label = 'Point Sources', color = 'gold', marker = '*', s = sizeofdot)\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-3, 3e1)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "#EGB\n",
    "for i in range(0, len(arrow[4])):\n",
    "    if arrow[4][i] == True:\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[4][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], egbflux[i]*cut_energy[i]**2, color = 'orchid', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[4]/units*cut_energy**2, color = 'thistle', label = 'Fermi')\n",
    "plt.scatter(0, 0, label = 'EGB', color = 'orchid', marker = '>', s = sizeofdot)\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 3e-1)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "#Dark matter\n",
    "for i in range(0, len(arrow[3])):\n",
    "    if arrow[3][i] == True:\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[3][i], capsize = 4, color = 'maroon', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], dmflux[i]*cut_energy[i]**2, color = 'maroon', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[3]/units*cut_energy**2, color = 'red', label = 'DarkSUSY')\n",
    "plt.scatter(0, 0, label = 'DM', color = 'maroon', marker = '>', s = sizeofdot)\n",
    "plt.scatter(cut_energy[14], dmflux[14]*cut_energy[14]**2, c = 'black')\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-7, 3e0)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = readfile(point_sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points[-1].header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
