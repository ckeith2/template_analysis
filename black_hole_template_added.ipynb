{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib; matplotlib.use('Agg')\n",
    "from matplotlib import transforms\n",
    "from matplotlib import rc\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "from astropy.io import fits as pyfits\n",
    "from astropy.io import fits\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import scipy\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "from scipy.special import factorial\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import quad\n",
    "from itertools import starmap\n",
    "from scipy import optimize\n",
    "import corner\n",
    "import time\n",
    "from mpl_toolkits import mplot3d\n",
    "import healpy as hp\n",
    "from scipy import nan\n",
    "import dark_matter_jfactors_test as dmj\n",
    "import math\n",
    "import random\n",
    "import importlib\n",
    "from pymultinest.solve import solve\n",
    "import pymultinest\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy as sp\n",
    "import scipy.interpolate\n",
    "import json\n",
    "from os import walk\n",
    "import re\n",
    "import acceptance_and_angle as aaa\n",
    "import photon_spectrum\n",
    "import evaporating_black_hole_template as dm_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = ['Bremss_00320087_E_50-814008_MeV_healpix_128.fits', 'Bremss_SL_Z6_R20_T100000_C5_E_50-814008_MeV_healpix_128.fits', 'pi0_Model_A_E_50-814008_MeV_healpix_128.fits', 'pi0_Model_F_E_50-814008_MeV_healpix_128.fits', 'ICS_Model_A_E_50-814008_MeV_healpix_128.fits', 'ICS_Model_F_E_50-814008_MeV_healpix_128.fits']\n",
    "evermore_shifted = np.asarray([\n",
    "    [33, 21, 49],\n",
    "    [154, 112, 82],\n",
    "    [241, 149, 91],\n",
    "    [142, 52, 38],\n",
    "    [33, 21, 49],\n",
    "])/256\n",
    "\n",
    "point_sources = ['point_sources.fits']\n",
    "\n",
    "#exposure_time = 4.1e6 #.13 years 47 days\n",
    "exposure_time = 4.1e8 #13 years\n",
    "#exposure_time = 4.1e9 #130 years\n",
    "#exposure_time = 4.1e11 #13000 years\n",
    "#exposure_time = 4.1e16 #13000 years\n",
    "#exposure_time = 3.154e10\n",
    "\n",
    "acceptance_interp = aaa.get_acceptance_interp() #put in the energy in MeV!\n",
    "angle_interp = aaa.get_angle_interp()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Egammas for evaporating black holes\n",
    "energiesforBH = np.logspace(np.log10(10), np.log10(1e6), num = 1000)\n",
    "egamma_values = photon_spectrum.get_egammas(energiesforBH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    \"\"\"\n",
    "    A simple function to read the maps of a given number n and given filename.\n",
    "    \"\"\"\n",
    "    file_to_read = fits.open(filename)\n",
    "    return file_to_read\n",
    "\n",
    "def reshape_file(hdu, n, inner20 = True):\n",
    "    \"\"\"\n",
    "    Reshapes the data to be in the size we want\n",
    "    \"\"\"\n",
    "    \n",
    "    if inner20:\n",
    "        numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "        NSIDE = int(hdu[0].header['NSIDE'])\n",
    "        degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "        vec = hp.ang2vec(np.pi/2, 0)\n",
    "        ipix_disc20 = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "        \n",
    "        data4 = hdu[n].data\n",
    "        test20 = np.copy(data4)[ipix_disc20]\n",
    "        \n",
    "        #might need this for plotting, not sure\n",
    "        #data4 = hdu[n].data\n",
    "        #test20 = np.copy(data4)\n",
    "        #test20[inner_20] = np.nan\n",
    "        #testbin = np.reshape(test20, (128*3, 1536//3))\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        testbin = np.reshape(hdu[n].data, (128*3, 1536//3))\n",
    "        \n",
    "        \"\"\"\n",
    "        numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "        NSIDE = int(hdu[0].header['NSIDE'])\n",
    "        degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "        \n",
    "        inner_20_pos = (np.sqrt((degrees[0])**2+degrees[1]**2)> 20)\n",
    "        inner_20_neg = (np.sqrt((degrees[0]-360)**2+degrees[1]**2)> 20)\n",
    "        inner_20 = np.logical_and(inner_20_pos, inner_20_neg)\n",
    "        \n",
    "        data4 = hdu[n].data\n",
    "        test20 = np.copy(data4)\n",
    "        #test20[inner_20] = np.nan\n",
    "        #print(sum(~np.isnan(test20)))\n",
    "        #testbin = np.reshape(test20, (128*3, 1536//3))\n",
    "        \n",
    "    return test20\n",
    "\n",
    "def get_energy_index(E_desired, hdu):\n",
    "    energy_hdu = np.concatenate(hdu[38].data, axis = 0)\n",
    "    idx = find_nearest(energy_hdu, E_desired)\n",
    "    return idx\n",
    "    \n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def log_interp1d(xx, yy, kind='linear'):\n",
    "    logx = np.log10(xx)\n",
    "    logy = np.log10(yy)\n",
    "    lin_interp = sp.interpolate.interp1d(logx, logy, kind=kind, fill_value=\"extrapolate\")\n",
    "    log_interp = lambda zz: np.power(10.0, lin_interp(np.log10(zz)))\n",
    "    return log_interp\n",
    "\n",
    "def get_all_egb_data(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "    return middle_bin, bin_width, egb_intensity\n",
    "\n",
    "def get_all_egb(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "        \n",
    "\n",
    "    log_interp = log_interp1d(middle_bin, egb_intensity/bin_width, kind='linear')\n",
    "    \n",
    "    '''\n",
    "    print(egb_intensity[2]/bin_width[2]*deltae[6])\n",
    "    print(energies[6])\n",
    "    x_trapz = np.logspace(np.log10(np.nanmin(energies)), np.log10(np.nanmax(energies)), num = 100)\n",
    "    plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "    plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "    plt.scatter(energies[6], log_interp(energies[6]), color = 'green')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    '''\n",
    "    \n",
    "    counts = []\n",
    "    #only want energies from 3 onward (about lowest at 80 MeV)\n",
    "    for x in range(3, len(energies)):\n",
    "        highest_val = energies[x]+deltae[x]\n",
    "        lowest_val = energies[x]-deltae[x]\n",
    "        x_trapz = np.logspace(np.log10(lowest_val), np.log10(highest_val), num = 400)\n",
    "        #counts.append(log_interp(energies[x])*deltae[x])\n",
    "        total_counts = np.trapz(log_interp(x_trapz), x = x_trapz)\n",
    "        \n",
    "        #print('total counts: {}'.format(total_counts))\n",
    "        '''\n",
    "        plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "        plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "        plt.scatter(energies[x], total_counts)\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        asdfads\n",
    "        '''\n",
    "        counts.append(total_counts)  \n",
    "    return counts #returns counts per cm^2 per sec per str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract(n):\n",
    "    ##Template for 1 GeV and 10 GeV\n",
    "    icsa = readfile(filelist[n])\n",
    "    \n",
    "    idx1 = get_energy_index(1*1e3, icsa)\n",
    "    idx10 = get_energy_index(10*1e3, icsa)\n",
    "    \n",
    "    array1 = reshape_file(icsa, idx1, inner20 = False)\n",
    "    array10 = reshape_file(icsa, idx10, inner20 = False)\n",
    "    \n",
    "    ##Sum up idx1 and idx 10, make them equal in sum\n",
    "    sum1 = np.nansum(array1)\n",
    "    sum10 = np.nansum(array10)\n",
    "    array10_adjusted = (array10*sum1/sum10)\n",
    "    subtract110 = np.abs(array1-array10_adjusted)/array1\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    image = ax.imshow(subtract110)\n",
    "    fig.colorbar(image, ax=ax, anchor=(0, 0.3), shrink=0.7)\n",
    "    plt.title(str(filelist[n]))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#n = 4 for ICSA, n = 2 for pi0\n",
    "def psf_smoothing(n, energyidx, inner20psf = False, pointsource = False, use_og = False):\n",
    "    inner20psf = False\n",
    "    if pointsource:\n",
    "        icsa = readfile(point_sources[0])\n",
    "        data50 = icsa[energyidx].data\n",
    "    else:\n",
    "        icsa = readfile(filelist[n])\n",
    "        data50 = reshape_file(icsa, energyidx, inner20 = inner20psf)\n",
    "    hdu = readfile(filelist[0])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #data50 = reshape_file(icsa, energyidx, inner20 = inner20psf) #get the data at 50 MeV\n",
    "    \n",
    "    #get_where_within_20deg\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    ipix_disc = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "    \n",
    "    init_sum = np.sum(data50[ipix_disc])\n",
    "    #print(init_sum)\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    sig = np.pi/180*angle_interp(energy_here)\n",
    "    if use_og:\n",
    "        data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=np.pi/180/1.508)\n",
    "    else:\n",
    "        data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=sig)\n",
    "\n",
    "    fin_sum = np.sum(data50_convolved[ipix_disc])\n",
    "    #print(fin_sum)\n",
    "    \n",
    "\n",
    "    #data50_convolved = gaussian_filter(data50, sigma=0)\n",
    "    \n",
    "    #only get data within 20 degrees\n",
    "    #hp.mollview(np.log10(data50_convolved), coord = 'G')\n",
    "    \n",
    "    \n",
    "    testbin = data50_convolved[ipix_disc]\n",
    "    #print(len(testbin))\n",
    "    \n",
    "    #print(init_sum)\n",
    "    #print(fin_sum)\n",
    "    #print('---------------')\n",
    "\n",
    "\n",
    "    return np.array(testbin)*init_sum/fin_sum\n",
    "\n",
    "def psf_smoothing_DM(energyidx, crosssec = 2.2e-26, anal_data = False, mass_dm = 100, use_og = False, evapbh = False, gam = 1, massbh = 2e16):\n",
    "    energybin = np.concatenate(readfile(filelist[0])[38].data, axis = 0)[energyidx]\n",
    "    bins_in_lin = np.log10(energybin)\n",
    "    deltae = get_deltaE(energyidx)\n",
    "    \n",
    "    highe = (energybin+deltae)/1e3\n",
    "    lowe = (energybin-deltae)/1e3\n",
    "    \n",
    "    hdu = readfile(filelist[0])\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "    degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "    \n",
    "    \n",
    "    #need to make sure the initsum is *only within the inner 20 degrees, same for finsum\n",
    "    if evapbh == True:\n",
    "        data50 = dm_template.get_dNdE(egamma_values, energyidx, gamma = gam, mass = massbh, for_normals = False) #photons per cm^2 per sec per str per MeV\n",
    "    else:\n",
    "        data50 = dmj.get_dNdE(highe, lowe, sigmav = crosssec, analyze_data = anal_data, massx = mass_dm)[1] #photons per cm^2 per sec per str per MeV\n",
    "    \n",
    "    #get_where_within_20deg\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    ipix_disc = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "    \n",
    "    init_sum = np.sum(data50[ipix_disc])\n",
    "        \n",
    "    #hp.mollview((data50), coord = 'G')\n",
    "    \n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    sig = np.pi/180*angle_interp(energy_here)\n",
    "    if use_og:\n",
    "        data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=np.pi/180/1.508)\n",
    "    else:\n",
    "        data50_convolved = hp.sphtfunc.smoothing(data50.flatten(), sigma=sig)\n",
    "    fin_sum = np.sum(data50_convolved[ipix_disc])\n",
    "    \n",
    "    #hp.mollview((data50_convolved), coord = 'G')\n",
    "    \n",
    "    #photons per cm^2 per sec per str per MeV\n",
    "    \n",
    "    #print(init_sum)\n",
    "    #print(fin_sum)\n",
    "    #print('---------------')\n",
    "    \n",
    "    return np.array(data50_convolved[ipix_disc])*init_sum/fin_sum\n",
    "\n",
    "def get_deltaE(n):\n",
    "    energybins = np.concatenate(readfile(filelist[0])[38].data, axis = 0)\n",
    "    bins_in_lin = np.log10(energybins)[n]\n",
    "    spacing = 0.05691431\n",
    "    \n",
    "    high_bin = 10**(bins_in_lin + spacing)\n",
    "    low_bin = 10**(bins_in_lin - spacing)\n",
    "    \n",
    "    deltaE = np.abs(high_bin - low_bin)\n",
    "    #print('delta E: {}'.format(deltaE))\n",
    "    \n",
    "    return deltaE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimportlib.reload(photon_spectrum)\\nimportlib.reload(dm_template)\\nlum = (photon_spectrum.get_integral(egamma_values, mass_bh = 2e16)[1])\\n#print(lum)\\nenergies = np.logspace(np.log10(.1), np.log10(1e6), num = 1000)\\n\\nplt.plot(energies, lum)\\nplt.xscale('log')\\nplt.yscale('log')\\nplt.ylim(1e15, 1e20)\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "importlib.reload(photon_spectrum)\n",
    "importlib.reload(dm_template)\n",
    "lum = (photon_spectrum.get_integral(egamma_values, mass_bh = 2e16)[1])\n",
    "#print(lum)\n",
    "energies = np.logspace(np.log10(.1), np.log10(1e6), num = 1000)\n",
    "\n",
    "plt.plot(energies, lum)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e15, 1e20)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here1 = psf_smoothing_DM(10, evapbh = True, gam = 1, massbh = 1e16)\n",
    "\n",
    "#here1 = psf_smoothing_DM(15, crosssec = 2.2e-24, mass_dm = 100, gam = 1, massbh = 2e16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nindexhere = 10\\npsf_smoothing_DM(indexhere, crosssec = 1e-25)\\npsf_smoothing_DM(indexhere, crosssec = 1e-25, use_og = True)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npsf_smoothing(0, indexhere)\\npsf_smoothing(0, indexhere, use_og = True)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npsf_smoothing(2, indexhere)\\npsf_smoothing(2, indexhere, use_og = True)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npsf_smoothing(4, indexhere)\\npsf_smoothing(4, indexhere, use_og = True)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npsf_smoothing(np.nan, indexhere, pointsource = True)\\npsf_smoothing(np.nan, indexhere, pointsource = True, use_og = True)\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "indexhere = 10\n",
    "psf_smoothing_DM(indexhere, crosssec = 1e-25)\n",
    "psf_smoothing_DM(indexhere, crosssec = 1e-25, use_og = True)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "psf_smoothing(0, indexhere)\n",
    "psf_smoothing(0, indexhere, use_og = True)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "psf_smoothing(2, indexhere)\n",
    "psf_smoothing(2, indexhere, use_og = True)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "psf_smoothing(4, indexhere)\n",
    "psf_smoothing(4, indexhere, use_og = True)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "psf_smoothing(np.nan, indexhere, pointsource = True)\n",
    "psf_smoothing(np.nan, indexhere, pointsource = True, use_og = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def poisson_dist(n, energyidx, cross_section =2.2e-26, dm = False, dm_bh = False, analyze_data = False, dm_mass = 100, egb = False, points = False, counts = 0, evapbh = True, blackholem = 2e16):   \n",
    "    '''\n",
    "    Performs a PSF smoothing of the array, before converting it into photons per pixel\n",
    "    \n",
    "    '''\n",
    "    deltaE = get_deltaE(energyidx)\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_for_poisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    \n",
    "    if dm:\n",
    "        convolved_data = psf_smoothing_DM(energyidx, cross_section, anal_data = analyze_data, mass_dm = dm_mass)/deltaE\n",
    "    elif egb:\n",
    "        convolved_data_init = np.empty(5938) #needs to be the length of the good vals\n",
    "        convolved_data_init.fill(1) #counts per cm^2 per sec per str\n",
    "        convolved_data = convolved_data_init*counts #in units of photons per cm^2 per mev per str per sec\n",
    "    elif points:\n",
    "        convolved_data = psf_smoothing(n, energyidx, pointsource = True)\n",
    "    elif dm_bh:\n",
    "        convolved_data = psf_smoothing_DM(energyidx, cross_section, anal_data = analyze_data, evapbh = True, massbh = blackholem)\n",
    "    else:\n",
    "        convolved_data = psf_smoothing(n, energyidx, cross_section) #data in units of photons cm^-2 MeV^-1 str^-1\n",
    "    #n_gamma = np.array(convolved_data)*deltaE*exposure_time*8500*4*np.pi/196608*.2 #13 years*.85meters^2, units of photons per pixel\n",
    "    n_gamma = np.array(convolved_data)*deltaE*exposure_time*acceptance_for_poisson/196608 #13 years*.85meters^2, units of photons per pixel\n",
    "\n",
    "    \n",
    "    return n_gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nindexhere = 7\\n\\npoisson_dist(np.nan, indexhere, cross_section =1e-25, dm = True)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npoisson_dist(0, indexhere, cross_section =1e-25)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npoisson_dist(2, indexhere, cross_section =1e-25)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npoisson_dist(4, indexhere, cross_section =1e-25)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npoisson_dist(np.nan, indexhere, cross_section =1e-25, points = True)\\nprint('~~~~~~~~~~~~~~~~~~~~~~~~~')\\npoisson_dist(np.nan, indexhere, cross_section =1e-25, egb = True, counts = egb_counts[indexhere-3])\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "indexhere = 7\n",
    "\n",
    "poisson_dist(np.nan, indexhere, cross_section =1e-25, dm = True)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "poisson_dist(0, indexhere, cross_section =1e-25)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "poisson_dist(2, indexhere, cross_section =1e-25)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "poisson_dist(4, indexhere, cross_section =1e-25)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "poisson_dist(np.nan, indexhere, cross_section =1e-25, points = True)\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "poisson_dist(np.nan, indexhere, cross_section =1e-25, egb = True, counts = egb_counts[indexhere-3])\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_image(data):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    image = ax.imshow(data)\n",
    "    fig.colorbar(image, ax=ax, anchor=(0, 0.3), shrink=0.7)\n",
    "    return\n",
    "\n",
    "def simulated_data(energyidx, templates):\n",
    "    \n",
    "    '''\n",
    "    Use PSF smoothed data to create a random poisson draw to obtain simulated data\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    n_gammatot = 0\n",
    "    for i in range(0, len(templates)):\n",
    "        n_gammatot += templates[i]\n",
    "    len_of_rand = len(n_gammatot)\n",
    "    simdata = np.zeros(len_of_rand)\n",
    "    randdata = np.random.rand(len_of_rand)\n",
    "    \n",
    "    #print(n_gammatot)\n",
    "    \n",
    "    for i in range(0, len_of_rand):\n",
    "        #print(n_gammatot[i])\n",
    "        if n_gammatot[i]<.01:\n",
    "            if n_gammatot[i] < randdata[i]:\n",
    "                simdata[i] = 0\n",
    "            else:\n",
    "                simdata[i] = 1\n",
    "        else:\n",
    "            simdata[i] = np.random.poisson(lam = n_gammatot[i])\n",
    "    #print('simdata just 20: ', np.nansum(simdata))\n",
    "    \n",
    "    return simdata\n",
    "\n",
    "def minimize_likelihood(energyidx, cross_sec = 2.2e-26, massdm = 100):\n",
    "    pi = poisson_dist(2, energyidx)\n",
    "    ics = poisson_dist(4, energyidx)   \n",
    "    brem = poisson_dist(0, energyidx)\n",
    "\n",
    "    darkm = poisson_dist(np.nan, energyidx, cross_section = cross_sec, dm = True, dm_mass = massdm)\n",
    "\n",
    "    k = simulated_data(energyidx, [pi, ics, brem])#remove DM for accurate\n",
    "    #print(np.nanmean(lamb))\n",
    "    #asdfasd\n",
    "    \n",
    "    #Need to minimize for lamb < 50 and lamb > 50\n",
    "    \n",
    "    bnds = ((0, np.inf), (0, np.inf), (0, np.inf), (1e-30, np.inf))\n",
    "    result = minimize(likelihood, (1, 1, 1, 1), args = (k, pi, ics, brem, darkm), bounds = bnds)\n",
    "    min_likelihood = result.x\n",
    "    chi2 = result.fun\n",
    "    #print(result)\n",
    "\n",
    "    return min_likelihood\n",
    "\n",
    "\n",
    "def likelihood(constants, k, pi, ics, brem, dm):\n",
    "    \"\"\"\n",
    "    Gets the Total Likelihoods from the Gaussian Regime and the Poisson Regime\n",
    "    Once each has been calculated, multiplies values together for final likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    likelihood_poiss = likelihood_poisson(constants, k, pi, ics, brem, dm)\n",
    "    #print('likelihood: {}'.format(likelihood_poiss)) \n",
    "    return likelihood_poiss\n",
    "\n",
    "def merge(list1, list2):\n",
    "      \n",
    "    merged_list = tuple(zip(list1, list2)) \n",
    "    return merged_list\n",
    "\n",
    "def likelihood_gaussian(constants, lamb, pi, ics):\n",
    "    sigma = np.sqrt(constants[0]*pi+constants[1]*ics)\n",
    "    mu = lamb\n",
    "    rng = 0.5\n",
    "    \n",
    "    #flatten arrays\n",
    "    sigma_flat = sigma.flatten()\n",
    "    mu_flat = mu.flatten()\n",
    "    length_flattened = len(mu_flat)\n",
    "    #get arrays in sigma, mu tuple format\n",
    "    ms_tuples = list(merge(mu_flat, sigma_flat))\n",
    "    ms = np.array(ms_tuples, dtype = 'f,f')\n",
    "    lower_bound_arr = mu_flat - rng\n",
    "    upper_bound_arr = mu_flat + rng\n",
    "    \n",
    "    args = np.concatenate((np.full((length_flattened, 1), prob_func), lower_bound_arr.reshape((length_flattened, 1)), upper_bound_arr.reshape((length_flattened, 1)), ms.reshape((length_flattened, 1))), axis = 1)\n",
    "    \n",
    "    #log likelihood\n",
    "    prob = list(starmap(lambda a, b, c, d: quad(a, b, c, d)[0], args))\n",
    "\n",
    "    #reshape for testing\n",
    "    l = np.sum(np.log(prob))\n",
    "    likely = -2*l\n",
    "    \n",
    "    return likely\n",
    "    \n",
    "def prob_func(x, mu, sigma):\n",
    "    probdens = 1/(sigma*np.sqrt(2*np.pi))*np.exp(-1/2*((x-sigma**2)/sigma)**2)\n",
    "    return probdens\n",
    "\n",
    "#def likelihood_poisson(a0):\n",
    "def likelihood_poisson(a0, a1, a2, a3, a4):\n",
    "#def likelihood_poisson(constants, ktest, pitest, icstest, bremtest, dmtest):\n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+a3*darkmtest+a4*egbtest\n",
    "    #lamb = a0*egbtest\n",
    "    \n",
    "    #print(a0, a1, a2, a3)\n",
    "    #print(lamb)\n",
    "\n",
    "    #lamb = constants[0]*pitest+constants[1]*icstest+constants[2]*bremtest+constants[3]*dmtest\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    #print(fprob)\n",
    "    return -2*np.nansum(fprob)\n",
    "\n",
    "def get_curves(n, energyidx, inner20psf = True):\n",
    "    icsa = readfile(filelist[n])\n",
    "    data50 = reshape_file(icsa, energyidx, inner20 = inner20psf) #get the data at energyidx MeV\n",
    "    delt = get_deltaE(energyidx)\n",
    "    \n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    \n",
    "    return np.asarray(data50)*exposure_time*acceptance_forpoisson*delt #try to get in terms of counts per bin\n",
    "    \n",
    "    \n",
    "def get_normalizations_spectrum(deltaE, cross_sec = 2.2e-26, dm_mass = 100, f_bh = 1, gam = 1, massbh = 2e26):\n",
    "    \n",
    "    range_templates = [2, 4, 0] #pi, ics, brem\n",
    "    e = readfile(filelist[0])[38].data\n",
    "    energies = np.array(list(e)).T[0]\n",
    "    templates = []\n",
    "    temp = []\n",
    "    deltaomega = 1/196608 #i think this should maybe still be total bins in whole image pre-20?\n",
    "    \n",
    "    acceptances = []\n",
    "    for energyidxhere in range(0, len(energies)):\n",
    "        energyidx = energyidxhere\n",
    "        hdu = readfile(filelist[0])\n",
    "        energy_here = float(hdu[38].data[energyidx][0])\n",
    "        acceptance_forpoisson = acceptance_interp(energy_here) #in cm^2*str\n",
    "        acceptances.append(acceptance_forpoisson)\n",
    "    \n",
    "\n",
    "    for n in range_templates:\n",
    "        temp = []\n",
    "        for index in range(0, len(energies)):\n",
    "            temp.append(np.nansum(get_curves(n, index))) #units of photons per bin\n",
    "        templates.append(np.asarray(temp)*deltaomega) #gets counts per bin\n",
    "\n",
    "    #eavporating black holes\n",
    "    dmevap_temp = []\n",
    "    dmevap_tot = []\n",
    "    for index in range(0, len(energies)):\n",
    "        energybin = np.concatenate(readfile(filelist[0])[38].data, axis = 0)[index]\n",
    "        bins_in_lin = np.log10(energybin)\n",
    "        print(gam)\n",
    "    \n",
    "        data50 = dm_template.get_dNdE(egamma_values, index, gamma = gam, mass = massbh, for_normals = True) #units of photons cm^-2 str^-1 per sec not per MeV?\n",
    "        #need to cut this to be inner 20 degrees\n",
    "        \n",
    "        \n",
    "        #dm_temp.append(np.nansum(data50))\n",
    "        dmevap_temp.append(np.nansum(data50)*exposure_time) #13 years * .85 m^2 * .2, return in photons/str\n",
    "    dmevap_tot.append(np.asarray(dmevap_temp)*deltaomega*f_bh) #photons per pixel?\n",
    "    \n",
    "    #dark matter template\n",
    "    \n",
    "    \n",
    "    \n",
    "    dm_temp = []\n",
    "    dm_templates_tot = []\n",
    "    for index in range(0, len(energies)):\n",
    "        energybin = np.concatenate(readfile(filelist[0])[38].data, axis = 0)[index]\n",
    "        bins_in_lin = np.log10(energybin)\n",
    "        deltae = get_deltaE(index)\n",
    "    \n",
    "        highe = (energybin+deltae)/1e3\n",
    "        lowe = (energybin-deltae)/1e3\n",
    "        data50 = dmj.get_dNdE(highe, lowe, sigmav = cross_sec, massx = dm_mass, for_normals = True)[1] #units of photons cm^-2 str^-1 per sec not per MeV?\n",
    "        #need to cut this to be inner 20 degrees\n",
    "        \n",
    "        \n",
    "        #dm_temp.append(np.nansum(data50))\n",
    "        dm_temp.append(np.nansum(data50)*exposure_time) #13 years * .85 m^2 * .2, return in photons/str\n",
    "    dm_templates_tot.append(np.asarray(dm_temp)*deltaomega) #photons per pixel?\n",
    "    \n",
    "    \n",
    "    #EGB template\n",
    "    egb_templates = np.array(get_all_egb(energies, deltaE)) #units of counts per cm^2 per sec per str\n",
    "    egb_temp_fin = egb_templates*acceptances*exposure_time*deltaomega #counts per bin\n",
    "    \n",
    "    #Point Source Template\n",
    "    point_source_arr = []\n",
    "    for index in range(0, len(energies)):\n",
    "        smaller_index = index\n",
    "        point_source_arr.append(np.nansum(get_curves_pointsource(index, smaller_index, inner20psf = True))*deltaomega) #photons per pixel\n",
    "        \n",
    "    print('yay!')\n",
    "        \n",
    "\n",
    "    return range_templates, energies, [np.array(templates[0]), np.array(templates[1]), np.array(templates[2]), np.array(egb_temp_fin), np.array(point_source_arr), np.array(dm_templates_tot), np.array(dmevap_tot)] #counts per pixel\n",
    "    #return range_templates, np.asarray(dm_temp)*deltaomega\n",
    "    \n",
    "def get_curves_pointsource(energyidx, smallindex, inner20psf = True):\n",
    "    pointsourcedata = readfile(point_sources[0])[smallindex].data\n",
    "    \n",
    "    hdu = readfile(filelist[0])\n",
    "    numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "    NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "    degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "    vec = hp.ang2vec(np.pi/2, 0)\n",
    "    ipix_disc20 = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "        \n",
    "    test20 = np.copy(pointsourcedata)[ipix_disc20]\n",
    "    \n",
    "    delt = get_deltaE(energyidx)\n",
    "    \n",
    "    deltaE = get_deltaE(energyidx)\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    \n",
    "    return np.asarray(test20)*exposure_time*acceptance_forpoisson*delt #13 years * .85 m^2 * .2, return in photons /bin\n",
    "    \n",
    "    \n",
    "def get_normalized(energyidx, normals, template_val, energies):\n",
    "    '''\n",
    "    Normalizes the ROI based on the shape the spectrums should have\n",
    "    \n",
    "    \n",
    "    Do not need to use this, as long as you stay consistent across all Fermi data\n",
    "    for the exposure time and collecting area\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    poisson_pi = poisson_dist(template_val, int(energyidx)) #units of photons per pixel\n",
    "    init_sum_pi = np.nansum(poisson_pi)\n",
    "    #print(np.nansum(init_sum_pi))\n",
    "    if template_val == 2:\n",
    "        normval = 0\n",
    "    if template_val == 4:\n",
    "        normval = 1\n",
    "    if template_val == 0:\n",
    "        normval = 2\n",
    "    print('normval for pi at 0: {}'.format(normals[normval][energyidx])) \n",
    "    print('delta E at 0: {}'.format(get_deltaE(energyidx)))\n",
    "    normal_pi = normals[normval][energyidx]*get_deltaE(energyidx)\n",
    "    pitest = poisson_pi*normal_pi/init_sum_pi\n",
    "    print('normalization: {}'.format(np.nansum(pitest)))\n",
    "    #print(np.nansum(pitest))\n",
    "    \n",
    "    #print('----------------------')\n",
    "    \n",
    "    return pitest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_darksusy_counts():\n",
    "    x = np.loadtxt('yield_DS_keith100.dat', dtype=str).T\n",
    "    energies = x[1].astype(np.float)*1e3\n",
    "    yieldperann = x[2].astype(np.float)/1e3 #convert from per GeV to per MeV\n",
    "    energybins = np.concatenate(readfile(filelist[0])[38].data, axis = 0)\n",
    "    \n",
    "    counts = []\n",
    "    delta = []\n",
    "    for n in range(0, 38):\n",
    "        \n",
    "        bins_in_lin = energybins[n]\n",
    "        deltae = get_deltaE(n)\n",
    "    \n",
    "        highe = (bins_in_lin+deltae)\n",
    "        lowe = (bins_in_lin-deltae)\n",
    "        \n",
    "        good_energies = np.where((energies <=highe) & (energies >= lowe))\n",
    "        \n",
    "        final_integral = np.trapz(yieldperann[good_energies], x = energies[good_energies])\n",
    "\n",
    "        counts.append(final_integral)\n",
    "        delta.append(deltae)\n",
    "    return np.array(counts), np.array(delta)\n",
    "\n",
    "\n",
    "\n",
    "def log_interp1d(xx, yy, kind='linear'):\n",
    "    logx = np.log10(xx)\n",
    "    logy = np.log10(yy)\n",
    "    lin_interp = sp.interpolate.interp1d(logx, logy, kind=kind, fill_value=\"extrapolate\")\n",
    "    log_interp = lambda zz: np.power(10.0, lin_interp(np.log10(zz)))\n",
    "    return log_interp\n",
    "\n",
    "def get_all_egb_data(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "    return middle_bin, bin_width, egb_intensity\n",
    "\n",
    "def get_all_egb(energies, deltae):\n",
    "    energy_range = np.array([(.1, .14), (.14, .2), (.2, .28), (.28, .4), (.4, .57), (.57, .8), (.8, 1.1), (1.1, 1.6), (1.6, 2.3), (2.3, 3.2), (3.2, 4.5), (4.5, 6.4), (6.4, 9.1), (9.1, 13), (13, 18), (18, 26), (26, 36), (36, 51), (51, 72), (72, 100), (100, 140), (140, 200), (200, 290), (290, 410), (410, 580), (580, 820)])*1e3 #GeV to MeV\n",
    "    egb_intensity = np.array([3.7e-6, 2.3e-6, 1.5e-6, 9.7e-7, 6.7e-7, 4.9e-7, 3e-7, 1.8e-7, 1.1e-7, 6.9e-8, 4.2e-8, 2.6e-8, 1.7e-8, 1.2e-8, 6.8e-9, 4.4e-9, 2.7e-9, 1.8e-9, 1.1e-9, 6.2e-10, 3.1e-10, 1.9e-10, 8.9e-11, 6.3e-11, 2.1e-11, 9.7e-12])\n",
    "    middle_bin = []\n",
    "    bin_width = []\n",
    "    for i in range(0, len(energy_range)):\n",
    "        low_e = np.log10(energy_range[i][0])\n",
    "        high_e = np.log10(energy_range[i][1])\n",
    "        difference = np.abs((low_e+high_e)/2)\n",
    "        middle_bin.append(10**(difference))\n",
    "        bin_width.append(np.abs(energy_range[i][1]-(10**(difference)))) \n",
    "        \n",
    "\n",
    "    log_interp = log_interp1d(middle_bin, egb_intensity/bin_width, kind='linear')\n",
    "    \n",
    "    '''\n",
    "    print(egb_intensity[2]/bin_width[2]*deltae[6])\n",
    "    print(energies[6])\n",
    "    x_trapz = np.logspace(np.log10(np.nanmin(energies)), np.log10(np.nanmax(energies)), num = 100)\n",
    "    plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "    plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "    plt.scatter(energies[6], log_interp(energies[6]), color = 'green')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    '''\n",
    "    \n",
    "    counts = []\n",
    "    #only want energies from 3 onward (about lowest at 80 MeV)\n",
    "    for x in range(0, len(energies)):\n",
    "        highest_val = energies[x]+deltae[x]\n",
    "        lowest_val = energies[x]-deltae[x]\n",
    "        x_trapz = np.logspace(np.log10(lowest_val), np.log10(highest_val), num = 400)\n",
    "        #counts.append(log_interp(energies[x])*deltae[x])\n",
    "        total_counts = np.trapz(log_interp(x_trapz), x = x_trapz)\n",
    "        \n",
    "        #print('total counts: {}'.format(total_counts))\n",
    "        '''\n",
    "        plt.scatter(middle_bin, egb_intensity/bin_width)\n",
    "        plt.plot(x_trapz, log_interp(x_trapz), color = 'red')\n",
    "        plt.scatter(energies[x], total_counts)\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        asdfads\n",
    "        '''\n",
    "        counts.append(total_counts)  \n",
    "    return counts #returns counts per cm^2 per sec per str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "sigmav: 2.2e-26\n",
      "yay!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(dmj)\n",
    "\n",
    "#gets darksusy counts for a specific dark matter mass\n",
    "counts, deltae = get_darksusy_counts()\n",
    "#be sure to change the dm mass\n",
    "temps, energies, normals = get_normalizations_spectrum(deltae, cross_sec = 2.2e-26, f_bh = 1, gam = 1, massbh = 2e16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acceptances = []\n",
    "cut_energy = energies\n",
    "for energyidxhere in range(0, len(cut_energy)):\n",
    "    energyidx = energyidxhere\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['pi', 'ics', 'brem', 'egb', 'point source', 'dm', 'BH']\n",
    "units = deltae*exposure_time/196608*acceptances\n",
    "fntsz=20\n",
    "plt.close()\n",
    "\n",
    "for i in range(0, len(normals)):\n",
    "    plt.scatter(energies, normals[i]/units*energies**2, label = labels[i])\n",
    "    \n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 1e4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('test_evap_norms1.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndeltae_cut = np.copy(deltae[3:])\\nunits = deltae_cut*exposure_time/196608*acceptances\\ncut_energy = energies[3:]\\nfntsz = 10\\n\\nfor i in range(0, len(normals)):\\n    plt.scatter(cut_energy, normals[i]/units*cut_energy**2, label = str(i))\\n\\n\\nplt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\\nplt.xlabel('MeV', fontsize = fntsz)\\nplt.legend(fontsize = 15)\\nplt.yscale('log')\\nplt.xscale('log')\\nplt.ylim(1e-7, 1e2)\\nplt.show()\\nplt.close()\\n#plt.ylim(1e-8, 3e1)\\n#plt.legend()\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "deltae_cut = np.copy(deltae[3:])\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "fntsz = 10\n",
    "\n",
    "for i in range(0, len(normals)):\n",
    "    plt.scatter(cut_energy, normals[i]/units*cut_energy**2, label = str(i))\n",
    "\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylim(1e-7, 1e2)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#plt.ylim(1e-8, 3e1)\n",
    "#plt.legend()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code that returns the log-likelihood at a specific point. cube is first parameter, everything else are random parameters\n",
    "#that are not part of the fitting cube\n",
    "\n",
    "#cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest\n",
    "def likelihood_poisson_multinest(cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest):\n",
    "    \n",
    "    a0 = 10**cube[0]\n",
    "    a1 = 10**cube[1]\n",
    "    a2 = 10**cube[2]\n",
    "    a3 = 10**cube[3]\n",
    "    lamb = a0*pitest+a1*icstest+a2*bremtest+egbtest+a3*pointstest+darkmtest + blackholetest\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest\n",
    "def likelihood_poisson_multinest2(cube, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest):\n",
    "    a0 = cube[0]\n",
    "    a1 = cube[1]\n",
    "    a2 = cube[2]\n",
    "    a3 = cube[3]\n",
    "    \n",
    "    lamb = a0 * pitest + a1 * icstest + a2 * bremtest + egbtest + a3 * pointstest + darkmtest + blackholetest\n",
    "\n",
    "    \n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return -2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "#finds the log likelihood based on the constants that pymultinest have found. The constants have already\n",
    "#been moved from logspace to linearspace\n",
    "#I do this for every energy bin, and come out with one big log likelihood value for some value of sigmav.\n",
    "#constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest\n",
    "def likelihood_poisson_forsigmav(constants, piflux, icsflux, bremflux, egbflux, pointsflux, ktesthere,\n",
    "                                 darkmatter, blackholetest):\n",
    "    #Constants are my consants for pi, ics, brem, and point sources. the constant in front of egb is 1.\n",
    "    #The 'constant' in front of the dark matter is already incorporated by changing sigmav\n",
    "    #This is my value of lambda\n",
    "    lambd = constants[0]*piflux+constants[1]*icsflux+constants[2]*bremflux+egbflux+constants[3]*pointsflux+darkmatter+blackholetest\n",
    "\n",
    "    \n",
    "    #my value of k (ktest) is the poisson drawn values from sum of all the templates except dm\n",
    "    fprob = -scipy.special.gammaln(ktesthere+1)+ktesthere*np.log(lambd)-lambd #log likelihood of poisson\n",
    "    #scipy.special.gammaln is for the log of a factorial\n",
    "    return -2*np.nansum(fprob) #the sum of all the log likelihoods for each spatial point. \n",
    "\n",
    "##This is the prior function, it is flat in linear space, from a minimum value specified by the first number, \n",
    "#to a maximum number specified by the first number + the second number. \n",
    "#The variables were called a/b/c/phi0/phi1/norm in this code.\n",
    "#You need to define a prior for every parameter you send\n",
    "\n",
    "cube_limits = [(-2, 4), (-2, 4), (-2, 4), (-2, 4)]\n",
    "\n",
    "def prior(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*cube_limits[0][1] + cube_limits[0][0])\n",
    "    cube[1] = (cube[1]*cube_limits[1][1] + cube_limits[1][0])\n",
    "    cube[2] = (cube[2]*cube_limits[2][1] + cube_limits[2][0])\n",
    "    cube[3] = (cube[3]*cube_limits[3][1] + cube_limits[3][0])\n",
    "    return cube\n",
    "\n",
    "##This is the loglikelihood function for multinest – it sends the cube,\n",
    "#and then a bunch of different arrays that were used in fitting, but were constant, to the pymultinest code\n",
    "def loglikelihood_formulti(cube, ndim, nparms):\n",
    "    return likelihood_poisson_multinest(cube, pitest, icstest, bremtest, egbtest, pointstest, darkmtest, blackholetest, ktest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dm_array(cross_section = 2.2e-26, dm_mass = 100):\n",
    "    e = readfile(filelist[0])[38].data\n",
    "    energies = np.array(list(e)).T[0]\n",
    "    templates = []\n",
    "    temp = []\n",
    "    deltaomega = 4*np.pi/196608 #i think this should maybe still be total bins in whole image pre-20?\n",
    "    dm_temp = []\n",
    "    dm_templates_tot = []\n",
    "    for index in range(3, len(energies)):\n",
    "        energybin = np.concatenate(readfile(filelist[0])[38].data, axis = 0)[index]\n",
    "        bins_in_lin = np.log10(energybin)\n",
    "        deltae = get_deltaE(index)\n",
    "    \n",
    "        highe = (energybin+deltae)/1e3\n",
    "        lowe = (energybin-deltae)/1e3\n",
    "        data50 = dmj.get_dNdE(highe, lowe, sigmav = cross_section, massx = dm_mass, for_normals = True)[1] #units of photons cm^-2 str^-1 per sec not per MeV?\n",
    "        #need to cut this to be inner 20 degrees\n",
    "        \n",
    "        \n",
    "        #dm_temp.append(np.nansum(data50))\n",
    "        dm_temp.append(np.nansum(data50)/deltae) \n",
    "    dm_templates_tot.append(np.asarray(dm_temp)) #photons per s per cm^2 per MeV per str\n",
    "    return dm_templates_tot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "#get all ktests\n",
    "\n",
    "ktest_array = []\n",
    "importlib.reload(aaa)\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "deltae_cut = np.copy(deltae)\n",
    "\n",
    "egb_counts = get_all_egb(energies, deltae)/deltae_cut\n",
    "counting = 0\n",
    "\n",
    "\n",
    "for energyidx in range(0, len(energies)):\n",
    "    print(energyidx)\n",
    "    pitest = poisson_dist(2, int(energyidx))\n",
    "    icstest = poisson_dist(4, int(energyidx))\n",
    "    bremtest = poisson_dist(0, int(energyidx))\n",
    "    #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "    egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[counting])\n",
    "\n",
    "\n",
    "    #Point Sources\n",
    "    pointstest = poisson_dist(np.nan, energyidx, points = True) \n",
    "\n",
    "\n",
    "    #gotta add egbtest to the ktest\n",
    "\n",
    "    #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "    ktest1 = simulated_data(int(energyidx), [pitest, icstest, bremtest, egbtest, pointstest])\n",
    "    ktest_array.append(ktest1)\n",
    "    \n",
    "    counting += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-25 1.58489319e-26 2.51188643e-27 3.98107171e-28\n",
      " 6.30957344e-29 1.00000000e-29 0.00000000e+00]\n",
      "--------------------------\n",
      "Frac of DM In Black Holes:\n",
      "0.0001\n",
      "--------------------------\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain0\n",
      "0 49.99999999999999\n",
      "sigmav: 2.2e-26\n",
      "5872285.0\n",
      "5871071.945301408\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain0/0\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain0/0.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -57661.3 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.08 +- 0.66\n",
      "    a1             0.0019 +- 0.0028\n",
      "    a2             0.047 +- 0.012\n",
      "    a3             -0.0071 +- 0.0051\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain0/0stats.json\n",
      "---------------\n",
      "[0.08261410192560333, 1.0044919023489935, 1.1137039146993322, 0.9836820775572847]\n",
      "57645.31437556879\n",
      "---------------\n",
      "[616.25316534 616.70034511 617.10176349 ... 563.49915214 564.1554553\n",
      " 564.83097189]\n",
      "57645.31437556879\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain1\n",
      "1 64.98283000000002\n",
      "sigmav: 2.2e-26\n",
      "5700587.0\n",
      "5701727.001573997\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain1/1\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain1/1.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -57511.3 +- 0.6\n",
      "  parameters:\n",
      "    a0             -1.13 +- 0.67\n",
      "    a1             -0.0052 +- 0.0022\n",
      "    a2             0.070 +- 0.016\n",
      "    a3             0.0111 +- 0.0045\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain1/1stats.json\n",
      "---------------\n",
      "[0.0741635399948594, 0.9881709329363371, 1.1754334408262295, 1.0260036419451812]\n",
      "57498.311123670115\n",
      "---------------\n",
      "[539.57510972 539.77544951 539.93306671 ... 472.06744306 472.81308197\n",
      " 473.59438213]\n",
      "57498.311123670115\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain2\n",
      "2 84.45536389617803\n",
      "sigmav: 2.2e-26\n",
      "5600068.0\n",
      "5603411.019495594\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain2/2\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain2/2.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -57111.0 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.09 +- 0.64\n",
      "    a1             0.0002 +- 0.0019\n",
      "    a2             0.094 +- 0.016\n",
      "    a3             0.0067 +- 0.0036\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain2/2stats.json\n",
      "---------------\n",
      "[0.08144873597994548, 1.000487545271198, 1.241769637176334, 1.0154693987049124]\n",
      "57091.88347775918\n",
      "---------------\n",
      "[490.7699285  490.54519826 490.29556814 ... 407.13254835 408.01643954\n",
      " 408.92668211]\n",
      "57091.88347775918\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain3\n",
      "3 109.7629710930695\n",
      "sigmav: 2.2e-26\n",
      "5571080.0\n",
      "5571783.832211568\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain3/3\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain3/3.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -57056.5 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.85 +- 0.69\n",
      "    a1             0.0030 +- 0.0016\n",
      "    a2             0.121 +- 0.033\n",
      "    a3             0.0043 +- 0.0041\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain3/3stats.json\n",
      "---------------\n",
      "[0.14218861231016516, 1.0070020326512896, 1.32087368214398, 1.010047392819198]\n",
      "57035.36474953748\n",
      "---------------\n",
      "[464.36465274 463.50186753 462.68323511 ... 362.00487767 363.1298166\n",
      " 364.23365206]\n",
      "57035.36474953748\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain4\n",
      "4 142.65416981671697\n",
      "sigmav: 2.2e-26\n",
      "5602365.0\n",
      "5607657.878119908\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain4/4\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain4/4.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -56764.0 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.018 +- 0.099\n",
      "    a1             -0.0018 +- 0.0015\n",
      "    a2             0.008 +- 0.046\n",
      "    a3             0.0037 +- 0.0039\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain4/4stats.json\n",
      "---------------\n",
      "[0.9591876773401252, 0.9958588167582554, 1.018783125877858, 1.0086574640972463]\n",
      "56740.09937564425\n",
      "---------------\n",
      "[451.61975517 450.08778306 448.77080977 ... 329.39061534 330.9535456\n",
      " 332.37591369]\n",
      "56740.09937564425\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain5\n",
      "5 185.40143331981702\n",
      "sigmav: 2.2e-26\n",
      "5613715.0\n",
      "5622488.1700201\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain5/5\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain5/5.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -56687.6 +- 1.0\n",
      "  parameters:\n",
      "    a0             -0.190 +- 0.080\n",
      "    a1             0.0008 +- 0.0013\n",
      "    a2             0.087 +- 0.027\n",
      "    a3             0.0068 +- 0.0032\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain5/5stats.json\n",
      "---------------\n",
      "[0.6458310176504672, 1.0019567601521364, 1.2214794810186165, 1.0157086180966266]\n",
      "56668.5578738013\n",
      "---------------\n",
      "[447.09987675 445.27682111 443.94022813 ... 307.24152736 309.6335018\n",
      " 311.64764357]\n",
      "56668.5578738013\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain6\n",
      "6 240.95819646356009\n",
      "sigmav: 2.2e-26\n",
      "5121474.0\n",
      "5136185.534537753\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain6/6\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain6/6.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -55995.6 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.087 +- 0.043\n",
      "    a1             -0.0056 +- 0.0013\n",
      "    a2             0.061 +- 0.024\n",
      "    a3             -0.0003 +- 0.0026\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain6/6stats.json\n",
      "---------------\n",
      "[0.8193277764401277, 0.9871002054256977, 1.1505057307215125, 0.9992200266982288]\n",
      "55970.35776993363\n",
      "---------------\n",
      "[408.87093659 407.46203823 406.76273311 ... 268.65652965 272.28052722\n",
      " 275.2033109 ]\n",
      "55970.35776993363\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain7\n",
      "7 313.1629103579625\n",
      "sigmav: 2.2e-26\n",
      "4614381.0\n",
      "4628704.369185708\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain7/7\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain7/7.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -54924.0 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.043 +- 0.027\n",
      "    a1             -0.0066 +- 0.0014\n",
      "    a2             0.034 +- 0.020\n",
      "    a3             0.0051 +- 0.0021\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain7/7stats.json\n",
      "---------------\n",
      "[0.9066126655579523, 0.984860037275936, 1.0809542115243767, 1.0118174315736985]\n",
      "54898.27090413618\n",
      "---------------\n",
      "[366.04616135 364.89889905 364.69202734 ... 233.40379553 238.75891541\n",
      " 243.13717588]\n",
      "54898.27090413618\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain8\n",
      "8 407.0042433219344\n",
      "sigmav: 2.2e-26\n",
      "3923432.0\n",
      "3939741.68812098\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain8/8\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain8/8.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -53934.2 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.116 +- 0.023\n",
      "    a1             -0.0060 +- 0.0015\n",
      "    a2             0.089 +- 0.014\n",
      "    a3             0.0012 +- 0.0018\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain8/8stats.json\n",
      "---------------\n",
      "[0.76577804324326, 0.9861709690334073, 1.2267988361810553, 1.0027873310039832]\n",
      "53908.08437027433\n",
      "---------------\n",
      "[312.26602025 310.40941909 309.92193679 ... 193.38911088 200.87629649\n",
      " 207.58647694]\n",
      "53908.08437027433\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain9\n",
      "9 528.965751061358\n",
      "sigmav: 2.2e-26\n",
      "3340136.0\n",
      "3359475.1521099536\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain9/9\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain9/9.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -53036.3 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.090 +- 0.017\n",
      "    a1             -0.0108 +- 0.0017\n",
      "    a2             0.079 +- 0.013\n",
      "    a3             0.0035 +- 0.0016\n",
      "creating marginal plot ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain9/9stats.json\n",
      "---------------\n",
      "[0.8134732166442076, 0.9754902896610397, 1.198591596196963, 1.008036804814236]\n",
      "53009.975282412845\n",
      "---------------\n",
      "[266.95290903 263.68959534 262.238112   ... 162.14207942 171.17176587\n",
      " 180.80119241]\n",
      "53009.975282412845\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain10\n",
      "10 687.4738295408508\n",
      "sigmav: 2.2e-26\n",
      "2777598.0\n",
      "2799360.5296791084\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain10/10\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain10/10.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -51797.2 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.046 +- 0.013\n",
      "    a1             -0.0153 +- 0.0019\n",
      "    a2             0.051 +- 0.012\n",
      "    a3             -0.0014 +- 0.0014\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain10/10stats.json\n",
      "---------------\n",
      "[0.8993615654438478, 0.9653929574536643, 1.1248724109709578, 0.9968106701099645]\n",
      "51771.28271296241\n",
      "---------------\n",
      "[222.24296749 218.40098    216.35817673 ... 132.88620851 141.90020278\n",
      " 153.45645727]\n",
      "51771.28271296241\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain11\n",
      "11 893.4798998900418\n",
      "sigmav: 2.2e-26\n",
      "2230198.0\n",
      "2256028.944332659\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain11/11\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain11/11.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -50394.0 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.067 +- 0.012\n",
      "    a1             -0.0265 +- 0.0021\n",
      "    a2             0.080 +- 0.011\n",
      "    a3             0.0008 +- 0.0013\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain11/11stats.json\n",
      "---------------\n",
      "[0.8576270427117918, 0.9407856927327433, 1.2009978973956714, 1.0018063759804232]\n",
      "50367.74297245611\n",
      "---------------\n",
      "[176.46795705 173.20732801 171.11204993 ... 103.27634226 111.24323271\n",
      " 123.45060727]\n",
      "50367.74297245611\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain12\n",
      "12 1161.217048859432\n",
      "sigmav: 2.2e-26\n",
      "1716155.0\n",
      "1740542.0570082525\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain12/12\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain12/12.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -48836.6 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.0255 +- 0.0097\n",
      "    a1             -0.0286 +- 0.0023\n",
      "    a2             0.040 +- 0.012\n",
      "    a3             -0.0034 +- 0.0013\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain12/12stats.json\n",
      "---------------\n",
      "[0.9429247047699874, 0.9362695637831917, 1.095915367186286, 0.992244348958986]\n",
      "48810.54237902032\n",
      "---------------\n",
      "[130.591142   128.80146686 126.81432866 ...  74.74851644  80.93433876\n",
      "  92.33384159]\n",
      "48810.54237902032\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain13\n",
      "13 1509.1834015826835\n",
      "sigmav: 2.2e-26\n",
      "1267413.0\n",
      "1290463.8346953604\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain13/13\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain13/13.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -46917.7 +- 0.2\n",
      "  parameters:\n",
      "    a0             -0.0501 +- 0.0099\n",
      "    a1             -0.0422 +- 0.0028\n",
      "    a2             0.081 +- 0.012\n",
      "    a3             -0.0030 +- 0.0013\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain13/13stats.json\n",
      "---------------\n",
      "[0.8911366072280398, 0.9074909463960475, 1.204495709428415, 0.9931423756723766]\n",
      "46892.14821494215\n",
      "---------------\n",
      "[93.05625133 92.74062436 90.6851356  ... 52.24062498 56.59740137\n",
      " 66.48573208]\n",
      "46892.14821494215\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain14\n",
      "14 1961.420168477385\n",
      "sigmav: 2.2e-26\n",
      "917609.0\n",
      "937825.613424177\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain14/14\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain14/14.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -44983.4 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.0118 +- 0.0092\n",
      "    a1             -0.0506 +- 0.0031\n",
      "    a2             0.039 +- 0.015\n",
      "    a3             -0.0029 +- 0.0013\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain14/14stats.json\n",
      "---------------\n",
      "[0.9731793933055074, 0.889923830457943, 1.0927769564881504, 0.9933341332530712]\n",
      "44958.21560728341\n",
      "---------------\n",
      "[65.48924717 66.05788784 64.01222531 ... 37.58723947 40.46290184\n",
      " 48.06679724]\n",
      "44958.21560728341\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain15\n",
      "15 2549.1726673347453\n",
      "sigmav: 2.2e-26\n",
      "634484.0\n",
      "653312.8098327718\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain15/15\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain15/15.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -43067.3 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.056 +- 0.011\n",
      "    a1             -0.0552 +- 0.0038\n",
      "    a2             0.098 +- 0.016\n",
      "    a3             0.0011 +- 0.0015\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain15/15stats.json\n",
      "---------------\n",
      "[0.87947429274288, 0.8805976918032333, 1.2542552646066567, 1.002440276602303]\n",
      "43042.657762153256\n",
      "---------------\n",
      "[45.00285085 45.96018546 43.97524297 ... 26.52334749 28.78759225\n",
      " 33.62557135]\n",
      "43042.657762153256\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain16\n",
      "16 3313.049081641206\n",
      "sigmav: 2.2e-26\n",
      "431774.0\n",
      "447274.20729069517\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain16/16\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain16/16.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -40574.4 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.044 +- 0.013\n",
      "    a1             -0.0736 +- 0.0046\n",
      "    a2             0.110 +- 0.021\n",
      "    a3             -0.0041 +- 0.0017\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain16/16stats.json\n",
      "---------------\n",
      "[0.9026653020274664, 0.8441934100955202, 1.2884751826772811, 0.9905651831256094]\n",
      "40550.66976969133\n",
      "---------------\n",
      "[30.60680058 31.75935825 29.72855878 ... 18.10741682 20.45089257\n",
      " 22.64266235]\n",
      "40550.66976969133\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain17\n",
      "17 4305.826105078932\n",
      "sigmav: 2.2e-26\n",
      "283037.0\n",
      "294999.69015156006\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain17/17\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain17/17.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -37909.1 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.057 +- 0.015\n",
      "    a1             -0.0925 +- 0.0060\n",
      "    a2             0.157 +- 0.023\n",
      "    a3             -0.0031 +- 0.0018\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain17/17stats.json\n",
      "---------------\n",
      "[0.8767111356315435, 0.8081815807869259, 1.4367920795660696, 0.9927914092801239]\n",
      "37885.84257482417\n",
      "---------------\n",
      "[20.06866407 21.24000672 19.29491975 ... 11.68374543 14.03952756\n",
      " 14.38078859]\n",
      "37885.84257482417\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain18\n",
      "18 5596.095315918128\n",
      "sigmav: 2.2e-26\n",
      "185313.0\n",
      "193118.2852343273\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain18/18\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain18/18.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -35373.3 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.038 +- 0.016\n",
      "    a1             -0.0833 +- 0.0064\n",
      "    a2             0.126 +- 0.032\n",
      "    a3             -0.0017 +- 0.0016\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain18/18stats.json\n",
      "---------------\n",
      "[0.9152231168154431, 0.8254042743985396, 1.337118839946546, 0.9961186993877846]\n",
      "35350.598180150264\n",
      "---------------\n",
      "[13.06221229 14.10709164 12.50891831 ...  7.70827719  9.64572751\n",
      "  9.21084294]\n",
      "35350.598180150264\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain19\n",
      "19 7273.00221156208\n",
      "sigmav: 2.2e-26\n",
      "122072.0\n",
      "127824.98030309968\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain19/19\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain19/19.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -32709.4 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.051 +- 0.020\n",
      "    a1             -0.0873 +- 0.0076\n",
      "    a2             0.165 +- 0.038\n",
      "    a3             -0.0054 +- 0.0022\n",
      "creating marginal plot ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain19/19stats.json\n",
      "---------------\n",
      "[0.8883902611579562, 0.8178225376240624, 1.4631769857392098, 0.9876047524687787]\n",
      "32687.484404659855\n",
      "---------------\n",
      "[8.75318476 9.55569086 8.4796314  ... 5.38046673 6.52967669 6.32619021]\n",
      "32687.484404659855\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain20\n",
      "20 9452.405326071254\n",
      "sigmav: 2.2e-26\n",
      "81569.0\n",
      "85041.6300608399\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain20/20\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain20/20.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -30230.0 +- 0.7\n",
      "  parameters:\n",
      "    a0             -0.014 +- 0.020\n",
      "    a1             -0.0911 +- 0.0093\n",
      "    a2             0.128 +- 0.054\n",
      "    a3             -0.0020 +- 0.0026\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain20/20stats.json\n",
      "---------------\n",
      "[0.9688432421072606, 0.8106878074080868, 1.3436235707751134, 0.9954228755472899]\n",
      "30210.24482267156\n",
      "---------------\n",
      "[5.97621815 6.54931166 5.918168   ... 3.92167022 4.36661496 4.56494029]\n",
      "30210.24482267156\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain21\n",
      "21 12284.880967903657\n",
      "sigmav: 2.2e-26\n",
      "53721.0\n",
      "56093.56361343799\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain21/21\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain21/21.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -27762.1 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.051 +- 0.028\n",
      "    a1             -0.100 +- 0.011\n",
      "    a2             0.250 +- 0.054\n",
      "    a3             -0.0068 +- 0.0033\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain21/21stats.json\n",
      "---------------\n",
      "[0.8892922429968215, 0.7939897391714618, 1.7792384244779236, 0.9844778690496948]\n",
      "27741.78529247681\n",
      "---------------\n",
      "[4.11138292 4.46167794 4.15443396 ... 2.86881327 2.80588904 3.36297282]\n",
      "27741.78529247681\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain22\n",
      "22 15966.126630150377\n",
      "sigmav: 2.2e-26\n",
      "35049.0\n",
      "36480.35224277983\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain22/22\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain22/22.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -25296.3 +- 0.0\n",
      "  parameters:\n",
      "    a0             -0.052 +- 0.034\n",
      "    a1             -0.068 +- 0.013\n",
      "    a2             0.226 +- 0.077\n",
      "    a3             -0.0091 +- 0.0043\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain22/22stats.json\n",
      "---------------\n",
      "[0.8867237653174729, 0.8547678763086874, 1.681706494435625, 0.979191874495785]\n",
      "25277.07981429065\n",
      "---------------\n",
      "[2.72005436 2.89890912 2.7912512  ... 2.0577063  1.72600953 2.40378992]\n",
      "25277.07981429065\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain23\n",
      "23 20750.481851310695\n",
      "sigmav: 2.2e-26\n",
      "22931.0\n",
      "23786.831436149223\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain23/23\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain23/23.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -22572.8 +- 0.1\n",
      "  parameters:\n",
      "    a0             0.041 +- 0.037\n",
      "    a1             -0.034 +- 0.014\n",
      "    a2             -0.32 +- 0.56\n",
      "    a3             -0.0119 +- 0.0056\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain23/23stats.json\n",
      "---------------\n",
      "[1.0994667252001993, 0.9236949721234005, 0.4799928611045064, 0.9730698621421873]\n",
      "22555.87897667747\n",
      "---------------\n",
      "[1.76026496 1.82530519 1.82090245 ... 1.45071932 1.03500053 1.67631842]\n",
      "22555.87897667747\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain24\n",
      "24 26968.50069123621\n",
      "sigmav: 2.2e-26\n",
      "15373.0\n",
      "15783.5931704662\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain24/24\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain24/24.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -19843.1 +- 0.2\n",
      "  parameters:\n",
      "    a0             0.049 +- 0.037\n",
      "    a1             -0.030 +- 0.017\n",
      "    a2             -0.48 +- 0.73\n",
      "    a3             0.0006 +- 0.0061\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain24/24stats.json\n",
      "---------------\n",
      "[1.1195359744473772, 0.9338997016731914, 0.33277658527193366, 1.0014536341692668]\n",
      "19827.438948659386\n",
      "---------------\n",
      "[1.20892535 1.21273351 1.24759669 ... 1.04517345 0.63444072 1.21200713]\n",
      "19827.438948659386\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain25\n",
      "25 35049.78991546971\n",
      "sigmav: 2.2e-26\n",
      "10218.0\n",
      "10528.839426806768\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain25/25\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain25/25.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -17171.6 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.044 +- 0.055\n",
      "    a1             0.025 +- 0.025\n",
      "    a2             -0.75 +- 0.81\n",
      "    a3             -0.014 +- 0.015\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain25/25stats.json\n",
      "---------------\n",
      "[0.9042576008259466, 1.0596346898635096, 0.17697580177205102, 0.968778589149502]\n",
      "17160.90893510613\n",
      "---------------\n",
      "[0.81238344 0.78701988 0.8352562  ... 0.75971215 0.42117311 0.86155208]\n",
      "17160.90893510613\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain26\n",
      "26 45552.69079225364\n",
      "sigmav: 2.2e-26\n",
      "7003.0\n",
      "6941.5083516960385\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain26/26\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain26/26.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -14396.2 +- 0.2\n",
      "  parameters:\n",
      "    a0             -1.878 +- 0.050\n",
      "    a1             -1.9920 +- 0.0090\n",
      "    a2             -1.75 +- 0.20\n",
      "    a3             0.57106 +- 0.00014\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain26/26stats.json\n",
      "---------------\n",
      "[0.013249808393728822, 0.01018599899380037, 0.017741402641258935, 3.7244048298399113]\n",
      "14376.530946200015\n",
      "---------------\n",
      "[ 0.39662759  0.25369937  0.47504351 ...  0.66576422 -0.37778964\n",
      "  0.78469824]\n",
      "14376.530946200015\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain27\n",
      "27 59202.85523591168\n",
      "sigmav: 2.2e-26\n",
      "4585.0\n",
      "4566.789374204771\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain27/27\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain27/27.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -11155.3 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.842 +- 0.072\n",
      "    a1             -1.795 +- 0.033\n",
      "    a2             -1.84 +- 0.16\n",
      "    a3             0.52996 +- 0.00051\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain27/27stats.json\n",
      "---------------\n",
      "[0.014381652061671338, 0.016017697713861714, 0.014524951932526009, 3.388116580437561]\n",
      "11138.295957617298\n",
      "---------------\n",
      "[ 0.28722944  0.15778814  0.32784305 ...  0.46820916 -0.26248909\n",
      "  0.54798883]\n",
      "11138.295957617298\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain28\n",
      "28 76943.38154619717\n",
      "sigmav: 2.2e-26\n",
      "3024.0\n",
      "2978.939012634929\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain28/28\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain28/28.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -8592.5 +- 0.1\n",
      "  parameters:\n",
      "    a0             -0.718 +- 0.048\n",
      "    a1             -1.62 +- 0.21\n",
      "    a2             -1.70 +- 0.27\n",
      "    a3             0.4591 +- 0.0041\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain28/28stats.json\n",
      "---------------\n",
      "[0.1915781190603513, 0.024015081280631265, 0.01980139534274545, 2.878277222848366]\n",
      "8584.216904083307\n",
      "---------------\n",
      "[ 0.2068098   0.10790617  0.2262652  ...  0.31137857 -0.15443228\n",
      "  0.36064461]\n",
      "8584.216904083307\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain29\n",
      "29 99999.97365283336\n",
      "sigmav: 2.2e-26\n",
      "1892.0\n",
      "1910.147897518727\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain29/29\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain29/29.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -6326.5 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.333 +- 0.092\n",
      "    a1             -1.911 +- 0.087\n",
      "    a2             -1.09 +- 0.42\n",
      "    a3             0.39704 +- 0.00081\n",
      "creating marginal plot ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain29/29stats.json\n",
      "---------------\n",
      "[0.04642035952079595, 0.012283585184435526, 0.08096838438249475, 2.494825593536724]\n",
      "6310.277778754495\n",
      "---------------\n",
      "[ 0.13071109  0.05595995  0.14137628 ...  0.20045693 -0.10286397\n",
      "  0.226683  ]\n",
      "6310.277778754495\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain30\n",
      "30 129965.62575773097\n",
      "sigmav: 2.2e-26\n",
      "1250.0\n",
      "1210.537661348191\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain30/30\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain30/30.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -4597.4 +- 0.2\n",
      "  parameters:\n",
      "    a0             -1.924 +- 0.056\n",
      "    a1             -1.963 +- 0.036\n",
      "    a2             -1.65 +- 0.26\n",
      "    a3             0.3298 +- 0.0010\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain30/30stats.json\n",
      "---------------\n",
      "[0.011916231738113198, 0.010894332302743172, 0.02236723575045725, 2.1370171012506676]\n",
      "4581.843274070259\n",
      "---------------\n",
      "[ 0.08238224  0.02763612  0.08850495 ...  0.12701724 -0.06863748\n",
      "  0.14030343]\n",
      "4581.843274070259\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain31\n",
      "31 168910.68328916508\n",
      "sigmav: 2.2e-26\n",
      "856.0\n",
      "775.0930281721423\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain31/31\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain31/31.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -3422.1 +- 0.2\n",
      "  parameters:\n",
      "    a0             -1.27 +- 0.18\n",
      "    a1             -1.64 +- 0.16\n",
      "    a2             -1.50 +- 0.42\n",
      "    a3             0.3359 +- 0.0053\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain31/31stats.json\n",
      "---------------\n",
      "[0.05415512128778742, 0.022950550615411486, 0.03176452089532275, 2.1674524972955087]\n",
      "3411.189387262658\n",
      "---------------\n",
      "[ 0.0591732   0.01217367  0.06362734 ...  0.09233103 -0.05795635\n",
      "  0.09993181]\n",
      "3411.189387262658\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain32\n",
      "32 219525.8843472731\n",
      "sigmav: 2.2e-26\n",
      "590.0\n",
      "495.4184664988843\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain32/32\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain32/32.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -2706.8 +- 0.3\n",
      "  parameters:\n",
      "    a0             -1.76 +- 0.15\n",
      "    a1             -1.78 +- 0.19\n",
      "    a2             -1.45 +- 0.39\n",
      "    a3             0.2895 +- 0.0026\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain32/32stats.json\n",
      "---------------\n",
      "[0.01754456443824048, 0.016520122917391074, 0.035644431879039276, 1.9475721300283237]\n",
      "2695.6865301566813\n",
      "---------------\n",
      "[ 0.03949413  0.00346974  0.04268434 ...  0.06230503 -0.04199392\n",
      "  0.06549937]\n",
      "2695.6865301566813\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain33\n",
      "33 285308.2644627702\n",
      "sigmav: 2.2e-26\n",
      "413.0\n",
      "322.8142396313823\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain33/33\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain33/33.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -2073.0 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.68 +- 0.26\n",
      "    a1             -1.76 +- 0.20\n",
      "    a2             -1.27 +- 0.56\n",
      "    a3             0.2359 +- 0.0053\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain33/33stats.json\n",
      "---------------\n",
      "[0.020701822058025697, 0.017504450303684374, 0.05322518469741183, 1.7213548436239225]\n",
      "2063.3523305170265\n",
      "---------------\n",
      "[ 0.02744325  0.0003607   0.02982823 ...  0.04282005 -0.02890899\n",
      "  0.04369583]\n",
      "2063.3523305170265\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain34\n",
      "34 370802.7689435847\n",
      "sigmav: 2.2e-26\n",
      "249.0\n",
      "212.63630482714402\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain34/34\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain34/34.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -1386.5 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.66 +- 0.25\n",
      "    a1             -1.83 +- 0.15\n",
      "    a2             -1.14 +- 0.58\n",
      "    a3             0.2326 +- 0.0032\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain34/34stats.json\n",
      "---------------\n",
      "[0.02184609017286535, 0.014693108086612772, 0.07165823111085563, 1.7083707550804557]\n",
      "1376.2106550350911\n",
      "---------------\n",
      "[ 0.02069459 -0.00196334  0.02277849 ...  0.03234882 -0.02301033\n",
      "  0.03205297]\n",
      "1376.2106550350911\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain35\n",
      "35 481916.2659558049\n",
      "sigmav: 2.2e-26\n",
      "187.0\n",
      "134.19156409157347\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain35/35\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain35/35.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -964.3 +- 0.3\n",
      "  parameters:\n",
      "    a0             -1.09 +- 0.55\n",
      "    a1             -1.11 +- 0.52\n",
      "    a2             -0.55 +- 0.90\n",
      "    a3             0.057 +- 0.024\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain35/35stats.json\n",
      "---------------\n",
      "[0.08118307367123216, 0.07681182512217247, 0.28182615374454617, 1.1399637195063412]\n",
      "958.4640910083266\n",
      "---------------\n",
      "[ 0.01137182 -0.00130702  0.01260625 ...  0.01719092 -0.01159087\n",
      "  0.01663253]\n",
      "958.4640910083266\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain36\n",
      "36 626325.6556968171\n",
      "sigmav: 2.2e-26\n",
      "136.0\n",
      "84.44079478881252\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain36/36\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain36/36.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -865.8 +- 0.2\n",
      "  parameters:\n",
      "    a0             -0.96 +- 0.56\n",
      "    a1             -1.03 +- 0.58\n",
      "    a2             -0.2 +- 1.0\n",
      "    a3             0.052 +- 0.028\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain36/36stats.json\n",
      "---------------\n",
      "[0.11062843354251427, 0.0929011918273147, 0.5948888604369215, 1.126202645202348]\n",
      "860.4999826487394\n",
      "---------------\n",
      "[ 0.00779604 -0.00260915  0.0088829  ...  0.01214602 -0.00993004\n",
      "  0.01141515]\n",
      "860.4999826487394\n",
      "created folder :  ./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain37\n",
      "37 814008.272175696\n",
      "sigmav: 2.2e-26\n",
      "85.0\n",
      "54.158243165894866\n",
      "model \"././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain37/37\"\n",
      "  analysing data from ././pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain37/37.txt\n",
      "  marginal likelihood:\n",
      "    ln Z = -572.0 +- 0.1\n",
      "  parameters:\n",
      "    a0             -1.52 +- 0.36\n",
      "    a1             -1.37 +- 0.44\n",
      "    a2             -0.72 +- 0.84\n",
      "    a3             -0.013 +- 0.032\n",
      "creating marginal plot ...\n",
      "./pymultinest_chains/f_BH_chains_13yrs/fbh0.0001/chain37/37stats.json\n",
      "---------------\n",
      "[0.02990571894504427, 0.0422721743801982, 0.1918442224437606, 0.9694206536506433]\n",
      "565.6948757169579\n",
      "---------------\n",
      "[ 0.00489277 -0.0024009   0.00571402 ...  0.00776265 -0.00677401\n",
      "  0.00707311]\n",
      "565.6948757169579\n",
      "------------\n",
      "----------\n",
      "------------------\n",
      "total time:\n",
      "954.2034990787506\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#now need to get the values from the folder, and then calculate the loglikelihood with the dark matter\n",
    "test_crosses_init = np.logspace(np.log10(1e-25), np.log10(1e-29), num = 6)\n",
    "\n",
    "test_crosses = np.concatenate((test_crosses_init, [0]))\n",
    "print(test_crosses)\n",
    "#likelihood_collection = [0 for x in range(len(test_crosses))]\n",
    "total_likelihood = 0\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae)\n",
    "    \n",
    "#probably do for every value of sigmav:\n",
    "counting_crossec = 0\n",
    "importlib.reload(dmj)\n",
    "\n",
    "deltaE = get_deltaE(energyidx)\n",
    "hdu = readfile(filelist[0])\n",
    "energy_here = float(hdu[38].data[energyidx][0])\n",
    "acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptance_forpoisson\n",
    "\n",
    "\n",
    "likelihood_collection = []\n",
    "test_cross_here = 2.2e-26\n",
    "\n",
    "frac_bh = [1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "frac_bh = [1e-4]\n",
    "\n",
    "#timing\n",
    "start = time.time()\n",
    "\n",
    "#for dark matter, going to choose a cross section, maybe 2.2e-26?\n",
    "for fbh in frac_bh:\n",
    "    #test_cross_here = 1e-26\n",
    "    print('--------------------------')\n",
    "    print('Frac of DM In Black Holes:')\n",
    "    test_cross = float(\"%.5g\" % test_cross_here)\n",
    "    print(fbh)\n",
    "    print('--------------------------')\n",
    "    #getting errors and stuff for the dark matter stuff\n",
    "    e = readfile(filelist[0])[38].data\n",
    "    energies = np.array(list(e)).T[0]\n",
    "    #cross_sec normalized to 2.2e-26\n",
    "    points = []\n",
    "    errors = []\n",
    "    values = []\n",
    "    fluxes = []\n",
    "    deltae_cut = np.copy(deltae)\n",
    "\n",
    "\n",
    "    #EGB getting now, first get all the counts at each energy bin\n",
    "    egb_counts = get_all_egb(energies, deltae)/deltae_cut #units of counts per cm^2 per sec per str per MeV\n",
    "\n",
    "    ##These are the names of the parameters we are fitting.\n",
    "    parameters = ['a0', 'a1', 'a2', 'a3']\n",
    "    #parameters = ['a0', 'a1', 'a2']\n",
    "    folder = './pymultinest_chains/f_BH_chains_13yrs/'\n",
    "    livepoints = 400\n",
    "\n",
    "    '''Check if directory exists, if not, create it'''\n",
    "    import os\n",
    "\n",
    "    # You should change 'test' to your preferred folder.\n",
    "    MYDIR1 = (folder + \"fbh\" + str(fbh))\n",
    "    folder_name = \"fbh\" + str(fbh)\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR1)\n",
    "        print(\"created folder : \", MYDIR1)\n",
    "\n",
    "    else:\n",
    "        print(MYDIR1, \"folder already exists.\")\n",
    "    \n",
    "    #getting the constants in front of the poission dist. arrays\n",
    "    temp_likelihood = 0\n",
    "    counting = 0\n",
    "    for energyidx in range(0, len(energies)):\n",
    "        counting = energyidx\n",
    "        #counting = 18-3\n",
    "        # You should change 'test' to your preferred folder.\n",
    "        MYDIR = (MYDIR1 + \"/chain\" + str(energyidx))\n",
    "        CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "        # If folder doesn't exist, then create it.\n",
    "        if not CHECK_FOLDER:\n",
    "            os.makedirs(MYDIR)\n",
    "            print(\"created folder : \", MYDIR)\n",
    "\n",
    "        else:\n",
    "            print(MYDIR, \"folder already exists.\")\n",
    "        print(energyidx, energies[energyidx])\n",
    "        \n",
    "        #photons per pixel\n",
    "        pitest = poisson_dist(2, int(energyidx), cross_section = test_cross)\n",
    "        icstest = poisson_dist(4, int(energyidx), cross_section = test_cross)\n",
    "        bremtest = poisson_dist(0, int(energyidx), cross_section = test_cross)\n",
    "        \n",
    "        #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "        egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[counting], cross_section = test_cross)\n",
    "        \n",
    "        #Dark matter\n",
    "        importlib.reload(dmj) \n",
    "        darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "\n",
    "        darkmtest[np.isnan(darkmtest)] = 0\n",
    "        \n",
    "        #Black Holes\n",
    "        blackholetest = poisson_dist(np.nan, int(energyidx), cross_section = test_cross, dm_bh = True, evapbh = True, blackholem = 2e16)\n",
    "\n",
    "        blackholetest[np.isnan(blackholetest)] = 0\n",
    "        \n",
    "        \n",
    "        #Point Sources\n",
    "        pointstest = poisson_dist(np.nan, energyidx, points = True, cross_section = test_cross) \n",
    "\n",
    "        #gotta add egbtest to the ktest\n",
    "\n",
    "        #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "        ktest = ktest_array[counting]\n",
    "        \n",
    "        print(np.sum(ktest))\n",
    "        print(np.sum(pitest+icstest+bremtest+egbtest+pointstest+darkmtest+blackholetest))\n",
    "\n",
    "        \n",
    "        \n",
    "        finals = pymultinest.run(loglikelihood_formulti, prior, int(len(parameters)), outputfiles_basename=MYDIR+\"/\"+ str(energyidx), n_live_points=livepoints, resume=True, verbose=True)\n",
    "        json.dump(parameters, open(MYDIR+'/' + str(energyidx) + 'params' +'.json', 'w'))\n",
    "        counting += 1\n",
    "        \n",
    "        #now need to find the likelihood for this best fit value\n",
    "        \n",
    "        #create the .data file\n",
    "        !python3 ./pymultinest_chains/multinest_marginals.py ./{MYDIR1}/chain{energyidx}/{energyidx}\n",
    "        #Open the file\n",
    "        path_to_this_file = MYDIR + '/' + str(energyidx) + 'stats.json'\n",
    "        f = open(path_to_this_file)\n",
    "        filehere = json.load(f)\n",
    "        print(path_to_this_file)\n",
    "        constants = []\n",
    "        testpoints0 = (10**(filehere['marginals'][0]['median'])) #pitest\n",
    "        testpoints1 = (10**(filehere['marginals'][1]['median'])) #icstest\n",
    "        testpoints2 = (10**(filehere['marginals'][2]['median'])) #bremtest\n",
    "        testpoints3 = (10**(filehere['marginals'][3]['median'])) #pointsources\n",
    "        print('---------------')\n",
    "        constants=[testpoints0, testpoints1, testpoints2, testpoints3]\n",
    "        print(constants)\n",
    "        print(likelihood_poisson_multinest2([testpoints0, testpoints1, testpoints2, testpoints3], pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest))\n",
    "        print('---------------')\n",
    "        lamb = constants[0]*pitest+constants[1]*icstest+constants[2]*bremtest+egbtest+constants[3]*pointstest+darkmtest+blackholetest\n",
    "        print(lamb)\n",
    "        \n",
    "        likehere = likelihood_poisson_forsigmav(constants, pitest, icstest, bremtest, egbtest, pointstest, ktest, darkmtest, blackholetest)\n",
    "        temp_likelihood += likehere\n",
    "        #temp_likelihood += -2*filehere[\"modes\"][0][\"strictly local log-evidence\"]\n",
    "        print(likehere)\n",
    "    print('------------')\n",
    "    likelihood_collection.append(temp_likelihood)\n",
    "    counting_crossec +=1\n",
    "\n",
    "print('----------')\n",
    "\n",
    "\n",
    "#break\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "end = time.time()\n",
    "print('total time:')\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fbh0.0001']\n"
     ]
    }
   ],
   "source": [
    "#collect the data from the saved files\n",
    "\n",
    "'''\n",
    "Collect the data from the files created\n",
    "'''\n",
    "def reverse_norm(arrhere, norm, deltae, acceptance_here, error = False):\n",
    "    if error:\n",
    "        normhere = np.copy(norm)\n",
    "        deltaehere = np.copy(deltae)\n",
    "        arr = np.array(arrhere).T\n",
    "        adjusted_arr0 = np.array(arr[0])*np.array(normhere)\n",
    "        adjusted_arr1 = np.array(arr[1])*np.array(normhere)\n",
    "        counts0 = np.array((adjusted_arr0*196608)).T #counts\n",
    "        counts1 = np.array((adjusted_arr1*196608)).T #counts\n",
    "        per0 = counts0/(np.array(exposure_time*acceptance_here)) #counts divided by exposure time divided by acceptance rate\n",
    "        per1 = counts1/(np.array(exposure_time*acceptance_here)) #counts divided by exposure time divided by acceptance rate\n",
    "        fin = np.array([np.array(per0/deltaehere), np.array(per1/deltaehere)])\n",
    "    else:\n",
    "        adjusted_arr = np.array(arrhere)*np.array(norm)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*acceptance_here) #counts divided by exposure time divided by acceptance rate\n",
    "\n",
    "        fin = per/deltae\n",
    "    return np.array(fin) #counts per MeV per sec per str per cm^2\n",
    "\n",
    "def get_errors(middle, index, filehere, val):\n",
    "    sigma1_down = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][0]))\n",
    "    sigma1_up = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][1]))\n",
    "    sigma_gap = 2*np.abs(10**(filehere['marginals'][val]['1sigma'][0])-middle)\n",
    "    \n",
    "    \n",
    "    dist = np.abs(10**(cube_limits[val][0])-middle)\n",
    "    \n",
    "    if sigma_gap > dist:\n",
    "        return [sigma1_down, sigma1_up], True\n",
    "    else:\n",
    "        return [sigma1_down, sigma1_up], False\n",
    "\n",
    "deltae_cut = np.copy(deltae)\n",
    "points0 = []\n",
    "points1 = []\n",
    "points2 = []\n",
    "points3 = []\n",
    "\n",
    "errors0 = []\n",
    "errors1 = []\n",
    "errors2 = []\n",
    "errors3 = []\n",
    "\n",
    "errors0_twosig = []\n",
    "errors1_twosig = []\n",
    "errors2_twosig = []\n",
    "errors3_twosig = []\n",
    "\n",
    "testerr3 = []\n",
    "\n",
    "arrow = []\n",
    "\n",
    "hdu = readfile(filelist[0])\n",
    "energy_here = np.concatenate(hdu[38].data, axis = 0)\n",
    "acceptance_here = acceptance_interp(energy_here)\n",
    "\n",
    "f = []\n",
    "folder = './pymultinest_chains/f_BH_chains_13yrs/'\n",
    "test_crosses = [2.2e-26]\n",
    "mypath = folder\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "print(chains_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pymultinest_chains/f_BH_chains/\n"
     ]
    }
   ],
   "source": [
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pymultinest_chains/f_BH_chains_13yrs/\n"
     ]
    }
   ],
   "source": [
    "# i think i need to go in order of the energies, not just the list of filenames\n",
    "print(mypath)\n",
    "for fbh in frac_bh:\n",
    "    for number in range(0, len(energies)):\n",
    "        file = 'chain' + str(number)\n",
    "        number = re.sub('\\D', '', str(file))\n",
    "        path_to_this_file = mypath + \"fbh\" + str(fbh) + \"/\" + file + '/' + str(number) + 'stats.json'\n",
    "        #print(path_to_this_file)\n",
    "        f = open(path_to_this_file)\n",
    "        filehere = json.load(f)\n",
    "\n",
    "        testpoints0 = (10**(filehere['marginals'][0]['median']))\n",
    "        testpoints1 = (10**(filehere['marginals'][1]['median']))\n",
    "        testpoints2 = (10**(filehere['marginals'][2]['median']))\n",
    "        testpoints3 = (10**(filehere['marginals'][3]['median']))\n",
    "        #testpoints4 = (10**(filehere['marginals'][4]['median']))\n",
    "        #testpoints5 = (10**(filehere['marginals'][5]['median']))\n",
    "\n",
    "        errors_pi, errors0_twosig = get_errors(testpoints0, 0, filehere, 0)\n",
    "        errors_ics, errors1_twosig = get_errors(testpoints1, 0, filehere, 1)\n",
    "        errors_brem, errors2_twosig = get_errors(testpoints2, 0, filehere, 2)\n",
    "        #errors_dm, errors3_twosig = get_errors(testpoints3, 0, filehere, 3)\n",
    "        #errors_egb, errors4_twosig = get_errors(testpoints4, 0, filehere, 4)\n",
    "        errors_sources, errors3_twosig = get_errors(testpoints3, 0, filehere, 3)\n",
    "\n",
    "        arrow.append([errors0_twosig, errors1_twosig, errors2_twosig, errors3_twosig])\n",
    "\n",
    "        order = 100\n",
    "    \n",
    "        if errors0_twosig == True:\n",
    "            points0.append(10**(filehere['marginals'][0]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][0]['2sigma'][1])\n",
    "            errors0.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points0.append(10**(filehere['marginals'][0]['median']))\n",
    "            errors0.append(errors_pi)\n",
    "        if errors1_twosig == True:\n",
    "            points1.append(10**(filehere['marginals'][1]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][1]['2sigma'][1])\n",
    "            errors1.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points1.append(10**(filehere['marginals'][1]['median']))\n",
    "            errors1.append(errors_ics)\n",
    "        if errors2_twosig == True:\n",
    "            points2.append(10**(filehere['marginals'][2]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][2]['2sigma'][1])\n",
    "            errors2.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points2.append(10**(filehere['marginals'][2]['median']))\n",
    "            errors2.append(errors_brem)\n",
    "        if errors3_twosig == True:\n",
    "            points3.append(10**(filehere['marginals'][3]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][3]['2sigma'][1])\n",
    "            errors3.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points3.append(10**(filehere['marginals'][3]['median']))\n",
    "            errors3.append(errors_sources)\n",
    "            testerr3.append(10**(filehere['marginals'][3]['median']))\n",
    "        '''\n",
    "        if errors4_twosig == True:\n",
    "            points4.append(10**(filehere['marginals'][4]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][4]['2sigma'][1])\n",
    "            errors4.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points4.append(10**(filehere['marginals'][4]['median']))\n",
    "            errors4.append(errors_egb)\n",
    "        if errors5_twosig == True:\n",
    "            points5.append(10**(filehere['marginals'][5]['2sigma'][1]))\n",
    "            sig1 = 10**(filehere['marginals'][5]['2sigma'][1])\n",
    "            errors5.append([np.abs(sig1/order-sig1), 0])\n",
    "        else:\n",
    "            points5.append(10**(filehere['marginals'][5]['median']))\n",
    "            errors5.append(errors_egb)\n",
    "        '''\n",
    "\n",
    "\n",
    "piflux = reverse_norm(points0, normals[0], deltae_cut, acceptance_here)\n",
    "piflux_err = reverse_norm(errors0, normals[0], deltae_cut, acceptance_here, error = True).T\n",
    "icsflux = reverse_norm(points1, normals[1], deltae_cut, acceptance_here)\n",
    "icsflux_err = reverse_norm(errors1, normals[1], deltae_cut, acceptance_here, error = True).T\n",
    "bremflux = reverse_norm(points2, normals[2], deltae_cut, acceptance_here)\n",
    "bremflux_err = reverse_norm(errors2, normals[2], deltae_cut, acceptance_here, error = True).T\n",
    "#dmflux = reverse_norm(points3, normals[3][0], deltae_cut)\n",
    "#dmfluxtest = reverse_norm(testerr3, normals[3][0], deltae_cut)\n",
    "#dmflux_err = reverse_norm(errors3, normals[3][0], deltae_cut, error = True).T\n",
    "#egbflux = reverse_norm(points3, normals[3], deltae_cut, acceptance_here)\n",
    "#egbflux_err = reverse_norm(errors3, normals[3], deltae_cut, acceptance_here, error = True).T\n",
    "pointsflux = reverse_norm(points3, normals[4], deltae_cut, acceptance_here)\n",
    "pointsflux_err = reverse_norm(errors3, normals[4], deltae_cut, acceptance_here, error = True).T\n",
    "arrow = np.array(arrow).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cut_energy[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[0])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time*acceptance_here/196608\n",
    "cut_energy = np.copy(energies)\n",
    "\n",
    "#Pi0\n",
    "for i in range(0, len(arrow[0])):\n",
    "    if arrow[0][i] == True:\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[i][0]*cut_energy[i]**2, piflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[0][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], piflux[i]*cut_energy[i]**2, color = 'greenyellow', marker = 'X', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[i][0]*cut_energy[i]**2, piflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[0]/units*cut_energy**2, color = 'green')\n",
    "plt.scatter(0, 0, label = r'$\\pi^0$', color = 'greenyellow', marker = 'x', s = sizeofdot)\n",
    "\n",
    "#ICS\n",
    "for i in range(0, len(arrow[1])):\n",
    "    if arrow[1][i] == True:\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[i][0]*cut_energy[i]**2, icsflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[1][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], icsflux[i]*cut_energy[i]**2, color = 'skyblue', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[i][0]*cut_energy[i]**2, icsflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[1]/units*cut_energy**2, color = 'blue')\n",
    "plt.scatter(0, 0, label = 'ICS', color = 'skyblue', marker = 'x', s = sizeofdot)\n",
    "\n",
    "#Bremm\n",
    "for i in range(0, len(arrow[2])):\n",
    "    if arrow[2][i] == True:\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[i][0]*cut_energy[i]**2, bremflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[2][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], bremflux[i]*cut_energy[i]**2, color = 'peachpuff', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[i][0]*cut_energy[i]**2, bremflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[2]/units*cut_energy**2, color = 'darkorange')\n",
    "plt.scatter(0, 0, label = 'Bremm', color = 'peachpuff', marker = '*', s = sizeofdot)\n",
    "\n",
    "#Point sources\n",
    "for i in range(0, len(arrow[3])):\n",
    "    if arrow[3][i] == True:\n",
    "        plt.errorbar(cut_energy[i], pointsflux[i]*cut_energy[i]**2, yerr = np.array([(pointsflux_err[i][0]*cut_energy[i]**2, pointsflux_err[i][1]*cut_energy[i]**2)]).T, uplims = arrow[3][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], pointsflux[i]*cut_energy[i]**2, color = 'orchid', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], pointsflux[i]*cut_energy[i]**2, yerr = np.array([(pointsflux_err[i][0]*cut_energy[i]**2, pointsflux_err[i][1]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[4]/units*cut_energy**2, color = 'thistle', label = 'Fermi')\n",
    "plt.scatter(0, 0, label = 'Point Sources', color = 'orchid', marker = '>', s = sizeofdot)\n",
    "\n",
    "\n",
    "#EGB\n",
    "plt.scatter(cut_energy, normals[3]/units*cut_energy**2, color = 'darkgoldenrod')\n",
    "plt.scatter(0, 0, label = 'EGB', color = 'gold', marker = '*', s = sizeofdot)\n",
    "\n",
    "\n",
    "#Dark Matter\n",
    "plt.scatter(cut_energy, normals[5][0]/units*cut_energy**2, color = 'red', label = 'DarkSUSY')\n",
    "plt.scatter(0, 0, label = 'DM', color = 'maroon', marker = '>', s = sizeofdot)\n",
    "\n",
    "\n",
    "#Black Holes\n",
    "plt.scatter(cut_energy, normals[6][0]/units*cut_energy**2, color = 'lightblue', label = 'Black Holes')\n",
    "plt.scatter(0, 0, label = 'BHs', color = 'aqua', marker = '*', s = sizeofdot)\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 3e2)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('testing1.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "\n",
    "plt.scatter(test_crosses[0:10], np.abs(np.array(likelihood_collection[0:10])-likelihood_collection[-1]), s = 50)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(np.nanmin(test_crosses)*.95, np.nanmax(test_crosses)*1.5)\n",
    "plt.xlabel(r'$\\sigma$v', fontsize=fntsz)\n",
    "plt.ylabel('2lnL', fontsize=fntsz)\n",
    "#plt.ylim(0, 10)\n",
    "plt.xlim(1e-30, 1e-25)\n",
    "plt.ylim(1e-1, 1e6)\n",
    "plt.hlines(4, 1e-30, 1e-23)\n",
    "plt.show()\n",
    "#plt.savefig('loglikelihood_zoomtest1.pdf')\n",
    "\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = ktest_array[12]\n",
    "\n",
    "\n",
    "\n",
    "energyidx = 12+3\n",
    "counting = 12\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "\n",
    "egb_counts = get_all_egb(energies, deltae)/deltae_cut\n",
    "\n",
    "#photons per pixel\n",
    "pitest = poisson_dist(2, int(energyidx), cross_section = test_cross)\n",
    "icstest = poisson_dist(4, int(energyidx), cross_section = test_cross)\n",
    "bremtest = poisson_dist(0, int(energyidx), cross_section = test_cross)\n",
    "\n",
    "\n",
    "#EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[counting], cross_section = test_cross)\n",
    "\n",
    "importlib.reload(dmj) \n",
    "darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "\n",
    "darkmtest[np.isnan(darkmtest)] = 0\n",
    "\n",
    "#Point Sources\n",
    "pointstest = poisson_dist(np.nan, energyidx, points = True, cross_section = test_cross) \n",
    "\n",
    "vals_w_dm = np.copy(lamb)\n",
    "\n",
    "print(test_cross)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energybin = np.concatenate(readfile(filelist[0])[38].data, axis = 0)[energyidx]\n",
    "bins_in_lin = np.log10(energybin)\n",
    "deltae_here = get_deltaE(energyidx)\n",
    "    \n",
    "highe = (energybin+deltae_here)/1e3\n",
    "lowe = (energybin-deltae_here)/1e3\n",
    "    \n",
    "hdu = readfile(filelist[0])\n",
    "numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "NSIDE = int(hdu[0].header['NSIDE'])\n",
    "    \n",
    "degrees = hp.pix2ang(NSIDE, np.array(numpix,dtype=np.int), lonlat = True)\n",
    "    \n",
    "    \n",
    "#need to make sure the initsum is *only within the inner 20 degrees, same for finsum\n",
    "data50 = dmj.get_dNdE(highe, lowe, sigmav = test_cross, analyze_data = False, massx = 100)[1] #photons per cm^2 per sec per str per MeV\n",
    "    \n",
    "#get_where_within_20deg\n",
    "numpix = np.linspace(0, hdu[0].header['NPIX']-1, num = hdu[0].header['NPIX'])\n",
    "NSIDE = int(hdu[0].header['NSIDE'])\n",
    "vec = hp.ang2vec(np.pi/2, 0)\n",
    "ipix_disc = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(20), inclusive = False)\n",
    "\n",
    "ipix_disc_big = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(360), inclusive = False)\n",
    "\n",
    "data50[ipix_disc_big] = np.nan\n",
    "\n",
    "data501 = np.copy(data50)\n",
    "data502 = np.copy(data50)\n",
    "\n",
    "data501[ipix_disc] = true_values\n",
    "data502[ipix_disc] = vals_w_dm\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview((data501), coord = 'G')\n",
    "\n",
    "hp.mollview((data502), coord = 'G')\n",
    "\n",
    "hp.mollview((np.abs(data501-data502)), coord = 'G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipix_disc1 = hp.query_disc(nside=NSIDE, vec=vec, radius=np.radians(1), inclusive = False)\n",
    "print(ipix_disc1)\n",
    "print(data50[99071])\n",
    "print(np.where(ipix_disc == 99071))\n",
    "\n",
    "print(pitest[3053])\n",
    "print(bremtest[3053])\n",
    "print(egbtest[3053])\n",
    "print(pointstest[3053])\n",
    "print(icstest[3053])\n",
    "print(darkmtest[3053])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data501)\n",
    "print(data502)\n",
    "\n",
    "plt.hist(np.abs(data501-data502), bins = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crosses_init = np.logspace(np.log10(1e-22), np.log10(1e-27), num = 20)\n",
    "test_crosses = np.concatenate((test_crosses_init, [0]))\n",
    "test_cross = test_crosses_init[0]\n",
    "counting = 0\n",
    "for energyidx in range(3, len(energies)):        \n",
    "    #photons per pixel\n",
    "    pitest = poisson_dist(2, int(energyidx))\n",
    "    icstest = poisson_dist(4, int(energyidx))\n",
    "    bremtest = poisson_dist(0, int(energyidx))\n",
    "\n",
    "\n",
    "    #EGB counts, we have at each energy bin in units of per cm^2 per s per str per MeV\n",
    "    egbtest = poisson_dist(np.nan, int(energyidx), egb = True, counts = egb_counts[counting])\n",
    "\n",
    "    importlib.reload(dmj) \n",
    "    darkmtest = poisson_dist(np.nan, int(energyidx), dm = True, cross_section = test_cross, analyze_data = False)\n",
    "\n",
    "    darkmtest[np.isnan(darkmtest)] = 0\n",
    "\n",
    "    #Point Sources\n",
    "    pointstest = poisson_dist(np.nan, energyidx, points = True) \n",
    "\n",
    "\n",
    "    #gotta add egbtest to the ktest\n",
    "\n",
    "    #ktest = pitest+icstest+bremtest+darkmtest+egbtest+pointstest+darkmtest\n",
    "    ktest = simulated_data(int(energyidx), [pitest, icstest, bremtest, egbtest, pointstest, darkmtest])\n",
    "    tot = pitest+icstest+bremtest+egbtest+pointstest+darkmtest\n",
    "    \n",
    "    plt.hist(ktest, color = 'red')\n",
    "    plt.hist(tot, color = 'blue')\n",
    "    counting += 1\n",
    "    sdfasd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(likelihood_collection[0:10]-likelihood_collection[-1]))\n",
    "print(likelihood_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "\n",
    "plt.scatter(test_crosses[0:10], np.abs(np.array(likelihood_collection[0:10]-likelihood_collection[-1])), s = 50)\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(np.nanmin(test_crosses)*.95, np.nanmax(test_crosses)*1.5)\n",
    "plt.xlabel(r'$\\sigma$v', fontsize=fntsz)\n",
    "plt.ylabel('2lnL', fontsize=fntsz)\n",
    "#plt.ylim(0, 10)\n",
    "plt.xlim(1e-27, 1e-20)\n",
    "plt.ylim(0, 1e2)\n",
    "#plt.savefig('loglikelihood_zoom.pdf')\n",
    "\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to get the log likelihoods another way\n",
    "import os\n",
    "\n",
    "test_crosses_init = np.logspace(np.log10(1e-22), np.log10(1e-27), num = 10)\n",
    "test_crosses = np.concatenate((test_crosses_init, [0]))\n",
    "\n",
    "likelihoods_test2 = []\n",
    "\n",
    "for test_cross in test_crosses:\n",
    "    # You should change 'test' to your preferred folder.\n",
    "    MYDIR1 = (folder + \"cross_sec\" + str(test_cross))\n",
    "    folder_name = \"cross_sec\" + str(test_cross)\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "    templike = 0\n",
    "    for energyidx in range(3, len(energies)):\n",
    "        MYDIR = (MYDIR1 + \"/chain\" + str(energyidx))\n",
    "        path_to_this_file = MYDIR + '/' + str(energyidx) + 'stats.json'\n",
    "        f = open(path_to_this_file)\n",
    "        filehere = json.load(f)\n",
    "        likeli = -filehere['modes'][0]['strictly local log-evidence']\n",
    "        templike += likeli\n",
    "    likelihoods_test2.append(templike)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihoods_test2[0:10])\n",
    "print(likelihoods_test2[-1])\n",
    "print(np.array(likelihoods_test2[0:10])-likelihoods_test2[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "\n",
    "plt.scatter(test_crosses[0:10], np.array(np.array(likelihoods_test2[0:10])-float(likelihoods_test2[-1])), s = 50)\n",
    "plt.scatter(test_crosses[0:10], np.array(np.array(likelihood_collection[0:10])-float(likelihood_collection[-1])), s = 50, color = 'red')\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(np.nanmin(test_crosses)*.95, np.nanmax(test_crosses)*1.5)\n",
    "plt.xlabel(r'$\\sigma$v', fontsize=fntsz)\n",
    "plt.ylabel('2lnL', fontsize=fntsz)\n",
    "#plt.ylim(0, 10)\n",
    "\n",
    "plt.savefig('loglikelihood1.pdf')\n",
    "plt.xlim(1e-27, 1e-22)\n",
    "plt.ylim(0, 10)\n",
    "#plt.ylim(1e-1, 1e3)\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))\n",
    "plt.savefig('weird_loglikelihood.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best fit\n",
    "\n",
    "#now find best fit for DM\n",
    "\n",
    "def likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux):\n",
    "    a0 = 10**cube[0]\n",
    "    lamb = piflux+icsflux+bremflux+egbflux+sourcesflux+a0*dmflux\n",
    "    fprob = -scipy.special.gammaln(kflux+1)+kflux*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "def prior_bestfit(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*2 -1)\n",
    "    return cube\n",
    "\n",
    "def loglikelihood_fordmbestfit(cube, ndim, nparms):\n",
    "    return likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux)\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae[3:])\n",
    "\n",
    "\n",
    "##These are the names of the parameters we are fitting.\n",
    "parameters = ['a0']\n",
    "folder = './pymultinest_chains/'\n",
    "livepoints = 200\n",
    "\n",
    "'''Check if directory exists, if not, create it'''\n",
    "import os\n",
    "\n",
    "# You should change 'test' to your preferred folder.\n",
    "MYDIR1 = (folder + \"bestfit_dm\")\n",
    "CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(MYDIR1)\n",
    "    print(\"created folder : \", MYDIR1)\n",
    "\n",
    "else:\n",
    "    print(MYDIR1, \"folder already exists.\")\n",
    "dmflux = (get_dm_array(cross_section = 2.2e-26, dm_mass = 100)*cut_energy**2)[0]\n",
    "print('got DM flux.')\n",
    "kflux = piflux+icsflux+bremflux+egbflux+sourcesflux\n",
    "\n",
    "finals = pymultinest.run(loglikelihood_fordmbestfit, prior_bestfit, int(len(parameters)), outputfiles_basename=MYDIR1+\"/bestfitDM\", n_live_points=livepoints, resume=True, verbose=True)\n",
    "\n",
    "json.dump(parameters, open(MYDIR1+'/' + 'params' +'.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now need to get the minimal sigmav\n",
    "mypath = './pymultinest_chains/bestfit_dm/'\n",
    "\n",
    "!python3 ./pymultinest_chains/multinest_marginals.py ./pymultinest_chains/bestfit_dm/bestfitDM\n",
    "number = re.sub('\\D', '', str(file))\n",
    "path_to_this_file = mypath + '/' + 'bestfitDM' + 'stats.json'\n",
    "f = open(path_to_this_file)\n",
    "filehere = json.load(f)\n",
    "print(path_to_this_file)\n",
    "bestfit_sigmav = (10**(filehere['marginals'][0]['median']))\n",
    "\n",
    "print(bestfit_sigmav*2.2e-26)\n",
    "\n",
    "ktesthere = piflux+icsflux+bremflux+egbflux+sourcesflux\n",
    "darkmatter_min = (get_dm_array(cross_section = bestfit_sigmav*2.2e-26, dm_mass = 100)*cut_energy**2)[0]\n",
    "min_loglikelihood = likelihood_poisson_forsigmav(piflux, icsflux, bremflux, egbflux, sourcesflux, ktesthere, darkmatter_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_energy = energies[3:]\n",
    "darkmatter = get_dm_array(cross_section = 1e-23, dm_mass = 100)*cut_energy**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmax(darkmatter[0]))\n",
    "print(darkmatter[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ktest_array))\n",
    "print(len(cut_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(cut_energy, egbflux, color = 'red')\n",
    "plt.scatter(cut_energy, darkmatter, color = 'orange')\n",
    "plt.scatter(cut_energy[14], darkmatter[0][14], color = 'black', s = 50)\n",
    "plt.scatter(cut_energy, piflux, color = 'yellow')\n",
    "plt.scatter(cut_energy, icsflux, color = 'green')\n",
    "plt.scatter(cut_energy, bremflux, color = 'blue')\n",
    "plt.scatter(cut_energy, sourcesflux, color = 'purple')\n",
    "plt.scatter(cut_energy[14], darkmatter[0][14], color = 'black', s = 100)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylim(1e-5, 1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to find best fit to dark matter\n",
    "#basically just take our best fit values from the 5 templates, and fit them with the dm template\n",
    "'''\n",
    "Collect the data from the files created\n",
    "'''\n",
    "\n",
    "def reverse_norm(arr, norm, deltae, accept, error = False):\n",
    "    \n",
    "    if error:\n",
    "        normhere = norm.reshape(35, 1)\n",
    "        deltaehere = deltae.reshape(35, 1)\n",
    "        adjusted_arr = np.array(arr)*np.array(normhere)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltaehere\n",
    "    else:\n",
    "        adjusted_arr = np.array(arr)*np.array(norm)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time**accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltae\n",
    "    return fin #counts per MeV per sec per str per cm^2\n",
    "\n",
    "        \n",
    "points0 = []\n",
    "points1 = []\n",
    "points2 = []\n",
    "points3 = []\n",
    "points4 = []\n",
    "points5 = []\n",
    "\n",
    "arrow = []\n",
    "\n",
    "f = []\n",
    "mypath = './pymultinest_chains/6templates_13yrs/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "\n",
    "for number in range(3, len(energies)):\n",
    "    file = 'chain' + str(number)\n",
    "    number = re.sub('\\D', '', str(file))\n",
    "    path_to_this_file = mypath + file + '/' + str(number) + 'stats.json'\n",
    "    f = open(path_to_this_file)\n",
    "    filehere = json.load(f)\n",
    "    print(path_to_this_file)\n",
    "\n",
    "    \n",
    "    testpoints0 = (10**(filehere['marginals'][0]['median']))\n",
    "    testpoints1 = (10**(filehere['marginals'][1]['median']))\n",
    "    testpoints2 = (10**(filehere['marginals'][2]['median']))\n",
    "    testpoints3 = (10**(filehere['marginals'][3]['median']))\n",
    "    testpoints4 = (10**(filehere['marginals'][4]['median']))\n",
    "    \n",
    "    points0.append(testpoints0)\n",
    "    points1.append(testpoints1)\n",
    "    points2.append(testpoints2)\n",
    "    points3.append(testpoints3)\n",
    "    points4.append(testpoints4)\n",
    "\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "\n",
    "acceptances = []\n",
    "for energyidxhere in range(0, len(cut_energy))\n",
    "    energyidx = energyidxhere+3\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson)\n",
    "\n",
    "piflux = reverse_norm(points0, normals[0], deltae_cut, acceptances)*cut_energy**2\n",
    "icsflux = reverse_norm(points1, normals[1], deltae_cut, acceptances)*cut_energy**2\n",
    "bremflux = reverse_norm(points2, normals[2], deltae_cut, acceptances)*cut_energy**2\n",
    "egbflux = reverse_norm(points3, normals[3], deltae_cut, acceptances)*cut_energy**2\n",
    "sourcesflux = reverse_norm(points4, normals[4], deltae_cut, acceptances)*cut_energy**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dmflux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now find best fit for DM\n",
    "\n",
    "def likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux):\n",
    "    a0 = 10**cube[0]\n",
    "    lamb = piflux+icsflux+bremflux+egbflux+sourcesflux+a0*dmflux\n",
    "    fprob = -scipy.special.gammaln(ktest+1)+ktest*np.log(lamb)-lamb #log likelihood of poisson\n",
    "    return 2*np.nansum(fprob) #perhaps add negative back, perhaps add 2 back?\n",
    "\n",
    "def prior_bestfit(cube, ndim, nparams):\n",
    "    #cube[0] = (cube[0]*np.abs(np.log10(1.1)-np.log10(.9)) - np.log10(.9)) #from 1e-4 to 1e6 apparently\n",
    "    cube[0] = (cube[0]*2 -1)\n",
    "    return cube\n",
    "\n",
    "def loglikelihood_fordmbestfit(cube, ndim, nparms):\n",
    "    return likelihood_dm(cube, piflux, icsflux, bremflux, egbflux, sourcesflux, dmflux, kflux)\n",
    "\n",
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae[3:])\n",
    "\n",
    "\n",
    "##These are the names of the parameters we are fitting.\n",
    "parameters = ['a0']\n",
    "folder = './pymultinest_chains/'\n",
    "livepoints = 200\n",
    "\n",
    "'''Check if directory exists, if not, create it'''\n",
    "import os\n",
    "\n",
    "# You should change 'test' to your preferred folder.\n",
    "MYDIR1 = (folder + \"bestfit_dm\")\n",
    "CHECK_FOLDER = os.path.isdir(MYDIR1)\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "if not CHECK_FOLDER:\n",
    "    os.makedirs(MYDIR1)\n",
    "    print(\"created folder : \", MYDIR1)\n",
    "\n",
    "else:\n",
    "    print(MYDIR1, \"folder already exists.\")\n",
    "    \n",
    "kflux = piflux+icsflux+bremflux+egbflux+sourcesflux\n",
    "finals = pymultinest.run(loglikelihood_fordmbestfit, prior, int(len(parameters)), outputfiles_basename=MYDIR+\"/\"+ str(energyidx), n_live_points=livepoints, resume=True, verbose=True)\n",
    "\n",
    "#now need to get the minimal sigmav\n",
    "mypath = './pymultinest_chains/bestfit_dm/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "\n",
    "file = 'chain' + str(number)\n",
    "number = re.sub('\\D', '', str(file))\n",
    "path_to_this_file = mypath + file + '/' + str(number) + 'stats.json'\n",
    "f = open(path_to_this_file)\n",
    "filehere = json.load(f)\n",
    "print(path_to_this_file)\n",
    "bestfit_sigmav = (10**(filehere['marginals'][0]['median']))\n",
    "\n",
    "print(bestfit_sigmav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(test_crosses, likelihood_collection)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1e-24, 1e-28)\n",
    "#plt.ylim(np.nanmin(likelihood_collection), np.nanmax(likelihood_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3, len(energies)):\n",
    "    !python3 ./pymultinest_chains/multinest_marginals.py ./pymultinest_chains/6templates_13yrs/chain{idx}/{idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = readfile(filelist[0])[38].data\n",
    "energies = np.array(list(e)).T[0]\n",
    "#cross_sec normalized to 2.2e-26\n",
    "points = []\n",
    "errors = []\n",
    "values = []\n",
    "fluxes = []\n",
    "deltae_cut = np.copy(deltae[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collect the data from the files created\n",
    "'''\n",
    "\n",
    "def reverse_norm(arr, norm, deltae, accept, error = False):\n",
    "    if error:\n",
    "        normhere = norm.reshape(35, 1)\n",
    "        deltaehere = deltae.reshape(35, 1)\n",
    "        adjusted_arr = np.array(arr)*np.array(normhere)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltaehere\n",
    "    else:\n",
    "        adjusted_arr = np.array(arr)*np.array(norm)\n",
    "        counts = adjusted_arr*196608 #counts\n",
    "        per = counts/(exposure_time*accept/4/np.pi) #counts divided by 13 years in seconds /.85m^2/.2\n",
    "\n",
    "        fin = per/4/np.pi/deltae\n",
    "    return fin #counts per MeV per sec per str per cm^2\n",
    "\n",
    "def get_errors(middle, index, filehere, val):\n",
    "    sigma1_down = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][0]))\n",
    "    sigma1_up = np.abs(middle-10**(filehere['marginals'][val]['1sigma'][1]))\n",
    "    sigma_gap = 2*np.abs(10**(filehere['marginals'][val]['1sigma'][0])-middle)\n",
    "    \n",
    "    \n",
    "    dist = np.abs(10**(cube_limits[val][0])-middle)\n",
    "    \n",
    "    if sigma_gap > dist:\n",
    "        return [sigma1_down, sigma1_up], True\n",
    "    else:\n",
    "        return [sigma1_down, sigma1_up], False\n",
    "        \n",
    "points0 = []\n",
    "points1 = []\n",
    "points2 = []\n",
    "points3 = []\n",
    "points4 = []\n",
    "points5 = []\n",
    "\n",
    "errors0 = []\n",
    "errors1 = []\n",
    "errors2 = []\n",
    "errors3 = []\n",
    "errors4 = []\n",
    "errors5 = []\n",
    "\n",
    "errors0_twosig = []\n",
    "errors1_twosig = []\n",
    "errors2_twosig = []\n",
    "errors3_twosig = []\n",
    "errors4_twosig = []\n",
    "errors5_twosig = []\n",
    "\n",
    "testerr3 = []\n",
    "\n",
    "arrow = []\n",
    "\n",
    "f = []\n",
    "mypath = './pymultinest_chains/6templates_13yrs/'\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.append(dirnames)\n",
    "chains_list = f[0]\n",
    "\n",
    "#for i in range(3, len(energies)):\n",
    "    #x = (np.loadtxt(MYDIR + '/params' + str(i) + 'stats.dat', skiprows = 4, dtype = 'float', max_rows = 5).T)\n",
    "\n",
    "# i think i need to go in order of the energies, not just the list of filenames\n",
    "for number in range(3, len(energies)):\n",
    "    file = 'chain' + str(number)\n",
    "    number = re.sub('\\D', '', str(file))\n",
    "    path_to_this_file = mypath + file + '/' + str(number) + 'stats.json'\n",
    "    f = open(path_to_this_file)\n",
    "    filehere = json.load(f)\n",
    "    print(path_to_this_file)\n",
    "    '''\n",
    "    points0.append((filehere['marginals'][0]['median']))\n",
    "    points1.append((filehere['marginals'][1]['median']))\n",
    "    points2.append((filehere['marginals'][2]['median']))\n",
    "    points3.append((filehere['marginals'][3]['median']))\n",
    "    points4.append((filehere['marginals'][4]['median']))\n",
    "    errors0.append((filehere['marginals'][0]['sigma']))\n",
    "    errors1.append((filehere['marginals'][1]['sigma']))\n",
    "    errors2.append((filehere['marginals'][2]['sigma']))\n",
    "    errors3.append((filehere['marginals'][3]['sigma']))\n",
    "    errors4.append((filehere['marginals'][4]['sigma']))\n",
    "    '''\n",
    "    \n",
    "    testpoints0 = (10**(filehere['marginals'][0]['median']))\n",
    "    testpoints1 = (10**(filehere['marginals'][1]['median']))\n",
    "    testpoints2 = (10**(filehere['marginals'][2]['median']))\n",
    "    testpoints3 = (10**(filehere['marginals'][3]['median']))\n",
    "    testpoints4 = (10**(filehere['marginals'][4]['median']))\n",
    "    testpoints5 = (10**(filehere['marginals'][5]['median']))\n",
    "\n",
    "    errors_pi, errors0_twosig = get_errors(testpoints0, 0, filehere, 0)\n",
    "    errors_ics, errors1_twosig = get_errors(testpoints1, 0, filehere, 1)\n",
    "    errors_brem, errors2_twosig = get_errors(testpoints2, 0, filehere, 2)\n",
    "    errors_dm, errors3_twosig = get_errors(testpoints3, 0, filehere, 3)\n",
    "    errors_egb, errors4_twosig = get_errors(testpoints4, 0, filehere, 4)\n",
    "    errors_sources, errors5_twosig = get_errors(testpoints5, 0, filehere, 4)\n",
    "    \n",
    "\n",
    "    \n",
    "    arrow.append([errors0_twosig, errors1_twosig, errors2_twosig, errors3_twosig, errors4_twosig, errors5_twosig])\n",
    "    \n",
    "    #want line to be one order of magnitude below the top part of the line\n",
    "    order = 100\n",
    "    \n",
    "    if errors0_twosig == True:\n",
    "        points0.append(10**(filehere['marginals'][0]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][0]['2sigma'][1])\n",
    "        errors0.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points0.append(10**(filehere['marginals'][0]['median']))\n",
    "        errors0.append(errors_pi)\n",
    "    if errors1_twosig == True:\n",
    "        points1.append(10**(filehere['marginals'][1]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][1]['2sigma'][1])\n",
    "        errors1.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points1.append(10**(filehere['marginals'][1]['median']))\n",
    "        errors1.append(errors_ics)\n",
    "    if errors2_twosig == True:\n",
    "        points2.append(10**(filehere['marginals'][2]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][2]['2sigma'][1])\n",
    "        errors2.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points2.append(10**(filehere['marginals'][2]['median']))\n",
    "        errors2.append(errors_brem)\n",
    "    if errors3_twosig == True:\n",
    "        points3.append(10**(filehere['marginals'][3]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][3]['2sigma'][1])\n",
    "        errors3.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points3.append(10**(filehere['marginals'][3]['median']))\n",
    "        errors3.append(errors_dm)\n",
    "    testerr3.append(10**(filehere['marginals'][3]['median']))\n",
    "    if errors4_twosig == True:\n",
    "        points4.append(10**(filehere['marginals'][4]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][4]['2sigma'][1])\n",
    "        errors4.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points4.append(10**(filehere['marginals'][4]['median']))\n",
    "        errors4.append(errors_egb)\n",
    "    if errors5_twosig == True:\n",
    "        points5.append(10**(filehere['marginals'][5]['2sigma'][1]))\n",
    "        sig1 = 10**(filehere['marginals'][5]['2sigma'][1])\n",
    "        errors5.append([np.abs(sig1/order-sig1), 0])\n",
    "    else:\n",
    "        points5.append(10**(filehere['marginals'][5]['median']))\n",
    "        errors5.append(errors_egb)\n",
    "    \n",
    "    #cube_limits = [(-2, 4), (-2, 4), (-3, 6), (-6, 12), (-3, 6)]\n",
    "    #print(int(number))\n",
    "    \n",
    "acceptances = []\n",
    "for energyidxhere in range(0, len(cut_energy))\n",
    "    energyidx = energyidxhere+3\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson) \n",
    "    \n",
    "piflux = reverse_norm(points0, normals[0], deltae_cut, acceptances)\n",
    "piflux_err = reverse_norm(errors0, normals[0], deltae_cut, acceptances, error = True).T\n",
    "icsflux = reverse_norm(points1, normals[1], deltae_cut, acceptances, )\n",
    "icsflux_err = reverse_norm(errors1, normals[1], deltae_cut, acceptances,  error = True).T\n",
    "bremflux = reverse_norm(points2, normals[2], deltae_cut, acceptances)\n",
    "bremflux_err = reverse_norm(errors2, normals[2], deltae_cut, acceptances, error = True).T\n",
    "dmflux = reverse_norm(points3, normals[3][0], deltae_cut, acceptances)\n",
    "dmfluxtest = reverse_norm(testerr3, normals[3][0], deltae_cut, acceptances)\n",
    "dmflux_err = reverse_norm(errors3, normals[3][0], deltae_cut, acceptances, error = True).T\n",
    "egbflux = reverse_norm(points4, normals[4], deltae_cut, acceptances)\n",
    "egbflux_err = reverse_norm(errors4, normals[4], deltae_cut, acceptances, error = True).T\n",
    "sourcesflux = reverse_norm(points5, normals[5], deltae_cut, acceptances)\n",
    "sourcesflux_err = reverse_norm(errors5, normals[5], deltae_cut, acceptances, error = True).T\n",
    "arrow = np.array(arrow).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptances = []\n",
    "for energyidxhere in range(0, len(cut_energy))\n",
    "    energyidx = energyidxhere+3\n",
    "    hdu = readfile(filelist[0])\n",
    "    energy_here = float(hdu[38].data[energyidx][0])\n",
    "    acceptance_forpoisson = acceptance_interp(energy_here) #in m^2*str, convert to cm^2\n",
    "    acceptances.append(acceptance_forpoisson) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = np.copy(energies)\n",
    "\n",
    "#Pi0\n",
    "for i in range(0, len(arrow[0])):\n",
    "    if arrow[0][i] == True:\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[0][i]*cut_energy[i]**2, piflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[0][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], piflux[i]*cut_energy[i]**2, color = 'greenyellow', marker = 'X', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], piflux[i]*cut_energy[i]**2, yerr = np.array([(piflux_err[0][i]*cut_energy[i]**2, piflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[0]/units*cut_energy**2, color = 'green')\n",
    "plt.scatter(0, 0, label = r'$\\pi^0$', color = 'greenyellow', marker = 'x', s = sizeofdot)\n",
    "\n",
    "\n",
    "\n",
    "#ICS\n",
    "for i in range(0, len(arrow[1])):\n",
    "    if arrow[1][i] == True:\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[0][i]*cut_energy[i]**2, icsflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[1][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], icsflux[i]*cut_energy[i]**2, color = 'skyblue', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], icsflux[i]*cut_energy[i]**2, yerr = np.array([(icsflux_err[0][i]*cut_energy[i]**2, icsflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[1]/units*cut_energy**2, color = 'blue')\n",
    "plt.scatter(0, 0, label = 'ICS', color = 'skyblue', marker = 'x', s = sizeofdot)\n",
    "\n",
    "\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.049139889547365305), normals[1][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(-0.19407879963313424), normals[1][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(-0.054794496088809604), normals[1][27], deltae_cut[27])*cut_energy[27]**2, color = 'purple')\n",
    "\n",
    "\n",
    "#Bremm\n",
    "for i in range(0, len(arrow[2])):\n",
    "    if arrow[2][i] == True:\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[0][i]*cut_energy[i]**2, bremflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[2][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], bremflux[i]*cut_energy[i]**2, color = 'peachpuff', marker = 's', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], bremflux[i]*cut_energy[i]**2, yerr = np.array([(bremflux_err[0][i]*cut_energy[i]**2, bremflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[2]/units*cut_energy**2, color = 'darkorange')\n",
    "plt.scatter(0, 0, label = 'Bremm', color = 'peachpuff', marker = '*', s = sizeofdot)\n",
    "\n",
    "\n",
    "#Dark matter\n",
    "for i in range(0, len(arrow[3])):\n",
    "    if arrow[3][i] == True:\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[3][i], capsize = 4, color = 'maroon', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], dmflux[i]*cut_energy[i]**2, color = 'maroon', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[3]/units*cut_energy**2, color = 'red', label = 'DarkSUSY')\n",
    "plt.scatter(0, 0, label = 'DM', color = 'maroon', marker = '>', s = sizeofdot)\n",
    "\n",
    "\n",
    "#EGB\n",
    "for i in range(0, len(arrow[4])):\n",
    "    if arrow[4][i] == True:\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[4][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], egbflux[i]*cut_energy[i]**2, color = 'orchid', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[4]/units*cut_energy**2, color = 'thistle', label = 'Fermi')\n",
    "plt.scatter(0, 0, label = 'EGB', color = 'orchid', marker = '>', s = sizeofdot)\n",
    "\n",
    "#Point Sources\n",
    "for i in range(0, len(arrow[5])):\n",
    "    if arrow[5][i] == True:\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[5][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, color = 'gold', marker = '*', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[5]/units*cut_energy**2, color = 'darkgoldenrod')\n",
    "plt.scatter(0, 0, label = 'Point Sources', color = 'gold', marker = '*', s = sizeofdot)\n",
    "\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.1060309379367271), normals[4][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.013060946061642112), normals[4][27], deltae_cut[27])*cut_energy[27]**2, color = 'green')\n",
    "#plt.scatter(cut_energy[27], reverse_norm(10**(0.058146620211512644), normals[4][27], deltae_cut[27])*cut_energy[27]**2, color = 'purple')\n",
    "\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 3e1)\n",
    "plt.legend()\n",
    "plt.savefig('images/6templates1_13yrs.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "#Point Sources\n",
    "for i in range(0, len(arrow[5])):\n",
    "    if arrow[5][i] == True:\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[5][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, color = 'gold', marker = '*', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], sourcesflux[i]*cut_energy[i]**2, yerr = np.array([(sourcesflux_err[0][i]*cut_energy[i]**2, sourcesflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[5]/units*cut_energy**2, color = 'darkgoldenrod')\n",
    "plt.scatter(0, 0, label = 'Point Sources', color = 'gold', marker = '*', s = sizeofdot)\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-3, 3e1)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "#EGB\n",
    "for i in range(0, len(arrow[4])):\n",
    "    if arrow[4][i] == True:\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[4][i], capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], egbflux[i]*cut_energy[i]**2, color = 'orchid', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], egbflux[i]*cut_energy[i]**2, yerr = np.array([(egbflux_err[0][i]*cut_energy[i]**2, egbflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[4]/units*cut_energy**2, color = 'thistle', label = 'Fermi')\n",
    "plt.scatter(0, 0, label = 'EGB', color = 'orchid', marker = '>', s = sizeofdot)\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-8, 3e-1)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "fntsz = 20\n",
    "sizeofdot = 70\n",
    "norm = 1\n",
    "lowlimits = np.full((len(arrow[3])), True)\n",
    "\n",
    "units = deltae_cut*exposure_time/196608*acceptances\n",
    "cut_energy = energies[3:]\n",
    "\n",
    "#Dark matter\n",
    "for i in range(0, len(arrow[3])):\n",
    "    if arrow[3][i] == True:\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, uplims = arrow[3][i], capsize = 4, color = 'maroon', ms = 10, ls = '')\n",
    "    else:\n",
    "        plt.scatter(cut_energy[i], dmflux[i]*cut_energy[i]**2, color = 'maroon', marker = '>', s = sizeofdot)\n",
    "        plt.errorbar(cut_energy[i], dmflux[i]*cut_energy[i]**2, yerr = np.array([(dmflux_err[0][i]*cut_energy[i]**2, dmflux_err[1][i]*cut_energy[i]**2)]).T, capsize = 4, color = 'black', ms = 10, ls = '')\n",
    "plt.scatter(cut_energy, normals[3]/units*cut_energy**2, color = 'red', label = 'DarkSUSY')\n",
    "plt.scatter(0, 0, label = 'DM', color = 'maroon', marker = '>', s = sizeofdot)\n",
    "plt.scatter(cut_energy[14], dmflux[14]*cut_energy[14]**2, c = 'black')\n",
    "\n",
    "plt.ylabel(r'MeV/cm^2/s/str', fontsize=fntsz)\n",
    "plt.xlabel('MeV', fontsize = fntsz)\n",
    "#plt.legend(fontsize = 15)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "#plt.ylim(1e-10, 1e-1)\n",
    "plt.ylim(1e-7, 3e0)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = readfile(point_sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points[-1].header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
